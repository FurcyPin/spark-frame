{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is spark-frame ?","text":""},{"location":"#spark-frame","title":"Spark-frame","text":""},{"location":"#what-is-it","title":"What is it ?","text":"<p>Spark-frame is a library that super-charges your Spark DataFrames!</p> <p>It brings several utility methods and transformation functions for PySpark DataFrames.</p> <p>Here is a quick list of the most exciting features </p> <ul> <li><code>spark_frame.data_diff.compare_dataframes</code>: compare two SQL tables or DataFrames and generate an HTML report    to view the result. And yes, this is completely free, open source, and it works even with    complex data structures ! It also detects column reordering and can handle type changes.   Go check it out </li> <li><code>spark_frame.nested</code>: Did you ever thought manipulating complex data structures in SQL or Spark was a    nightmare  ? You just found the solution ! The <code>nested</code> library  makes those manipulations much    cleaner and simpler.    Get started over there </li> <li><code>spark_frame.transformations</code>: A wide collection of generic dataframe transformations.<ul> <li>Ever wanted to apply a transformation to every field of a DataFrame depending on it's name or type ?    Easy as pie </li> <li>Ever wanted to rename every field of a DataFrame, including the deeply nested ones ?    Done: </li> <li>Ever wanted to analyze the content of a DataFrame,    but <code>DataFrame.describe()</code>   does not work with complex data types ?    You're welcome </li> </ul> </li> <li><code>spark_frame.schema_utils</code>: Need to dump the schema of a DataFrame somewhere to be able to load it later ?      We got you covered </li> <li><code>spark_frame.graph.ascending_forest_traversal</code>: Need an algorithm that takes the adjacency matrix of a   tree  (or forest) graph and associates each node to their corresponding root node ?  But that other algorithm you tried went into an infinite loop \u221e because your graph isn't really a tree   and occasionally contains cycles ?   Try this </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Visit the official Spark-frame website documentation  for use cases examples  and reference.</p>"},{"location":"#installation","title":"Installation","text":"<p>spark-frame is available on PyPi.</p> <pre><code>pip install spark-frame\n</code></pre>"},{"location":"#compatibilities-and-requirements","title":"Compatibilities and requirements","text":"<p>This library does not depend on any other library. Pyspark must be installed separately to use it. It is compatible with the following versions:</p> <ul> <li>Python: requires 3.8.1 or higher (tested against Python 3.9, 3.10 and 3.11)</li> <li>Pyspark: requires 3.3.0 or higher</li> </ul> <p>This library is tested against Windows, Mac and Linux.</p> <p>Some features require extra libraries to be installed alongside this project. We chose to not include them as direct dependencies for security and flexibility reasons. This way, users who are not using these features don't need to worry about these dependencies.</p> feature Method spark-frame's  version dependency required Generating HTML  reports for data diff <code>DiffResult.export_to_html</code> &gt;= 0.4.0 data-diff-viewer==0.2.* Generating HTML  reports for data diff <code>DiffResult.export_to_html</code> &lt; 0.4 jinja2 <p>Since version 0.4, the code used to generate HTML diff reports has been moved to  data-diff-viewer from the same author.  It comes with a dependency to duckdb,  which is used to store the diff results and embed them in the HTML page.</p>"},{"location":"#genesis-of-the-project","title":"Genesis of the project","text":"<p>These methods were initially part of the karadoc project  used at Younited, but they were fully independent from karadoc,  so it made more sense to keep them as a standalone library.</p> <p>Several of these methods were my initial inspiration to make the cousin project bigquery-frame, which was first made to illustrate this blog article. This is why you will find similar methods in both <code>spark_frame</code> and <code>bigquery_frame</code>,  except the former runs on PySpark while the latter runs on BigQuery (obviously). I try to keep both projects consistent together, and will eventually port new developments made on  one project to the other one.</p>"},{"location":"#changelog","title":"Changelog","text":""},{"location":"#v040","title":"v0.4.0","text":"<p>Fixes and improvements on data_diff.</p> <p>Improvements: - data-diff:    - Now supports complex data types. Declaring a repeated field (e.g. <code>\"s!.id\"</code> in join_cols will now explode the     corresponding array and perform the diff on it).   - When columns are removed or renamed, they are now still displayed in the per-column diff report.   - Refactored and improved the HTML report: it is now fully standalone and can be opened without any      internet connection .   - Can now generate the HTML report directly on any remote file system accessible by Spark (e.g. \"hdfs\", \"s3\", etc.)   - A user-friendly error is now raised when one of the <code>join_cols</code> does not exist.  - added package <code>spark_frame.filesystem</code> that can be used to read and write files directly from the driver using   the java FileSystem from Spark's JVM.</p> <p>Breaking Changes: - data-diff:   - <code>spark_frame.data_diff.DataframeComparator</code> object has been removed.      Please use directly the method <code>spark_frame.data_diff.compare_dataframes</code>.   - package <code>spark_frame.data_diff.diff_results</code> has been renamed to <code>diff_results</code>.   - Generating HTML reports for data diff does not require jinja anymore, but it does now require the installation      of the library data-diff-viewer,      please check the Compatibilities and requirements      section to know which version to use.   - The DiffResult object returned by the <code>compare_dataframes</code> method has evolved. In particular, the     type of <code>diff_df_shards</code> changed from a single <code>DataFrame</code> to a <code>Dict[str, DataFrame]</code>.   - <code>DiffFormatOptions.max_string_length</code> option has been removed   - <code>DiffFormatOptions.nb_diffed_rows</code> has been renamed to <code>nb_top_values_kept_per_column</code>   - <code>spark_frame.data_diff.compare_dataframes_impl.DataframeComparatorException</code> was replaced with     <code>spark_frame.exceptions.DataFrameComparisonException</code>   - <code>spark_frame.data_diff.compare_dataframes_impl.CombinatorialExplosionError</code> was replaced with     <code>spark_frame.exceptions.CombinatorialExplosionError</code></p> <p>QA: - Spark: Added tests to ensure compatibility with Pyspark versions 3.3, 3.4 and 3.5 - Replaced flake and isort with ruff</p>"},{"location":"#v032","title":"v0.3.2","text":"<p>Fixes and improvements on data_diff</p> <ul> <li>Fix: automatic detection of join_col was sometimes selecting the wrong column</li> <li>Visual improvements to HTML diff report:</li> <li>Name of columns used for join are now displayed in bold</li> <li>Total number of column is now displayed when the diff is ok</li> <li>Fix incorrect HTML diff display when one of the DataFrames is empty</li> </ul>"},{"location":"#v031","title":"v0.3.1","text":"<p>Fixes and improvements on data_diff</p> <ul> <li>The <code>export_html_diff_report</code> method now accepts arguments to specify the path and encoding of the output html report. </li> <li>Data-diff join now works correctly with null values</li> <li>Visual improvements to HTML diff report</li> </ul>"},{"location":"#v030","title":"v0.3.0","text":"<p>Fixes and improvements on data_diff</p> <ul> <li>Fixed incorrect diff results</li> <li>Column values are not truncated at all, this was causing incorrect results. The possibility to limit the size    of the column values will be added back in a later version</li> <li>Made sure that the most frequent values per column are now displayed by decreasing order of frequency</li> </ul>"},{"location":"#v020","title":"v0.2.0","text":"<p>Two new exciting features: analyze and data_diff.  They are still in experimental stage and will be improved in future releases.</p> <ul> <li>Added a new transformation <code>spark_frame.transformations.analyze</code>.</li> <li>Added new data_diff feature. Example:</li> </ul> <pre><code>from pyspark.sql import DataFrame\nfrom spark_frame.data_diff import DataframeComparator\ndf1: DataFrame = ...\ndf2: DataFrame = ...\ndiff_result = DataframeComparator().compare_df(df1, df2) # Produces a DiffResult object\ndiff_result.display() # Print a diff report in the terminal\ndiff_result.export_to_html() # Generates a html diff report file named diff_report.html\n</code></pre>"},{"location":"#v011","title":"v0.1.1","text":"<ul> <li>Added a new transformation <code>spark_frame.transformations.flatten_all_arrays</code>.</li> <li>Added support for multi-arg transformation to <code>nested.select</code> and <code>nested.with_fields</code>    With this feature, we can now access parent fields from higher levels   when applying a transformation. Example:</li> </ul> <pre><code>&gt;&gt;&gt; nested.print_schema(df)\n\"\"\"\nroot\n |-- id: integer (nullable = false)\n |-- s1!.average: integer (nullable = false)\n |-- s1!.values!: integer (nullable = false)\n\"\"\"\n&gt;&gt;&gt; df.show(truncate=False)\n+---+--------------------------------------+\n|id |s1                                    |\n+---+--------------------------------------+\n|1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n+---+--------------------------------------+\n&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n&gt;&gt;&gt;     \"s1!.values!\": lambda s1, value: value - s1[\"average\"]  # This transformation takes 2 arguments\n&gt;&gt;&gt; })\n+---+-----------------------------------------+\n|id |s1                                       |\n+---+-----------------------------------------+\n|1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n+---+-----------------------------------------+\n</code></pre>"},{"location":"#v010","title":"v0.1.0","text":"<ul> <li> <p>Added a new amazing module called <code>spark_frame.nested</code>,    which makes manipulation of nested data structure much easier!   Make sure to check out the reference   and the use-cases.</p> </li> <li> <p>Also added a new module called <code>spark_frame.nested_functions</code>,   which contains aggregation methods for nested data structures   (See Reference).</p> </li> <li> <p>New transformations:</p> </li> <li><code>spark_frame.transformations.transform_all_field_names</code></li> <li><code>spark_frame.transformations.transform_all_fields</code></li> <li><code>spark_frame.transformations.unnest_field</code></li> <li><code>spark_frame.transformations.unnest_all_fields</code></li> <li><code>spark_frame.transformations.union_dataframes</code></li> </ul>"},{"location":"#v003","title":"v0.0.3","text":"<ul> <li>New transformation: <code>spark_frame.transformations.convert_all_maps_to_arrays</code>.</li> <li>New transformation: <code>spark_frame.transformations.sort_all_arrays</code>.</li> <li>New transformation: <code>spark_frame.transformations.harmonize_dataframes</code>.</li> </ul>"},{"location":"reference/data_diff/","title":"spark_frame.data_diff","text":"<p>This module contains a the method <code>compare_dataframes</code>  which is used to compare two DataFrames. It generates a <code>DiffResult</code> object, which can be used to display the results on <code>stdout</code> or even be exported as an interactive HTML report file.  For more advanced use cases, the underlying results can also be accessed as DataFrames.</p>"},{"location":"reference/data_diff/#spark_frame.data_diff.compare_dataframes","title":"<code>compare_dataframes(left_df: DataFrame, right_df: DataFrame, join_cols: Optional[List[str]] = None) -&gt; DiffResult</code>","text":"<p>Compares two DataFrames and return a <code>DiffResult</code> object.</p> <p>We first compare the DataFrame schemas. If the schemas are different, we adapt the DataFrames to make them as much comparable as possible: - If the order of the columns changed, we re-order them automatically to perform the diff - If the order of the fields inside a struct changed, we re-order them automatically to perform the diff - If a column type changed, we cast the column to the smallest common type - We don't recognize when a column is renamed, we treat it as if the old column was removed and the new column added</p> <p>If <code>join_cols</code> is specified, we will use the specified columns to perform the comparison join between the two DataFrames. Ideally, the <code>join_cols</code> should respect an unicity constraint.</p> <p>If they contain duplicates, a safety check is performed to prevent a potential combinatorial explosion: if the number of rows in the joined DataFrame would be more than twice the size of the original DataFrames, then an Exception is raised and the user will be asked to provide another set of <code>join_cols</code>.</p> <p>If no <code>join_cols</code> is specified, the algorithm will try to automatically find a single column suitable for the join. However, the automatic inference can only find join keys based on a single column. If the DataFrame's unique keys are composite (multiple columns) they must be given explicitly via <code>join_cols</code> to perform the diff analysis.</p> <p>Tips</p> <ul> <li>If you want to test a column renaming, you can temporarily add renaming steps to the DataFrame   you want to test.</li> <li>If you want to exclude columns from the diff, you can simply drop them from the DataFrames you want to   compare.</li> <li>When comparing arrays, this algorithm ignores their ordering (e.g. <code>[1, 2, 3] == [3, 2, 1]</code>).</li> <li>When dealing with a nested structure, if the struct contains a unique identifier, it can be specified   in the join_cols and the structure will be automatically unnested in the diff results.   For instance, if we have a structure <code>my_array: ARRAY&lt;STRUCT&lt;a, b, ...&gt;&gt;</code>   and if <code>a</code> is a unique identifier, then you can add <code>\"my_array!.a\"</code> in the join_cols argument.   (cf. Example 2)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>left_df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>right_df</code> <code>DataFrame</code> <p>Another DataFrame</p> required <code>join_cols</code> <code>Optional[List[str]]</code> <p>Specifies the columns on which the two DataFrames should be joined to compare them</p> <code>None</code> <p>Returns:</p> Type Description <code>DiffResult</code> <p>A DiffResult object</p> <p>Example 1: simple diff</p> <pre><code>&gt;&gt;&gt; from spark_frame.data_diff.compare_dataframes_impl import __get_test_dfs\n&gt;&gt;&gt; from spark_frame.data_diff import compare_dataframes\n&gt;&gt;&gt; df1, df2 = __get_test_dfs()\n</code></pre> <pre><code>&gt;&gt;&gt; df1.show()\n+---+-----------+\n| id|   my_array|\n+---+-----------+\n|  1|[{1, 2, 3}]|\n|  2|[{1, 2, 3}]|\n|  3|[{1, 2, 3}]|\n+---+-----------+\n</code></pre> <pre><code>&gt;&gt;&gt; df2.show()\n+---+--------------+\n| id|      my_array|\n+---+--------------+\n|  1|[{1, 2, 3, 4}]|\n|  2|[{2, 2, 3, 4}]|\n|  4|[{1, 2, 3, 4}]|\n+---+--------------+\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result = compare_dataframes(df1, df2)\n\nAnalyzing differences...\nNo join_cols provided: trying to automatically infer a column that can be used for joining the two DataFrames\nFound the following column: id\nGenerating the diff by joining the DataFrames together using the inferred column: id\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result.display()\nSchema has changed:\n@@ -1,2 +1,2 @@\n\n id INT\n-my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT&gt;&gt;\n+my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT,d:INT&gt;&gt;\nWARNING: columns that do not match both sides will be ignored\n\ndiff NOT ok\n\nRow count ok: 3 rows\n\n0 (0.0%) rows are identical\n2 (50.0%) rows have changed\n1 (25.0%) rows are only in 'left'\n1 (25.0%) rows are only in 'right\n\nFound the following changes:\n+-----------+-------------+---------------------+---------------------------+--------------+\n|column_name|total_nb_diff|left_value           |right_value                |nb_differences|\n+-----------+-------------+---------------------+---------------------------+--------------+\n|my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1             |\n|my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":2,\"b\":2,\"c\":3,\"d\":4}]|1             |\n+-----------+-------------+---------------------+---------------------------+--------------+\n\n1 rows were only found in 'left' :\nMost frequent values in 'left' for each column :\n+-----------+---------------------+---+\n|column_name|value                |nb |\n+-----------+---------------------+---+\n|id         |3                    |1  |\n|my_array   |[{\"a\":1,\"b\":2,\"c\":3}]|1  |\n+-----------+---------------------+---+\n\n1 rows were only found in 'right' :\nMost frequent values in 'right' for each column :\n+-----------+---------------------------+---+\n|column_name|value                      |nb |\n+-----------+---------------------------+---+\n|id         |4                          |1  |\n|my_array   |[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1  |\n+-----------+---------------------------+---+\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_1.html\")\nReport exported as test_working_dir/compare_dataframes_example_1.html\n</code></pre> <p>Check out the exported report here</p> <p>Example 2: diff on complex structures</p> <p>By adding <code>\"my_array!.a\"</code> to the join_cols argument, the array gets unnested for the diff</p> <pre><code>&gt;&gt;&gt; diff_result_unnested = compare_dataframes(df1, df2, join_cols=[\"id\", \"my_array!.a\"])\n\nAnalyzing differences...\nGenerating the diff by joining the DataFrames together using the provided column: id\nGenerating the diff by joining the DataFrames together using the provided columns: ['id', 'my_array!.a']\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result_unnested.display()\nSchema has changed:\n@@ -1,4 +1,5 @@\n\n id INT\n my_array!.a INT\n my_array!.b INT\n my_array!.c INT\n+my_array!.d INT\nWARNING: columns that do not match both sides will be ignored\n\ndiff NOT ok\n\nWARNING: This diff has multiple granularity levels, we will print the results for each granularity level,\n         but we recommend to export the results to html for a much more digest result.\n\n##############################################################\nGranularity : root (4 rows)\n\nRow count ok: 3 rows\n\n2 (50.0%) rows are identical\n0 (0.0%) rows have changed\n1 (25.0%) rows are only in 'left'\n1 (25.0%) rows are only in 'right\n\n1 rows were only found in 'left' :\nMost frequent values in 'left' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |3    |1  |\n|my_array!.a|1    |2  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n+-----------+-----+---+\n\n1 rows were only found in 'right' :\nMost frequent values in 'right' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |4    |1  |\n|my_array!.a|1    |1  |\n|my_array!.a|2    |1  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n|my_array!.d|4    |3  |\n+-----------+-----+---+\n\n##############################################################\nGranularity : my_array (5 rows)\n\nRow count ok: 3 rows\n\n1 (20.0%) rows are identical\n0 (0.0%) rows have changed\n2 (40.0%) rows are only in 'left'\n2 (40.0%) rows are only in 'right\n\n2 rows were only found in 'left' :\nMost frequent values in 'left' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |3    |1  |\n|my_array!.a|1    |2  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n+-----------+-----+---+\n\n2 rows were only found in 'right' :\nMost frequent values in 'right' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |4    |1  |\n|my_array!.a|1    |1  |\n|my_array!.a|2    |1  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n|my_array!.d|4    |3  |\n+-----------+-----+---+\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result_unnested.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_2.html\")\nReport exported as test_working_dir/compare_dataframes_example_2.html\n</code></pre> <p>Check out the exported report here</p> Source code in <code>spark_frame/data_diff/compare_dataframes_impl.py</code> <pre><code>def compare_dataframes(\n    left_df: DataFrame,\n    right_df: DataFrame,\n    join_cols: Optional[List[str]] = None,\n) -&gt; DiffResult:\n    \"\"\"Compares two DataFrames and return a [`DiffResult`][spark_frame.data_diff.diff_result.DiffResult] object.\n\n    We first compare the DataFrame schemas. If the schemas are different, we adapt the DataFrames to make them\n    as much comparable as possible:\n    - If the order of the columns changed, we re-order them automatically to perform the diff\n    - If the order of the fields inside a struct changed, we re-order them automatically to perform the diff\n    - If a column type changed, we cast the column to the smallest common type\n    - We don't recognize when a column is renamed, we treat it as if the old column was removed and the new column added\n\n    If `join_cols` is specified, we will use the specified columns to perform the comparison join between the\n    two DataFrames. Ideally, the `join_cols` should respect an unicity constraint.\n\n    If they contain duplicates, a safety check is performed to prevent a potential combinatorial explosion:\n    if the number of rows in the joined DataFrame would be more than twice the size of the original DataFrames,\n    then an Exception is raised and the user will be asked to provide another set of `join_cols`.\n\n    If no `join_cols` is specified, the algorithm will try to automatically find a single column suitable for\n    the join. However, the automatic inference can only find join keys based on a single column.\n    If the DataFrame's unique keys are composite (multiple columns) they must be given explicitly via `join_cols`\n    to perform the diff analysis.\n\n    !!! tip \"Tips\"\n        - If you want to test a column renaming, you can temporarily add renaming steps to the DataFrame\n          you want to test.\n        - If you want to exclude columns from the diff, you can simply drop them from the DataFrames you want to\n          compare.\n        - When comparing arrays, this algorithm ignores their ordering (e.g. `[1, 2, 3] == [3, 2, 1]`).\n        - When dealing with a nested structure, if the struct contains a unique identifier, it can be specified\n          in the join_cols and the structure will be automatically unnested in the diff results.\n          For instance, if we have a structure `my_array: ARRAY&lt;STRUCT&lt;a, b, ...&gt;&gt;`\n          and if `a` is a unique identifier, then you can add `\"my_array!.a\"` in the join_cols argument.\n          (cf. Example 2)\n\n    Args:\n        left_df: A Spark DataFrame\n        right_df: Another DataFrame\n        join_cols: Specifies the columns on which the two DataFrames should be joined to compare them\n\n    Returns:\n        A DiffResult object\n\n    Examples: Example 1: simple diff\n        &gt;&gt;&gt; from spark_frame.data_diff.compare_dataframes_impl import __get_test_dfs\n        &gt;&gt;&gt; from spark_frame.data_diff import compare_dataframes\n        &gt;&gt;&gt; df1, df2 = __get_test_dfs()\n\n        &gt;&gt;&gt; df1.show()\n        +---+-----------+\n        | id|   my_array|\n        +---+-----------+\n        |  1|[{1, 2, 3}]|\n        |  2|[{1, 2, 3}]|\n        |  3|[{1, 2, 3}]|\n        +---+-----------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df2.show()\n        +---+--------------+\n        | id|      my_array|\n        +---+--------------+\n        |  1|[{1, 2, 3, 4}]|\n        |  2|[{2, 2, 3, 4}]|\n        |  4|[{1, 2, 3, 4}]|\n        +---+--------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; diff_result = compare_dataframes(df1, df2)\n        &lt;BLANKLINE&gt;\n        Analyzing differences...\n        No join_cols provided: trying to automatically infer a column that can be used for joining the two DataFrames\n        Found the following column: id\n        Generating the diff by joining the DataFrames together using the inferred column: id\n\n        &gt;&gt;&gt; diff_result.display()\n        Schema has changed:\n        @@ -1,2 +1,2 @@\n        &lt;BLANKLINE&gt;\n         id INT\n        -my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT&gt;&gt;\n        +my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT,d:INT&gt;&gt;\n        WARNING: columns that do not match both sides will be ignored\n        &lt;BLANKLINE&gt;\n        diff NOT ok\n        &lt;BLANKLINE&gt;\n        Row count ok: 3 rows\n        &lt;BLANKLINE&gt;\n        0 (0.0%) rows are identical\n        2 (50.0%) rows have changed\n        1 (25.0%) rows are only in 'left'\n        1 (25.0%) rows are only in 'right\n        &lt;BLANKLINE&gt;\n        Found the following changes:\n        +-----------+-------------+---------------------+---------------------------+--------------+\n        |column_name|total_nb_diff|left_value           |right_value                |nb_differences|\n        +-----------+-------------+---------------------+---------------------------+--------------+\n        |my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1             |\n        |my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":2,\"b\":2,\"c\":3,\"d\":4}]|1             |\n        +-----------+-------------+---------------------+---------------------------+--------------+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'left' :\n        Most frequent values in 'left' for each column :\n        +-----------+---------------------+---+\n        |column_name|value                |nb |\n        +-----------+---------------------+---+\n        |id         |3                    |1  |\n        |my_array   |[{\"a\":1,\"b\":2,\"c\":3}]|1  |\n        +-----------+---------------------+---+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'right' :\n        Most frequent values in 'right' for each column :\n        +-----------+---------------------------+---+\n        |column_name|value                      |nb |\n        +-----------+---------------------------+---+\n        |id         |4                          |1  |\n        |my_array   |[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1  |\n        +-----------+---------------------------+---+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_1.html\")\n        Report exported as test_working_dir/compare_dataframes_example_1.html\n\n        [Check out the exported report here](../diff_reports/compare_dataframes_example_1.html)\n\n    Examples: Example 2: diff on complex structures\n        By adding `\"my_array!.a\"` to the join_cols argument, the array gets unnested for the diff\n        &gt;&gt;&gt; diff_result_unnested = compare_dataframes(df1, df2, join_cols=[\"id\", \"my_array!.a\"])\n        &lt;BLANKLINE&gt;\n        Analyzing differences...\n        Generating the diff by joining the DataFrames together using the provided column: id\n        Generating the diff by joining the DataFrames together using the provided columns: ['id', 'my_array!.a']\n\n        &gt;&gt;&gt; diff_result_unnested.display()\n        Schema has changed:\n        @@ -1,4 +1,5 @@\n        &lt;BLANKLINE&gt;\n         id INT\n         my_array!.a INT\n         my_array!.b INT\n         my_array!.c INT\n        +my_array!.d INT\n        WARNING: columns that do not match both sides will be ignored\n        &lt;BLANKLINE&gt;\n        diff NOT ok\n        &lt;BLANKLINE&gt;\n        WARNING: This diff has multiple granularity levels, we will print the results for each granularity level,\n                 but we recommend to export the results to html for a much more digest result.\n        &lt;BLANKLINE&gt;\n        ##############################################################\n        Granularity : root (4 rows)\n        &lt;BLANKLINE&gt;\n        Row count ok: 3 rows\n        &lt;BLANKLINE&gt;\n        2 (50.0%) rows are identical\n        0 (0.0%) rows have changed\n        1 (25.0%) rows are only in 'left'\n        1 (25.0%) rows are only in 'right\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'left' :\n        Most frequent values in 'left' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |3    |1  |\n        |my_array!.a|1    |2  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'right' :\n        Most frequent values in 'right' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |4    |1  |\n        |my_array!.a|1    |1  |\n        |my_array!.a|2    |1  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        |my_array!.d|4    |3  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n        ##############################################################\n        Granularity : my_array (5 rows)\n        &lt;BLANKLINE&gt;\n        Row count ok: 3 rows\n        &lt;BLANKLINE&gt;\n        1 (20.0%) rows are identical\n        0 (0.0%) rows have changed\n        2 (40.0%) rows are only in 'left'\n        2 (40.0%) rows are only in 'right\n        &lt;BLANKLINE&gt;\n        2 rows were only found in 'left' :\n        Most frequent values in 'left' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |3    |1  |\n        |my_array!.a|1    |2  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n        2 rows were only found in 'right' :\n        Most frequent values in 'right' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |4    |1  |\n        |my_array!.a|1    |1  |\n        |my_array!.a|2    |1  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        |my_array!.d|4    |3  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; diff_result_unnested.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_2.html\")\n        Report exported as test_working_dir/compare_dataframes_example_2.html\n\n        [Check out the exported report here](../diff_reports/compare_dataframes_example_2.html)\n    \"\"\"\n    print(\"\\nAnalyzing differences...\")\n\n    if join_cols == []:\n        join_cols = None\n    specified_join_cols = join_cols\n    left_df = convert_all_maps_to_arrays(left_df)\n    right_df = convert_all_maps_to_arrays(right_df)\n\n    if join_cols is None:\n        left_flat = flatten(left_df, struct_separator=STRUCT_SEPARATOR_REPLACEMENT)\n        right_flat = flatten(right_df, struct_separator=STRUCT_SEPARATOR_REPLACEMENT)\n        join_cols, _ = _get_join_cols(\n            left_flat,\n            right_flat,\n            join_cols,\n        )\n    else:\n        validate_fields_exist(join_cols, nested.fields(left_df))\n        validate_fields_exist(join_cols, nested.fields(right_df))\n\n    global_schema_diff_result = diff_dataframe_schemas(left_df, right_df, join_cols)\n    left_df, right_df = _harmonize_and_normalize_dataframes(\n        left_df,\n        right_df,\n        skip_make_dataframes_comparable=global_schema_diff_result.same_schema,\n    )\n\n    diff_dataframe_shards = _build_diff_dataframe_shards(\n        left_df,\n        right_df,\n        global_schema_diff_result,\n        join_cols,\n        specified_join_cols,\n    )\n    diff_result = DiffResult(\n        global_schema_diff_result,\n        diff_dataframe_shards,\n        join_cols,\n    )\n\n    return diff_result\n</code></pre>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffResult","title":"<code>DiffResult</code>","text":"<p>Object summarizing the results of a diff between two DataFrames.</p> Source code in <code>spark_frame/data_diff/diff_result.py</code> <pre><code>class DiffResult:\n    \"\"\"Object summarizing the results of a diff between two DataFrames.\"\"\"\n\n    def __init__(\n        self,\n        schema_diff_result: SchemaDiffResult,\n        diff_df_shards: Dict[str, DataFrame],\n        join_cols: List[str],\n    ) -&gt; None:\n        \"\"\"Class containing the results of a diff between two DataFrames\"\"\"\n        self.schema_diff_result: SchemaDiffResult = schema_diff_result\n        self.diff_df_shards: Dict[str, DataFrame] = diff_df_shards\n        \"\"\"A dict containing one DataFrame for each level of granularity generated by the diff.\n\n        The DataFrames have the following schema:\n\n        - All fields from join_cols present at this level of granularity\n        - For all other fields at this granularity level:\n          a Column `col_name: STRUCT&lt;left_value, right_value, is_equal&gt;`\n        - A Column `__EXISTS__: STRUCT&lt;left_value, right_value&gt;`\n        - A Column `__IS_EQUAL__: BOOLEAN`\n\n        In the simplest cases, there is only one granularity level, called the root level and represented\n        by the string `\"\"`. When comparing DataFrames containing arrays of structs, if the user passes a repeated\n        field as join_cols (for example `\"a!.id\"`), then each level of granularity will be generated.\n        In the example, there will be two: the root level `\"\"` containing all root-level columns, and the\n        level `\"a!\"` containing all the fields inside the exploded array `a!`; with one row per element inside `a!`.\n        \"\"\"\n        self.join_cols: List[str] = join_cols\n        \"\"\"The list of column names to join\"\"\"\n\n    @property\n    def same_schema(self) -&gt; bool:\n        return self.schema_diff_result.same_schema\n\n    @cached_property\n    def same_data(self) -&gt; bool:\n        return self.top_per_col_state_df.where(\n            f.col(\"state\") != f.lit(\"no_change\"),\n        ).isEmpty()\n\n    @cached_property\n    def total_nb_rows(self) -&gt; int:\n        a_join_col = next(col for col in self.join_cols if REPETITION_MARKER not in col)\n        return self.top_per_col_state_df.where(\n            f.col(\"column_name\") == f.lit(a_join_col),\n        ).count()\n\n    @property\n    def is_ok(self) -&gt; bool:\n        return self.same_schema and self.same_data\n\n    @cached_property\n    def diff_stats_shards(self) -&gt; Dict[str, DiffStats]:\n        return self._compute_diff_stats()\n\n    @cached_property\n    def top_per_col_state_df(self) -&gt; DataFrame:\n        def generate() -&gt; Generator[DataFrame, None, None]:\n            for key, diff_df in self.diff_df_shards.items():\n                keep_cols = [\n                    col_name\n                    for col_name in self.schema_diff_result.column_names\n                    if substring_before_last_occurrence(col_name, \"!.\") == key\n                ]\n                df = self._compute_top_per_col_state_df(diff_df)\n                yield df.where(f.col(\"column_name\").isin(keep_cols))\n\n        return union_dataframes(*generate()).localCheckpoint()\n\n    def get_diff_per_col_df(self, max_nb_rows_per_col_state: int) -&gt; DataFrame:\n        \"\"\"Return a DataFrame that gives for each column and each column state (changed, no_change, only_in_left,\n        only_in_right) the total number of occurences and the most frequent occurrences.\n\n        The results returned by this method are cached to avoid unecessary recomputations.\n\n        !!! warning\n            The arrays contained in the field `diff` are NOT guaranteed to be sorted,\n            and Spark currently does not provide any way to perform a sort_by on an ARRAY&lt;STRUCT&gt;.\n\n        Args:\n            max_nb_rows_per_col_state: The maximal size of the arrays in `diff`\n\n        Returns:\n            A DataFrame with the following schema:\n\n                root\n                 |-- column_number: integer (nullable = true)\n                 |-- column_name: string (nullable = true)\n                 |-- counts.total: long (nullable = false)\n                 |-- counts.changed: long (nullable = false)\n                 |-- counts.no_change: long (nullable = false)\n                 |-- counts.only_in_left: long (nullable = false)\n                 |-- counts.only_in_right: long (nullable = false)\n                 |-- diff.changed!.left_value: string (nullable = true)\n                 |-- diff.changed!.right_value: string (nullable = true)\n                 |-- diff.changed!.nb: long (nullable = false)\n                 |-- diff.no_change!.value: string (nullable = true)\n                 |-- diff.no_change!.nb: long (nullable = false)\n                 |-- diff.only_in_left!.value: string (nullable = true)\n                 |-- diff.only_in_left!.nb: long (nullable = false)\n                 |-- diff.only_in_right!.value: string (nullable = true)\n                 |-- diff.only_in_right!.nb: long (nullable = false)\n                &lt;BLANKLINE&gt;\n\n        Examples:\n            &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n            &gt;&gt;&gt; diff_result = _get_test_diff_result()\n            &gt;&gt;&gt; diff_result.diff_df_shards[''].show(truncate=False)\n            +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n            |id                           |c1                           |c2                           |c3                               |c4                               |__EXISTS__   |__IS_EQUAL__|\n            +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n            |{1, 1, true, true, true}     |{a, a, true, true, true}     |{1, 1, true, true, true}     |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |true        |\n            |{2, 2, true, true, true}     |{b, b, true, true, true}     |{2, 3, false, true, true}    |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |false       |\n            |{3, 3, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n            |{4, 4, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n            |{5, NULL, false, true, false}|{c, NULL, false, true, false}|{3, NULL, false, true, false}|{3, NULL, false, true, false}    |{NULL, NULL, false, false, false}|{true, false}|false       |\n            |{NULL, 6, false, false, true}|{NULL, f, false, false, true}|{NULL, 3, false, false, true}|{NULL, NULL, false, false, false}|{NULL, 3, false, false, true}    |{false, true}|false       |\n            +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n            &lt;BLANKLINE&gt;\n            &gt;&gt;&gt; diff_result.top_per_col_state_df.show(100)\n            +-----------+-------------+----------+-----------+---+-------+\n            |column_name|        state|left_value|right_value| nb|row_num|\n            +-----------+-------------+----------+-----------+---+-------+\n            |         c1|    no_change|         b|          b|  3|      1|\n            |         c1|    no_change|         a|          a|  1|      2|\n            |         c1| only_in_left|         c|       NULL|  1|      1|\n            |         c1|only_in_right|      NULL|          f|  1|      1|\n            |         c2|      changed|         2|          4|  2|      1|\n            |         c2|      changed|         2|          3|  1|      2|\n            |         c2|    no_change|         1|          1|  1|      1|\n            |         c2| only_in_left|         3|       NULL|  1|      1|\n            |         c2|only_in_right|      NULL|          3|  1|      1|\n            |         c3| only_in_left|         1|       NULL|  2|      1|\n            |         c3| only_in_left|         2|       NULL|  2|      2|\n            |         c3| only_in_left|         3|       NULL|  1|      3|\n            |         c4|only_in_right|      NULL|          1|  2|      1|\n            |         c4|only_in_right|      NULL|          2|  2|      2|\n            |         c4|only_in_right|      NULL|          3|  1|      3|\n            |         id|    no_change|         1|          1|  1|      1|\n            |         id|    no_change|         2|          2|  1|      2|\n            |         id|    no_change|         3|          3|  1|      3|\n            |         id|    no_change|         4|          4|  1|      4|\n            |         id| only_in_left|         5|       NULL|  1|      1|\n            |         id|only_in_right|      NULL|          6|  1|      1|\n            +-----------+-------------+----------+-----------+---+-------+\n            &lt;BLANKLINE&gt;\n\n            &gt;&gt;&gt; diff_per_col_df = diff_result.get_diff_per_col_df(max_nb_rows_per_col_state=10)\n            &gt;&gt;&gt; from spark_frame import nested\n            &gt;&gt;&gt; nested.print_schema(diff_per_col_df)\n            root\n             |-- column_number: integer (nullable = true)\n             |-- column_name: string (nullable = true)\n             |-- counts.total: long (nullable = false)\n             |-- counts.changed: long (nullable = false)\n             |-- counts.no_change: long (nullable = false)\n             |-- counts.only_in_left: long (nullable = false)\n             |-- counts.only_in_right: long (nullable = false)\n             |-- diff.changed!.left_value: string (nullable = true)\n             |-- diff.changed!.right_value: string (nullable = true)\n             |-- diff.changed!.nb: long (nullable = false)\n             |-- diff.no_change!.value: string (nullable = true)\n             |-- diff.no_change!.nb: long (nullable = false)\n             |-- diff.only_in_left!.value: string (nullable = true)\n             |-- diff.only_in_left!.nb: long (nullable = false)\n             |-- diff.only_in_right!.value: string (nullable = true)\n             |-- diff.only_in_right!.nb: long (nullable = false)\n            &lt;BLANKLINE&gt;\n            &gt;&gt;&gt; diff_per_col_df.show(truncate=False)\n            +-------------+-----------+---------------+----------------------------------------------------------+\n            |column_number|column_name|counts         |diff                                                      |\n            +-------------+-----------+---------------+----------------------------------------------------------+\n            |0            |id         |{6, 0, 4, 1, 1}|{[], [{1, 1}, {2, 1}, {3, 1}, {4, 1}], [{5, 1}], [{6, 1}]}|\n            |1            |c1         |{6, 0, 4, 1, 1}|{[], [{b, 3}, {a, 1}], [{c, 1}], [{f, 1}]}                |\n            |2            |c2         |{6, 3, 1, 1, 1}|{[{2, 4, 2}, {2, 3, 1}], [{1, 1}], [{3, 1}], [{3, 1}]}    |\n            |3            |c3         |{5, 0, 0, 5, 0}|{[], [], [{1, 2}, {2, 2}, {3, 1}], []}                    |\n            |4            |c4         |{5, 0, 0, 0, 5}|{[], [], [], [{1, 2}, {2, 2}, {3, 1}]}                    |\n            +-------------+-----------+---------------+----------------------------------------------------------+\n            &lt;BLANKLINE&gt;\n        \"\"\"  # noqa: E501\n        return _get_diff_per_col_df_with_cache(self, max_nb_rows_per_col_state)\n\n    def _compute_diff_stats_shard(self, diff_df_shard: DataFrame) -&gt; DiffStats:\n        \"\"\"Given a diff_df and its list of join_cols, return stats about the number of differing or missing rows\n\n        &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n        &gt;&gt;&gt; diff_result = _get_test_diff_result()\n        &gt;&gt;&gt; diff_result.diff_df_shards[''].select('__EXISTS__', '__IS_EQUAL__').show()\n        +-------------+------------+\n        |   __EXISTS__|__IS_EQUAL__|\n        +-------------+------------+\n        | {true, true}|        true|\n        | {true, true}|       false|\n        | {true, true}|       false|\n        | {true, true}|       false|\n        |{true, false}|       false|\n        |{false, true}|       false|\n        +-------------+------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; diff_result._compute_diff_stats()['']\n        DiffStats(total=6, no_change=1, changed=3, in_left=5, in_right=5, only_in_left=1, only_in_right=1)\n        \"\"\"\n        res_df = diff_df_shard.select(\n            f.count(f.lit(1)).alias(\"total\"),\n            f.sum(\n                f.when(\n                    PREDICATES.present_in_both &amp; PREDICATES.row_is_equal,\n                    f.lit(1),\n                ).otherwise(f.lit(0)),\n            ).alias(\n                \"no_change\",\n            ),\n            f.sum(\n                f.when(\n                    PREDICATES.present_in_both &amp; PREDICATES.row_changed,\n                    f.lit(1),\n                ).otherwise(f.lit(0)),\n            ).alias(\n                \"changed\",\n            ),\n            f.sum(f.when(PREDICATES.in_left, f.lit(1)).otherwise(f.lit(0))).alias(\n                \"in_left\",\n            ),\n            f.sum(f.when(PREDICATES.in_right, f.lit(1)).otherwise(f.lit(0))).alias(\n                \"in_right\",\n            ),\n            f.sum(f.when(PREDICATES.only_in_left, f.lit(1)).otherwise(f.lit(0))).alias(\n                \"only_in_left\",\n            ),\n            f.sum(f.when(PREDICATES.only_in_right, f.lit(1)).otherwise(f.lit(0))).alias(\n                \"only_in_right\",\n            ),\n        )\n        res = res_df.collect()\n        return DiffStats(\n            **{k: (v if v is not None else 0) for k, v in res[0].asDict().items()},\n        )\n\n    def _compute_diff_stats(self) -&gt; Dict[str, DiffStats]:\n        \"\"\"Given a diff_df and its list of join_cols, return stats about the number of differing or missing rows\n\n        &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n        &gt;&gt;&gt; diff_result = _get_test_diff_result()\n        &gt;&gt;&gt; diff_result.diff_df_shards[''].select('__EXISTS__', '__IS_EQUAL__').show()\n        +-------------+------------+\n        |   __EXISTS__|__IS_EQUAL__|\n        +-------------+------------+\n        | {true, true}|        true|\n        | {true, true}|       false|\n        | {true, true}|       false|\n        | {true, true}|       false|\n        |{true, false}|       false|\n        |{false, true}|       false|\n        +-------------+------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; diff_result._compute_diff_stats()['']\n        DiffStats(total=6, no_change=1, changed=3, in_left=5, in_right=5, only_in_left=1, only_in_right=1)\n        \"\"\"\n        return {\n            key: self._compute_diff_stats_shard(diff_df_shard) for key, diff_df_shard in self.diff_df_shards.items()\n        }\n\n    def _compute_top_per_col_state_df(self, diff_df: DataFrame) -&gt; DataFrame:\n        \"\"\"Given a diff_df, return a DataFrame with the following properties:\n\n        - One row per tuple (column_name, state, left_value, right_value)\n          (where `state` can take the following values: \"only_in_left\", \"only_in_right\", \"no_change\", \"changed\")\n        - A column `nb` that gives the number of occurrence of this specific tuple\n        - At most `max_nb_rows_per_col_state` per tuple (column_name, state). Rows with the highest \"nb\" are kept first.\n\n        Examples:\n            &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n            &gt;&gt;&gt; _diff_result = _get_test_diff_result()\n            &gt;&gt;&gt; diff_df = _diff_result.diff_df_shards['']\n            &gt;&gt;&gt; diff_df.show(truncate=False)\n            +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n            |id                           |c1                           |c2                           |c3                               |c4                               |__EXISTS__   |__IS_EQUAL__|\n            +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n            |{1, 1, true, true, true}     |{a, a, true, true, true}     |{1, 1, true, true, true}     |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |true        |\n            |{2, 2, true, true, true}     |{b, b, true, true, true}     |{2, 3, false, true, true}    |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |false       |\n            |{3, 3, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n            |{4, 4, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n            |{5, NULL, false, true, false}|{c, NULL, false, true, false}|{3, NULL, false, true, false}|{3, NULL, false, true, false}    |{NULL, NULL, false, false, false}|{true, false}|false       |\n            |{NULL, 6, false, false, true}|{NULL, f, false, false, true}|{NULL, 3, false, false, true}|{NULL, NULL, false, false, false}|{NULL, 3, false, false, true}    |{false, true}|false       |\n            +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n            &lt;BLANKLINE&gt;\n            &gt;&gt;&gt; (_diff_result._compute_top_per_col_state_df(diff_df)\n            ...  .orderBy(\"column_name\", \"state\", \"left_value\", \"right_value\")\n            ... ).show(100)\n            +-----------+-------------+----------+-----------+---+-------+\n            |column_name|        state|left_value|right_value| nb|row_num|\n            +-----------+-------------+----------+-----------+---+-------+\n            |         c1|    no_change|         a|          a|  1|      2|\n            |         c1|    no_change|         b|          b|  3|      1|\n            |         c1| only_in_left|         c|       NULL|  1|      1|\n            |         c1|only_in_right|      NULL|          f|  1|      1|\n            |         c2|      changed|         2|          3|  1|      2|\n            |         c2|      changed|         2|          4|  2|      1|\n            |         c2|    no_change|         1|          1|  1|      1|\n            |         c2| only_in_left|         3|       NULL|  1|      1|\n            |         c2|only_in_right|      NULL|          3|  1|      1|\n            |         c3| only_in_left|         1|       NULL|  2|      1|\n            |         c3| only_in_left|         2|       NULL|  2|      2|\n            |         c3| only_in_left|         3|       NULL|  1|      3|\n            |         c4|only_in_right|      NULL|          1|  2|      1|\n            |         c4|only_in_right|      NULL|          2|  2|      2|\n            |         c4|only_in_right|      NULL|          3|  1|      3|\n            |         id|    no_change|         1|          1|  1|      1|\n            |         id|    no_change|         2|          2|  1|      2|\n            |         id|    no_change|         3|          3|  1|      3|\n            |         id|    no_change|         4|          4|  1|      4|\n            |         id| only_in_left|         5|       NULL|  1|      1|\n            |         id|only_in_right|      NULL|          6|  1|      1|\n            +-----------+-------------+----------+-----------+---+-------+\n            &lt;BLANKLINE&gt;\n        \"\"\"  # noqa: E501\n        unpivoted_diff_df = _unpivot(diff_df.drop(IS_EQUAL_COL_NAME, EXISTS_COL_NAME))\n\n        only_in_left = f.col(\"diff\")[\"exists_left\"] &amp; ~f.col(\"diff\")[\"exists_right\"]\n        only_in_right = ~f.col(\"diff\")[\"exists_left\"] &amp; f.col(\"diff\")[\"exists_right\"]\n        exists_in_left_or_right = f.col(\"diff\")[\"exists_left\"] | f.col(\"diff\")[\"exists_right\"]\n\n        df_1 = unpivoted_diff_df.select(\n            \"column_name\",\n            f.when(only_in_left, f.lit(\"only_in_left\"))\n            .when(only_in_right, f.lit(\"only_in_right\"))\n            .when(f.col(\"diff\")[\"is_equal\"], f.lit(\"no_change\"))\n            .otherwise(f.lit(\"changed\"))\n            .alias(\"state\"),\n            \"diff.left_value\",\n            \"diff.right_value\",\n        ).where(exists_in_left_or_right)\n        window = Window.partitionBy(\"column_name\", \"state\").orderBy(\n            f.col(\"nb\").desc(),\n            f.col(\"left_value\"),\n            f.col(\"right_value\"),\n        )\n        df_2 = (\n            df_1.groupBy(\"column_name\", \"state\", \"left_value\", \"right_value\")\n            .agg(f.count(f.lit(1)).alias(\"nb\"))\n            .withColumn(\"row_num\", f.row_number().over(window))\n        )\n        return df_2\n\n    def display(\n        self,\n        show_examples: bool = False,\n        diff_format_options: Optional[DiffFormatOptions] = None,\n    ) -&gt; None:\n        \"\"\"Print a summary of the results in the standard output\n\n        Args:\n            show_examples: If true, display example of rows for each type of change\n            diff_format_options: Formatting options\n\n        Examples:\n            See [spark_frame.data_diff.compare_dataframes][spark_frame.data_diff.compare_dataframes] for more examples.\n\n            &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n            &gt;&gt;&gt; diff_result = _get_test_diff_result()\n            &gt;&gt;&gt; diff_result.display()\n            Schema has changed:\n            @@ -1,6 +1,6 @@\n                 id INT\n                 c1 STRING\n                 c2 STRING\n            -    c3 STRING\n            +    c4 STRING\n            &lt;BLANKLINE&gt;\n            WARNING: columns that do not match both sides will be ignored\n            &lt;BLANKLINE&gt;\n            diff NOT ok\n            &lt;BLANKLINE&gt;\n            Row count ok: 5 rows\n            &lt;BLANKLINE&gt;\n            1 (16.67%) rows are identical\n            3 (50.0%) rows have changed\n            1 (16.67%) rows are only in 'left'\n            1 (16.67%) rows are only in 'right\n            &lt;BLANKLINE&gt;\n            Found the following changes:\n            +-----------+-------------+----------+-----------+--------------+\n            |column_name|total_nb_diff|left_value|right_value|nb_differences|\n            +-----------+-------------+----------+-----------+--------------+\n            |c2         |3            |2         |4          |2             |\n            |c2         |3            |2         |3          |1             |\n            +-----------+-------------+----------+-----------+--------------+\n            &lt;BLANKLINE&gt;\n            1 rows were only found in 'left' :\n            Most frequent values in 'left' for each column :\n            +-----------+-----+---+\n            |column_name|value|nb |\n            +-----------+-----+---+\n            |id         |5    |1  |\n            |c1         |c    |1  |\n            |c2         |3    |1  |\n            |c3         |1    |2  |\n            |c3         |2    |2  |\n            |c3         |3    |1  |\n            +-----------+-----+---+\n            &lt;BLANKLINE&gt;\n            1 rows were only found in 'right' :\n            Most frequent values in 'right' for each column :\n            +-----------+-----+---+\n            |column_name|value|nb |\n            +-----------+-----+---+\n            |id         |6    |1  |\n            |c1         |f    |1  |\n            |c2         |3    |1  |\n            |c4         |1    |2  |\n            |c4         |2    |2  |\n            |c4         |3    |1  |\n            +-----------+-----+---+\n            &lt;BLANKLINE&gt;\n        \"\"\"\n        if diff_format_options is None:\n            diff_format_options = DiffFormatOptions()\n        from spark_frame.data_diff.diff_result_analyzer import DiffResultAnalyzer\n\n        self.schema_diff_result.display()\n        analyzer = DiffResultAnalyzer(diff_format_options)\n        analyzer.display_diff_results(self, show_examples)\n\n    def export_to_html(\n        self,\n        title: Optional[str] = None,\n        output_file_path: str = \"diff_report.html\",\n        encoding: str = \"utf8\",\n        diff_format_options: Optional[DiffFormatOptions] = None,\n    ) -&gt; None:\n        \"\"\"Generate an HTML report of this diff result.\n\n        This generates an HTML report file at the specified `output_file_path` URI location.\n\n        The report file can be opened directly with a web browser, even without any internet connection.\n\n        !!! info\n            This method uses Spark's FileSystem API to write the report.\n            This means that `output_file_path` behaves the same way as the path argument in `df.write.save(path)`:\n\n            - It can be a fully qualified URI pointing to a location on a remote filesystem\n              (e.g. \"hdfs://...\", \"s3://...\", etc.), provided that Spark is configured to access it\n            - If a relative path with no scheme is specified (e.g. `output_file_path=\"diff_report.html\"`), it will\n              write on Spark's default's output location. For example:\n                - when running locally, it will be the process current working directory.\n                - when running on Hadoop, it will be the user's home directory on HDFS.\n                - when running on the cloud (EMR, Dataproc, Azure Synapse, Databricks), it should write on the\n                  default remote storage linked to the cluster.\n\n        Args:\n            title: The title of the report\n            encoding: Encoding used when writing the html report\n            output_file_path: URI of the file to write to.\n            diff_format_options: Formatting options\n\n        Examples:\n            &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n            &gt;&gt;&gt; diff_result = _get_test_diff_result()\n            &gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/diff_result_export_to_html_example.html\")\n            Report exported as test_working_dir/diff_result_export_to_html_example.html\n\n            [Check out the exported report here](../diff_reports/diff_result_export_to_html_example.html)\n        \"\"\"\n        if diff_format_options is None:\n            diff_format_options = DiffFormatOptions()\n        from spark_frame.data_diff.diff_result_analyzer import DiffResultAnalyzer\n\n        analyzer = DiffResultAnalyzer(diff_format_options)\n        diff_result_summary = analyzer.get_diff_result_summary(self)\n        export_html_diff_report(\n            diff_result_summary,\n            title=title,\n            output_file_path=output_file_path,\n            encoding=encoding,\n        )\n</code></pre>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffResult.diff_df_shards","title":"<code>diff_df_shards: Dict[str, DataFrame] = diff_df_shards</code>  <code>instance-attribute</code>","text":"<p>A dict containing one DataFrame for each level of granularity generated by the diff.</p> <p>The DataFrames have the following schema:</p> <ul> <li>All fields from join_cols present at this level of granularity</li> <li>For all other fields at this granularity level:   a Column <code>col_name: STRUCT&lt;left_value, right_value, is_equal&gt;</code></li> <li>A Column <code>__EXISTS__: STRUCT&lt;left_value, right_value&gt;</code></li> <li>A Column <code>__IS_EQUAL__: BOOLEAN</code></li> </ul> <p>In the simplest cases, there is only one granularity level, called the root level and represented by the string <code>\"\"</code>. When comparing DataFrames containing arrays of structs, if the user passes a repeated field as join_cols (for example <code>\"a!.id\"</code>), then each level of granularity will be generated. In the example, there will be two: the root level <code>\"\"</code> containing all root-level columns, and the level <code>\"a!\"</code> containing all the fields inside the exploded array <code>a!</code>; with one row per element inside <code>a!</code>.</p>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffResult.display","title":"<code>display(show_examples: bool = False, diff_format_options: Optional[DiffFormatOptions] = None) -&gt; None</code>","text":"<p>Print a summary of the results in the standard output</p> <p>Parameters:</p> Name Type Description Default <code>show_examples</code> <code>bool</code> <p>If true, display example of rows for each type of change</p> <code>False</code> <code>diff_format_options</code> <code>Optional[DiffFormatOptions]</code> <p>Formatting options</p> <code>None</code> <p>Examples:</p> <p>See spark_frame.data_diff.compare_dataframes for more examples.</p> <pre><code>&gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n&gt;&gt;&gt; diff_result = _get_test_diff_result()\n&gt;&gt;&gt; diff_result.display()\nSchema has changed:\n@@ -1,6 +1,6 @@\n     id INT\n     c1 STRING\n     c2 STRING\n-    c3 STRING\n+    c4 STRING\n\nWARNING: columns that do not match both sides will be ignored\n\ndiff NOT ok\n\nRow count ok: 5 rows\n\n1 (16.67%) rows are identical\n3 (50.0%) rows have changed\n1 (16.67%) rows are only in 'left'\n1 (16.67%) rows are only in 'right\n\nFound the following changes:\n+-----------+-------------+----------+-----------+--------------+\n|column_name|total_nb_diff|left_value|right_value|nb_differences|\n+-----------+-------------+----------+-----------+--------------+\n|c2         |3            |2         |4          |2             |\n|c2         |3            |2         |3          |1             |\n+-----------+-------------+----------+-----------+--------------+\n\n1 rows were only found in 'left' :\nMost frequent values in 'left' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |5    |1  |\n|c1         |c    |1  |\n|c2         |3    |1  |\n|c3         |1    |2  |\n|c3         |2    |2  |\n|c3         |3    |1  |\n+-----------+-----+---+\n\n1 rows were only found in 'right' :\nMost frequent values in 'right' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |6    |1  |\n|c1         |f    |1  |\n|c2         |3    |1  |\n|c4         |1    |2  |\n|c4         |2    |2  |\n|c4         |3    |1  |\n+-----------+-----+---+\n</code></pre> Source code in <code>spark_frame/data_diff/diff_result.py</code> <pre><code>def display(\n    self,\n    show_examples: bool = False,\n    diff_format_options: Optional[DiffFormatOptions] = None,\n) -&gt; None:\n    \"\"\"Print a summary of the results in the standard output\n\n    Args:\n        show_examples: If true, display example of rows for each type of change\n        diff_format_options: Formatting options\n\n    Examples:\n        See [spark_frame.data_diff.compare_dataframes][spark_frame.data_diff.compare_dataframes] for more examples.\n\n        &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n        &gt;&gt;&gt; diff_result = _get_test_diff_result()\n        &gt;&gt;&gt; diff_result.display()\n        Schema has changed:\n        @@ -1,6 +1,6 @@\n             id INT\n             c1 STRING\n             c2 STRING\n        -    c3 STRING\n        +    c4 STRING\n        &lt;BLANKLINE&gt;\n        WARNING: columns that do not match both sides will be ignored\n        &lt;BLANKLINE&gt;\n        diff NOT ok\n        &lt;BLANKLINE&gt;\n        Row count ok: 5 rows\n        &lt;BLANKLINE&gt;\n        1 (16.67%) rows are identical\n        3 (50.0%) rows have changed\n        1 (16.67%) rows are only in 'left'\n        1 (16.67%) rows are only in 'right\n        &lt;BLANKLINE&gt;\n        Found the following changes:\n        +-----------+-------------+----------+-----------+--------------+\n        |column_name|total_nb_diff|left_value|right_value|nb_differences|\n        +-----------+-------------+----------+-----------+--------------+\n        |c2         |3            |2         |4          |2             |\n        |c2         |3            |2         |3          |1             |\n        +-----------+-------------+----------+-----------+--------------+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'left' :\n        Most frequent values in 'left' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |5    |1  |\n        |c1         |c    |1  |\n        |c2         |3    |1  |\n        |c3         |1    |2  |\n        |c3         |2    |2  |\n        |c3         |3    |1  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'right' :\n        Most frequent values in 'right' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |6    |1  |\n        |c1         |f    |1  |\n        |c2         |3    |1  |\n        |c4         |1    |2  |\n        |c4         |2    |2  |\n        |c4         |3    |1  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    if diff_format_options is None:\n        diff_format_options = DiffFormatOptions()\n    from spark_frame.data_diff.diff_result_analyzer import DiffResultAnalyzer\n\n    self.schema_diff_result.display()\n    analyzer = DiffResultAnalyzer(diff_format_options)\n    analyzer.display_diff_results(self, show_examples)\n</code></pre>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffResult.export_to_html","title":"<code>export_to_html(title: Optional[str] = None, output_file_path: str = 'diff_report.html', encoding: str = 'utf8', diff_format_options: Optional[DiffFormatOptions] = None) -&gt; None</code>","text":"<p>Generate an HTML report of this diff result.</p> <p>This generates an HTML report file at the specified <code>output_file_path</code> URI location.</p> <p>The report file can be opened directly with a web browser, even without any internet connection.</p> <p>Info</p> <p>This method uses Spark's FileSystem API to write the report. This means that <code>output_file_path</code> behaves the same way as the path argument in <code>df.write.save(path)</code>:</p> <ul> <li>It can be a fully qualified URI pointing to a location on a remote filesystem   (e.g. \"hdfs://...\", \"s3://...\", etc.), provided that Spark is configured to access it</li> <li>If a relative path with no scheme is specified (e.g. <code>output_file_path=\"diff_report.html\"</code>), it will   write on Spark's default's output location. For example:<ul> <li>when running locally, it will be the process current working directory.</li> <li>when running on Hadoop, it will be the user's home directory on HDFS.</li> <li>when running on the cloud (EMR, Dataproc, Azure Synapse, Databricks), it should write on the   default remote storage linked to the cluster.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>Optional[str]</code> <p>The title of the report</p> <code>None</code> <code>encoding</code> <code>str</code> <p>Encoding used when writing the html report</p> <code>'utf8'</code> <code>output_file_path</code> <code>str</code> <p>URI of the file to write to.</p> <code>'diff_report.html'</code> <code>diff_format_options</code> <code>Optional[DiffFormatOptions]</code> <p>Formatting options</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n&gt;&gt;&gt; diff_result = _get_test_diff_result()\n&gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/diff_result_export_to_html_example.html\")\nReport exported as test_working_dir/diff_result_export_to_html_example.html\n</code></pre> <p>Check out the exported report here</p> Source code in <code>spark_frame/data_diff/diff_result.py</code> <pre><code>def export_to_html(\n    self,\n    title: Optional[str] = None,\n    output_file_path: str = \"diff_report.html\",\n    encoding: str = \"utf8\",\n    diff_format_options: Optional[DiffFormatOptions] = None,\n) -&gt; None:\n    \"\"\"Generate an HTML report of this diff result.\n\n    This generates an HTML report file at the specified `output_file_path` URI location.\n\n    The report file can be opened directly with a web browser, even without any internet connection.\n\n    !!! info\n        This method uses Spark's FileSystem API to write the report.\n        This means that `output_file_path` behaves the same way as the path argument in `df.write.save(path)`:\n\n        - It can be a fully qualified URI pointing to a location on a remote filesystem\n          (e.g. \"hdfs://...\", \"s3://...\", etc.), provided that Spark is configured to access it\n        - If a relative path with no scheme is specified (e.g. `output_file_path=\"diff_report.html\"`), it will\n          write on Spark's default's output location. For example:\n            - when running locally, it will be the process current working directory.\n            - when running on Hadoop, it will be the user's home directory on HDFS.\n            - when running on the cloud (EMR, Dataproc, Azure Synapse, Databricks), it should write on the\n              default remote storage linked to the cluster.\n\n    Args:\n        title: The title of the report\n        encoding: Encoding used when writing the html report\n        output_file_path: URI of the file to write to.\n        diff_format_options: Formatting options\n\n    Examples:\n        &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n        &gt;&gt;&gt; diff_result = _get_test_diff_result()\n        &gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/diff_result_export_to_html_example.html\")\n        Report exported as test_working_dir/diff_result_export_to_html_example.html\n\n        [Check out the exported report here](../diff_reports/diff_result_export_to_html_example.html)\n    \"\"\"\n    if diff_format_options is None:\n        diff_format_options = DiffFormatOptions()\n    from spark_frame.data_diff.diff_result_analyzer import DiffResultAnalyzer\n\n    analyzer = DiffResultAnalyzer(diff_format_options)\n    diff_result_summary = analyzer.get_diff_result_summary(self)\n    export_html_diff_report(\n        diff_result_summary,\n        title=title,\n        output_file_path=output_file_path,\n        encoding=encoding,\n    )\n</code></pre>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffResult.get_diff_per_col_df","title":"<code>get_diff_per_col_df(max_nb_rows_per_col_state: int) -&gt; DataFrame</code>","text":"<p>Return a DataFrame that gives for each column and each column state (changed, no_change, only_in_left, only_in_right) the total number of occurences and the most frequent occurrences.</p> <p>The results returned by this method are cached to avoid unecessary recomputations.</p> <p>Warning</p> <p>The arrays contained in the field <code>diff</code> are NOT guaranteed to be sorted, and Spark currently does not provide any way to perform a sort_by on an ARRAY. <p>Parameters:</p> Name Type Description Default <code>max_nb_rows_per_col_state</code> <code>int</code> <p>The maximal size of the arrays in <code>diff</code></p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the following schema:</p> <p>root  |-- column_number: integer (nullable = true)  |-- column_name: string (nullable = true)  |-- counts.total: long (nullable = false)  |-- counts.changed: long (nullable = false)  |-- counts.no_change: long (nullable = false)  |-- counts.only_in_left: long (nullable = false)  |-- counts.only_in_right: long (nullable = false)  |-- diff.changed!.left_value: string (nullable = true)  |-- diff.changed!.right_value: string (nullable = true)  |-- diff.changed!.nb: long (nullable = false)  |-- diff.no_change!.value: string (nullable = true)  |-- diff.no_change!.nb: long (nullable = false)  |-- diff.only_in_left!.value: string (nullable = true)  |-- diff.only_in_left!.nb: long (nullable = false)  |-- diff.only_in_right!.value: string (nullable = true)  |-- diff.only_in_right!.nb: long (nullable = false)  <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n&gt;&gt;&gt; diff_result = _get_test_diff_result()\n&gt;&gt;&gt; diff_result.diff_df_shards[''].show(truncate=False)\n+-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n|id                           |c1                           |c2                           |c3                               |c4                               |__EXISTS__   |__IS_EQUAL__|\n+-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n|{1, 1, true, true, true}     |{a, a, true, true, true}     |{1, 1, true, true, true}     |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |true        |\n|{2, 2, true, true, true}     |{b, b, true, true, true}     |{2, 3, false, true, true}    |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |false       |\n|{3, 3, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n|{4, 4, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n|{5, NULL, false, true, false}|{c, NULL, false, true, false}|{3, NULL, false, true, false}|{3, NULL, false, true, false}    |{NULL, NULL, false, false, false}|{true, false}|false       |\n|{NULL, 6, false, false, true}|{NULL, f, false, false, true}|{NULL, 3, false, false, true}|{NULL, NULL, false, false, false}|{NULL, 3, false, false, true}    |{false, true}|false       |\n+-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n\n&gt;&gt;&gt; diff_result.top_per_col_state_df.show(100)\n+-----------+-------------+----------+-----------+---+-------+\n|column_name|        state|left_value|right_value| nb|row_num|\n+-----------+-------------+----------+-----------+---+-------+\n|         c1|    no_change|         b|          b|  3|      1|\n|         c1|    no_change|         a|          a|  1|      2|\n|         c1| only_in_left|         c|       NULL|  1|      1|\n|         c1|only_in_right|      NULL|          f|  1|      1|\n|         c2|      changed|         2|          4|  2|      1|\n|         c2|      changed|         2|          3|  1|      2|\n|         c2|    no_change|         1|          1|  1|      1|\n|         c2| only_in_left|         3|       NULL|  1|      1|\n|         c2|only_in_right|      NULL|          3|  1|      1|\n|         c3| only_in_left|         1|       NULL|  2|      1|\n|         c3| only_in_left|         2|       NULL|  2|      2|\n|         c3| only_in_left|         3|       NULL|  1|      3|\n|         c4|only_in_right|      NULL|          1|  2|      1|\n|         c4|only_in_right|      NULL|          2|  2|      2|\n|         c4|only_in_right|      NULL|          3|  1|      3|\n|         id|    no_change|         1|          1|  1|      1|\n|         id|    no_change|         2|          2|  1|      2|\n|         id|    no_change|         3|          3|  1|      3|\n|         id|    no_change|         4|          4|  1|      4|\n|         id| only_in_left|         5|       NULL|  1|      1|\n|         id|only_in_right|      NULL|          6|  1|      1|\n+-----------+-------------+----------+-----------+---+-------+\n</code></pre> <pre><code>&gt;&gt;&gt; diff_per_col_df = diff_result.get_diff_per_col_df(max_nb_rows_per_col_state=10)\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; nested.print_schema(diff_per_col_df)\nroot\n |-- column_number: integer (nullable = true)\n |-- column_name: string (nullable = true)\n |-- counts.total: long (nullable = false)\n |-- counts.changed: long (nullable = false)\n |-- counts.no_change: long (nullable = false)\n |-- counts.only_in_left: long (nullable = false)\n |-- counts.only_in_right: long (nullable = false)\n |-- diff.changed!.left_value: string (nullable = true)\n |-- diff.changed!.right_value: string (nullable = true)\n |-- diff.changed!.nb: long (nullable = false)\n |-- diff.no_change!.value: string (nullable = true)\n |-- diff.no_change!.nb: long (nullable = false)\n |-- diff.only_in_left!.value: string (nullable = true)\n |-- diff.only_in_left!.nb: long (nullable = false)\n |-- diff.only_in_right!.value: string (nullable = true)\n |-- diff.only_in_right!.nb: long (nullable = false)\n\n&gt;&gt;&gt; diff_per_col_df.show(truncate=False)\n+-------------+-----------+---------------+----------------------------------------------------------+\n|column_number|column_name|counts         |diff                                                      |\n+-------------+-----------+---------------+----------------------------------------------------------+\n|0            |id         |{6, 0, 4, 1, 1}|{[], [{1, 1}, {2, 1}, {3, 1}, {4, 1}], [{5, 1}], [{6, 1}]}|\n|1            |c1         |{6, 0, 4, 1, 1}|{[], [{b, 3}, {a, 1}], [{c, 1}], [{f, 1}]}                |\n|2            |c2         |{6, 3, 1, 1, 1}|{[{2, 4, 2}, {2, 3, 1}], [{1, 1}], [{3, 1}], [{3, 1}]}    |\n|3            |c3         |{5, 0, 0, 5, 0}|{[], [], [{1, 2}, {2, 2}, {3, 1}], []}                    |\n|4            |c4         |{5, 0, 0, 0, 5}|{[], [], [], [{1, 2}, {2, 2}, {3, 1}]}                    |\n+-------------+-----------+---------------+----------------------------------------------------------+\n</code></pre> Source code in <code>spark_frame/data_diff/diff_result.py</code> <pre><code>def get_diff_per_col_df(self, max_nb_rows_per_col_state: int) -&gt; DataFrame:\n    \"\"\"Return a DataFrame that gives for each column and each column state (changed, no_change, only_in_left,\n    only_in_right) the total number of occurences and the most frequent occurrences.\n\n    The results returned by this method are cached to avoid unecessary recomputations.\n\n    !!! warning\n        The arrays contained in the field `diff` are NOT guaranteed to be sorted,\n        and Spark currently does not provide any way to perform a sort_by on an ARRAY&lt;STRUCT&gt;.\n\n    Args:\n        max_nb_rows_per_col_state: The maximal size of the arrays in `diff`\n\n    Returns:\n        A DataFrame with the following schema:\n\n            root\n             |-- column_number: integer (nullable = true)\n             |-- column_name: string (nullable = true)\n             |-- counts.total: long (nullable = false)\n             |-- counts.changed: long (nullable = false)\n             |-- counts.no_change: long (nullable = false)\n             |-- counts.only_in_left: long (nullable = false)\n             |-- counts.only_in_right: long (nullable = false)\n             |-- diff.changed!.left_value: string (nullable = true)\n             |-- diff.changed!.right_value: string (nullable = true)\n             |-- diff.changed!.nb: long (nullable = false)\n             |-- diff.no_change!.value: string (nullable = true)\n             |-- diff.no_change!.nb: long (nullable = false)\n             |-- diff.only_in_left!.value: string (nullable = true)\n             |-- diff.only_in_left!.nb: long (nullable = false)\n             |-- diff.only_in_right!.value: string (nullable = true)\n             |-- diff.only_in_right!.nb: long (nullable = false)\n            &lt;BLANKLINE&gt;\n\n    Examples:\n        &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n        &gt;&gt;&gt; diff_result = _get_test_diff_result()\n        &gt;&gt;&gt; diff_result.diff_df_shards[''].show(truncate=False)\n        +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n        |id                           |c1                           |c2                           |c3                               |c4                               |__EXISTS__   |__IS_EQUAL__|\n        +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n        |{1, 1, true, true, true}     |{a, a, true, true, true}     |{1, 1, true, true, true}     |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |true        |\n        |{2, 2, true, true, true}     |{b, b, true, true, true}     |{2, 3, false, true, true}    |{1, NULL, false, true, false}    |{NULL, 1, false, false, true}    |{true, true} |false       |\n        |{3, 3, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n        |{4, 4, true, true, true}     |{b, b, true, true, true}     |{2, 4, false, true, true}    |{2, NULL, false, true, false}    |{NULL, 2, false, false, true}    |{true, true} |false       |\n        |{5, NULL, false, true, false}|{c, NULL, false, true, false}|{3, NULL, false, true, false}|{3, NULL, false, true, false}    |{NULL, NULL, false, false, false}|{true, false}|false       |\n        |{NULL, 6, false, false, true}|{NULL, f, false, false, true}|{NULL, 3, false, false, true}|{NULL, NULL, false, false, false}|{NULL, 3, false, false, true}    |{false, true}|false       |\n        +-----------------------------+-----------------------------+-----------------------------+---------------------------------+---------------------------------+-------------+------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; diff_result.top_per_col_state_df.show(100)\n        +-----------+-------------+----------+-----------+---+-------+\n        |column_name|        state|left_value|right_value| nb|row_num|\n        +-----------+-------------+----------+-----------+---+-------+\n        |         c1|    no_change|         b|          b|  3|      1|\n        |         c1|    no_change|         a|          a|  1|      2|\n        |         c1| only_in_left|         c|       NULL|  1|      1|\n        |         c1|only_in_right|      NULL|          f|  1|      1|\n        |         c2|      changed|         2|          4|  2|      1|\n        |         c2|      changed|         2|          3|  1|      2|\n        |         c2|    no_change|         1|          1|  1|      1|\n        |         c2| only_in_left|         3|       NULL|  1|      1|\n        |         c2|only_in_right|      NULL|          3|  1|      1|\n        |         c3| only_in_left|         1|       NULL|  2|      1|\n        |         c3| only_in_left|         2|       NULL|  2|      2|\n        |         c3| only_in_left|         3|       NULL|  1|      3|\n        |         c4|only_in_right|      NULL|          1|  2|      1|\n        |         c4|only_in_right|      NULL|          2|  2|      2|\n        |         c4|only_in_right|      NULL|          3|  1|      3|\n        |         id|    no_change|         1|          1|  1|      1|\n        |         id|    no_change|         2|          2|  1|      2|\n        |         id|    no_change|         3|          3|  1|      3|\n        |         id|    no_change|         4|          4|  1|      4|\n        |         id| only_in_left|         5|       NULL|  1|      1|\n        |         id|only_in_right|      NULL|          6|  1|      1|\n        +-----------+-------------+----------+-----------+---+-------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; diff_per_col_df = diff_result.get_diff_per_col_df(max_nb_rows_per_col_state=10)\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; nested.print_schema(diff_per_col_df)\n        root\n         |-- column_number: integer (nullable = true)\n         |-- column_name: string (nullable = true)\n         |-- counts.total: long (nullable = false)\n         |-- counts.changed: long (nullable = false)\n         |-- counts.no_change: long (nullable = false)\n         |-- counts.only_in_left: long (nullable = false)\n         |-- counts.only_in_right: long (nullable = false)\n         |-- diff.changed!.left_value: string (nullable = true)\n         |-- diff.changed!.right_value: string (nullable = true)\n         |-- diff.changed!.nb: long (nullable = false)\n         |-- diff.no_change!.value: string (nullable = true)\n         |-- diff.no_change!.nb: long (nullable = false)\n         |-- diff.only_in_left!.value: string (nullable = true)\n         |-- diff.only_in_left!.nb: long (nullable = false)\n         |-- diff.only_in_right!.value: string (nullable = true)\n         |-- diff.only_in_right!.nb: long (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; diff_per_col_df.show(truncate=False)\n        +-------------+-----------+---------------+----------------------------------------------------------+\n        |column_number|column_name|counts         |diff                                                      |\n        +-------------+-----------+---------------+----------------------------------------------------------+\n        |0            |id         |{6, 0, 4, 1, 1}|{[], [{1, 1}, {2, 1}, {3, 1}, {4, 1}], [{5, 1}], [{6, 1}]}|\n        |1            |c1         |{6, 0, 4, 1, 1}|{[], [{b, 3}, {a, 1}], [{c, 1}], [{f, 1}]}                |\n        |2            |c2         |{6, 3, 1, 1, 1}|{[{2, 4, 2}, {2, 3, 1}], [{1, 1}], [{3, 1}], [{3, 1}]}    |\n        |3            |c3         |{5, 0, 0, 5, 0}|{[], [], [{1, 2}, {2, 2}, {3, 1}], []}                    |\n        |4            |c4         |{5, 0, 0, 0, 5}|{[], [], [], [{1, 2}, {2, 2}, {3, 1}]}                    |\n        +-------------+-----------+---------------+----------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"  # noqa: E501\n    return _get_diff_per_col_df_with_cache(self, max_nb_rows_per_col_state)\n</code></pre>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffFormatOptions","title":"<code>DiffFormatOptions</code>  <code>dataclass</code>","text":"<p>Class used to pass formatting option when displaying a <code>DiffResult</code></p> Source code in <code>spark_frame/data_diff/diff_format_options.py</code> <pre><code>@dataclass\nclass DiffFormatOptions:\n    \"\"\"Class used to pass formatting option when displaying a [`DiffResult`][spark_frame.data_diff.DiffResult]\"\"\"\n\n    nb_top_values_kept_per_column: int = 10\n    \"\"\"Number of most frequent values/changes kept for each column\"\"\"\n    left_df_alias: str = \"left\"\n    \"\"\"Name given to the left DataFrame in the diff\"\"\"\n    right_df_alias: str = \"right\"\n    \"\"\"Name given to the right DataFrame in the diff\"\"\"\n</code></pre>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffFormatOptions.left_df_alias","title":"<code>left_df_alias: str = 'left'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name given to the left DataFrame in the diff</p>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffFormatOptions.nb_top_values_kept_per_column","title":"<code>nb_top_values_kept_per_column: int = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of most frequent values/changes kept for each column</p>"},{"location":"reference/data_diff/#spark_frame.data_diff.DiffFormatOptions.right_df_alias","title":"<code>right_df_alias: str = 'right'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name given to the right DataFrame in the diff</p>"},{"location":"reference/filesystem/","title":"spark_frame.filesystem","text":"<p>This module contains methods that can be used to interact with the FileSystem API used by Spark's JVM,  directly in Python.</p>"},{"location":"reference/filesystem/#spark_frame.filesystem.read_file","title":"<code>read_file(path: str, encoding: str = 'utf8') -&gt; str</code>","text":"<p>Read the content of a file using the <code>org.apache.hadoop.fs.FileSystem</code> from Spark's JVM. Depending on how Spark is configured, it can write on any file system supported by Spark. (like \"file://\", \"hdfs://\", \"s3://\", \"gs://\", \"abfs://\", etc.)</p> <p>Warning</p> <p>This method loads the entirety of the file in memory as a Python str object. It's use should be restricted to reading small files such as configuration files or reports.</p> <p>When reading large data files, it is recommended to use the <code>spark.read</code> method.</p> <p>Warning</p> <p>The SparkSession must be instantiated before this method can be used.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file to read</p> required <code>encoding</code> <code>str</code> <code>'utf8'</code> <p>Raises:</p> Type Description <code>SparkSessionNotStarted</code> <p>if the SparkSession is not started when this method is called.</p> <code>FileNotFoundError</code> <p>if no file were found at the specified path.</p> <p>Returns:</p> Type Description <code>str</code> <p>the content of the file.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; write_file(text=\"Hello\\n\", path=\"test_working_dir/read_file.txt\", mode=\"overwrite\")\n&gt;&gt;&gt; text = read_file(\"test_working_dir/read_file.txt\")\n&gt;&gt;&gt; print(text)\nHello\n\n&gt;&gt;&gt; read_file(\"test_working_dir/this_file_does_not_exist.txt\")\nTraceback (most recent call last):\n  ...\nFileNotFoundError: The file test_working_dir/this_file_does_not_exist.txt was not found.\n</code></pre> Source code in <code>spark_frame/filesystem.py</code> <pre><code>def read_file(path: str, encoding: str = \"utf8\") -&gt; str:\n    r\"\"\"Read the content of a file using the `org.apache.hadoop.fs.FileSystem` from Spark's JVM.\n    Depending on how Spark is configured, it can write on any file system supported by Spark.\n    (like \"file://\", \"hdfs://\", \"s3://\", \"gs://\", \"abfs://\", etc.)\n\n    !!! warning\n        This method loads the entirety of the file in memory as a Python str object.\n        It's use should be restricted to reading small files such as configuration files or reports.\n\n        When reading large data files, it is recommended to use the `spark.read` method.\n\n    !!! warning\n        The SparkSession must be instantiated before this method can be used.\n\n    Args:\n        path: The path of the file to read\n        encoding:\n\n    Raises:\n        spark_frame.exceptions.SparkSessionNotStarted: if the SparkSession is not started when this method is called.\n        FileNotFoundError: if no file were found at the specified path.\n\n    Returns:\n        the content of the file.\n\n    Examples:\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; write_file(text=\"Hello\\n\", path=\"test_working_dir/read_file.txt\", mode=\"overwrite\")\n        &gt;&gt;&gt; text = read_file(\"test_working_dir/read_file.txt\")\n        &gt;&gt;&gt; print(text)\n        Hello\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; read_file(\"test_working_dir/this_file_does_not_exist.txt\")\n        Traceback (most recent call last):\n          ...\n        FileNotFoundError: The file test_working_dir/this_file_does_not_exist.txt was not found.\n    \"\"\"\n    spark = SparkSession.getActiveSession()\n    assert_true(spark is not None, SparkSessionNotStarted())\n    java_fs = _get_java_file_system(path, spark)\n    java_path = spark._jvm.org.apache.hadoop.fs.Path(path)  # noqa: SLF001\n    if not java_fs.exists(java_path):\n        msg = f\"The file {path} was not found.\"\n        raise FileNotFoundError(msg)\n    java_file_stream = java_fs.open(java_path)\n    scala_source_module = getattr(getattr(spark._jvm.scala.io, \"Source$\"), \"MODULE$\")  # noqa: SLF001\n    scala_source = scala_source_module.fromInputStream(java_file_stream, encoding)\n    return scala_source.mkString()\n</code></pre>"},{"location":"reference/filesystem/#spark_frame.filesystem.write_file","title":"<code>write_file(text: str, path: str, mode: str = MODE_ERROR_IF_EXISTS, encoding: str = 'utf8') -&gt; None</code>","text":"<p>Write given text to a file using the <code>org.apache.hadoop.fs.FileSystem</code> from Spark's JVM. Depending on how Spark is configured, it can write on any file system supported by Spark. (like \"file://\", \"hdfs://\", \"s3://\", \"gs://\", \"abfs://\", etc.)</p> <p>Warning</p> <p>This method loads the entirety of the file in memory as a Python str object. It's use should be restricted to writing small files such as configuration files or reports.</p> <p>When reading large data files, it is recommended to use the <code>spark.read</code> method.</p> <p>Warning</p> <p>The SparkSession must be instantiated before this method can be used.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The content of the file to write</p> required <code>path</code> <code>str</code> <p>The path of the file to write</p> required <code>mode</code> <code>str</code> <p>Either one of [\"overwrite\", \"append\", \"error_if_exists\"].</p> <code>MODE_ERROR_IF_EXISTS</code> <code>encoding</code> <code>str</code> <p>Encoding to use</p> <code>'utf8'</code> <p>Raises:</p> Type Description <code>SparkSessionNotStarted</code> <p>if the SparkSession is not started when this method is called.</p> <code>IllegalArgumentException</code> <p>if the mode is incorrect.</p> <code>FileAlreadyExistsError</code> <p>if the file already exists and mode = \"error_if_exists\".</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; write_file(text=\"Hello\\n\", path=\"test_working_dir/write_file.txt\", mode=\"overwrite\")\n&gt;&gt;&gt; text = read_file(\"test_working_dir/write_file.txt\")\n&gt;&gt;&gt; print(text)\nHello\n\n&gt;&gt;&gt; write_file(text=\"World\\n\", path=\"test_working_dir/write_file.txt\", mode=\"append\")\n&gt;&gt;&gt; text = read_file(\"test_working_dir/write_file.txt\")\n&gt;&gt;&gt; print(text)\nHello\nWorld\n\n&gt;&gt;&gt; write_file(text=\"Never mind\\n\", path=\"test_working_dir/write_file.txt\", mode=\"overwrite\")\n&gt;&gt;&gt; text = read_file(\"test_working_dir/write_file.txt\")\n&gt;&gt;&gt; print(text)\nNever mind\n\n&gt;&gt;&gt; write_file(text=\"Never mind\\n\", path=\"test_working_dir/write_file.txt\", mode=\"error_if_exists\")\nTraceback (most recent call last):\n    ...\nspark_frame.exceptions.FileAlreadyExistsError: The file test_working_dir/write_file.txt already exists.\n&gt;&gt;&gt; write_file(text=\"Never mind\\n\", path=\"test_working_dir/write_file.txt\", mode=\"incorrect_mode\")\nTraceback (most recent call last):\n    ...\nspark_frame.exceptions.IllegalArgumentException: Invalid write mode: incorrect_mode. Accepted modes are: ['overwrite', 'append', 'error_if_exists']\n</code></pre> Source code in <code>spark_frame/filesystem.py</code> <pre><code>def write_file(\n    text: str,\n    path: str,\n    mode: str = MODE_ERROR_IF_EXISTS,\n    encoding: str = \"utf8\",\n) -&gt; None:\n    r\"\"\"Write given text to a file using the `org.apache.hadoop.fs.FileSystem` from Spark's JVM.\n    Depending on how Spark is configured, it can write on any file system supported by Spark.\n    (like \"file://\", \"hdfs://\", \"s3://\", \"gs://\", \"abfs://\", etc.)\n\n    !!! warning\n        This method loads the entirety of the file in memory as a Python str object.\n        It's use should be restricted to writing small files such as configuration files or reports.\n\n        When reading large data files, it is recommended to use the `spark.read` method.\n\n    !!! warning\n        The SparkSession must be instantiated before this method can be used.\n\n    Args:\n        text: The content of the file to write\n        path: The path of the file to write\n        mode: Either one of [\"overwrite\", \"append\", \"error_if_exists\"].\n        encoding: Encoding to use\n\n    Raises:\n        spark_frame.exceptions.SparkSessionNotStarted: if the SparkSession is not started when this method is called.\n        spark_frame.exceptions.IllegalArgumentException: if the mode is incorrect.\n        spark_frame.exceptions.FileAlreadyExistsError: if the file already exists and mode = \"error_if_exists\".\n\n    Examples:\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; write_file(text=\"Hello\\n\", path=\"test_working_dir/write_file.txt\", mode=\"overwrite\")\n        &gt;&gt;&gt; text = read_file(\"test_working_dir/write_file.txt\")\n        &gt;&gt;&gt; print(text)\n        Hello\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; write_file(text=\"World\\n\", path=\"test_working_dir/write_file.txt\", mode=\"append\")\n        &gt;&gt;&gt; text = read_file(\"test_working_dir/write_file.txt\")\n        &gt;&gt;&gt; print(text)\n        Hello\n        World\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; write_file(text=\"Never mind\\n\", path=\"test_working_dir/write_file.txt\", mode=\"overwrite\")\n        &gt;&gt;&gt; text = read_file(\"test_working_dir/write_file.txt\")\n        &gt;&gt;&gt; print(text)\n        Never mind\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; write_file(text=\"Never mind\\n\", path=\"test_working_dir/write_file.txt\", mode=\"error_if_exists\")\n        Traceback (most recent call last):\n            ...\n        spark_frame.exceptions.FileAlreadyExistsError: The file test_working_dir/write_file.txt already exists.\n        &gt;&gt;&gt; write_file(text=\"Never mind\\n\", path=\"test_working_dir/write_file.txt\", mode=\"incorrect_mode\")\n        Traceback (most recent call last):\n            ...\n        spark_frame.exceptions.IllegalArgumentException: Invalid write mode: incorrect_mode. Accepted modes are: ['overwrite', 'append', 'error_if_exists']\n    \"\"\"  # noqa: E501\n    spark = SparkSession.getActiveSession()\n    assert_true(spark is not None, SparkSessionNotStarted())\n\n    java_fs = _get_java_file_system(path, spark)\n    java_path = spark._jvm.org.apache.hadoop.fs.Path(path)  # noqa: SLF001\n\n    if mode.lower() not in VALID_MODES:\n        msg = f\"Invalid write mode: {mode}. Accepted modes are: {VALID_MODES}\"\n        raise IllegalArgumentException(msg)\n    if not java_fs.exists(java_path):\n        stream = java_fs.create(java_path)\n    else:\n        if mode == MODE_ERROR_IF_EXISTS:\n            msg = f\"The file {path} already exists.\"\n            raise FileAlreadyExistsError(msg)\n        if mode == MODE_OVERWRITE:\n            stream = java_fs.create(java_path)\n        elif mode == MODE_APPEND:\n            stream = java_fs.append(java_path)\n    stream.write(text.encode(encoding))\n    stream.flush()\n    stream.close()\n</code></pre>"},{"location":"reference/functions/","title":"spark_frame.functions","text":"<p>Like with pyspark.sql.functions,  the methods in this module all return Column expressions and can be used to build operations on Spark DataFrames using <code>select</code>, <code>withColumn</code>, etc.</p>"},{"location":"reference/functions/#spark_frame.functions.empty_array","title":"<code>empty_array(element_type: Union[DataType, str]) -&gt; Column</code>","text":"<p>Create an empty Spark array column of the specified type. This is a workaround to the Spark method <code>typedLit</code> not being available in PySpark</p> <p>Parameters:</p> Name Type Description Default <code>element_type</code> <code>Union[DataType, str]</code> <p>The type of the array's element</p> required <p>Returns:</p> Type Description <code>Column</code> <p>A Spark Column representing an empty array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; df = spark.sql('''SELECT 1 as a''')\n&gt;&gt;&gt; res = df.withColumn('empty_array', empty_array(\"STRUCT&lt;b: int, c: array&lt;string&gt;&gt;\"))\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- a: integer (nullable = false)\n |-- empty_array: array (nullable = false)\n |    |-- element: struct (containsNull = true)\n |    |    |-- b: integer (nullable = true)\n |    |    |-- c: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n\n&gt;&gt;&gt; res.show()\n+---+-----------+\n|  a|empty_array|\n+---+-----------+\n|  1|         []|\n+---+-----------+\n</code></pre> Source code in <code>spark_frame/functions.py</code> <pre><code>def empty_array(element_type: Union[DataType, str]) -&gt; Column:\n    \"\"\"Create an empty Spark array column of the specified type.\n    This is a workaround to the Spark method `typedLit` not being available in PySpark\n\n    Args:\n        element_type: The type of the array's element\n\n    Returns:\n        A Spark Column representing an empty array.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; df = spark.sql('''SELECT 1 as a''')\n        &gt;&gt;&gt; res = df.withColumn('empty_array', empty_array(\"STRUCT&lt;b: int, c: array&lt;string&gt;&gt;\"))\n        &gt;&gt;&gt; res.printSchema()\n        root\n         |-- a: integer (nullable = false)\n         |-- empty_array: array (nullable = false)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- b: integer (nullable = true)\n         |    |    |-- c: array (nullable = true)\n         |    |    |    |-- element: string (containsNull = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res.show()\n        +---+-----------+\n        |  a|empty_array|\n        +---+-----------+\n        |  1|         []|\n        +---+-----------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.array_except(f.array(f.lit(None).cast(element_type)), f.array(f.lit(None)))\n</code></pre>"},{"location":"reference/functions/#spark_frame.functions.generic_struct","title":"<code>generic_struct(*columns: str, col_name_alias: str = 'name', col_value_alias: str = 'value') -&gt; Column</code>","text":"<p>Transform a set of columns into a generic array of struct of type ARRAY (column_name -&gt; column_value) <p>Parameters:</p> Name Type Description Default <code>*columns</code> <code>str</code> <p>One or multiple column names to add to the generic struct</p> <code>()</code> <code>col_name_alias</code> <code>str</code> <p>Alias of the field containing the column names in the returned struct</p> <code>'name'</code> <code>col_value_alias</code> <code>str</code> <p>Alias of the field containing the column values in the returned struct</p> <code>'value'</code> <p>Returns:</p> Type Description <code>Column</code> <p>A Spark Column</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...       col1 as `pokemon.id`,\n...       col2 as `pokemon.name`,\n...       col3 as `pokemon.types`\n...     FROM VALUES\n...       (4, 'Charmander', ARRAY(NAMED_STRUCT('type', 'Fire'))),\n...       (5, 'Charmeleon', ARRAY(NAMED_STRUCT('type', 'Fire'))),\n...       (6, 'Charizard',  ARRAY(NAMED_STRUCT('type', 'Fire'), NAMED_STRUCT('type', 'Flying')))\n... ''')\n&gt;&gt;&gt; df.show()\n+----------+------------+------------------+\n|pokemon.id|pokemon.name|     pokemon.types|\n+----------+------------+------------------+\n|         4|  Charmander|          [{Fire}]|\n|         5|  Charmeleon|          [{Fire}]|\n|         6|   Charizard|[{Fire}, {Flying}]|\n+----------+------------+------------------+\n\n&gt;&gt;&gt; res = df.select(generic_struct(\"pokemon.id\", \"pokemon.name\", \"pokemon.types\").alias('values'))\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- values: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- name: string (nullable = false)\n |    |    |-- value: string (nullable = false)\n\n&gt;&gt;&gt; res.show(10, False)\n+---------------------------------------------------------------------------------+\n|values                                                                           |\n+---------------------------------------------------------------------------------+\n|[{pokemon.id, 4}, {pokemon.name, Charmander}, {pokemon.types, [{Fire}]}]         |\n|[{pokemon.id, 5}, {pokemon.name, Charmeleon}, {pokemon.types, [{Fire}]}]         |\n|[{pokemon.id, 6}, {pokemon.name, Charizard}, {pokemon.types, [{Fire}, {Flying}]}]|\n+---------------------------------------------------------------------------------+\n</code></pre> Source code in <code>spark_frame/functions.py</code> <pre><code>def generic_struct(*columns: str, col_name_alias: str = \"name\", col_value_alias: str = \"value\") -&gt; Column:\n    \"\"\"Transform a set of columns into a generic array of struct of type ARRAY&lt;STRUCT&lt;name: STRING, value: STRING&gt;\n    (column_name -&gt; column_value)\n\n    Args:\n        *columns: One or multiple column names to add to the generic struct\n        col_name_alias: Alias of the field containing the column names in the returned struct\n        col_value_alias: Alias of the field containing the column values in the returned struct\n\n    Returns:\n        A Spark Column\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...       col1 as `pokemon.id`,\n        ...       col2 as `pokemon.name`,\n        ...       col3 as `pokemon.types`\n        ...     FROM VALUES\n        ...       (4, 'Charmander', ARRAY(NAMED_STRUCT('type', 'Fire'))),\n        ...       (5, 'Charmeleon', ARRAY(NAMED_STRUCT('type', 'Fire'))),\n        ...       (6, 'Charizard',  ARRAY(NAMED_STRUCT('type', 'Fire'), NAMED_STRUCT('type', 'Flying')))\n        ... ''')\n        &gt;&gt;&gt; df.show()\n        +----------+------------+------------------+\n        |pokemon.id|pokemon.name|     pokemon.types|\n        +----------+------------+------------------+\n        |         4|  Charmander|          [{Fire}]|\n        |         5|  Charmeleon|          [{Fire}]|\n        |         6|   Charizard|[{Fire}, {Flying}]|\n        +----------+------------+------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res = df.select(generic_struct(\"pokemon.id\", \"pokemon.name\", \"pokemon.types\").alias('values'))\n        &gt;&gt;&gt; res.printSchema()\n        root\n         |-- values: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- name: string (nullable = false)\n         |    |    |-- value: string (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res.show(10, False)\n        +---------------------------------------------------------------------------------+\n        |values                                                                           |\n        +---------------------------------------------------------------------------------+\n        |[{pokemon.id, 4}, {pokemon.name, Charmander}, {pokemon.types, [{Fire}]}]         |\n        |[{pokemon.id, 5}, {pokemon.name, Charmeleon}, {pokemon.types, [{Fire}]}]         |\n        |[{pokemon.id, 6}, {pokemon.name, Charizard}, {pokemon.types, [{Fire}, {Flying}]}]|\n        +---------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.array(\n        *[\n            f.struct(f.lit(c).alias(col_name_alias), f.col(quote(c)).astype(StringType()).alias(col_value_alias))\n            for c in columns\n        ],\n    )\n</code></pre>"},{"location":"reference/functions/#spark_frame.functions.nullable","title":"<code>nullable(col: Column) -&gt; Column</code>","text":"<p>Make a <code>pyspark.sql.Column</code> nullable. This is especially useful for literal which are always non-nullable by default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\"))\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- a: integer (nullable = false)\n |-- b: string (nullable = false)\n\n&gt;&gt;&gt; res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b')))\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: string (nullable = true)\n</code></pre> Source code in <code>spark_frame/functions.py</code> <pre><code>def nullable(col: Column) -&gt; Column:\n    \"\"\"Make a `pyspark.sql.Column` nullable.\n    This is especially useful for literal which are always non-nullable by default.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\"))\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- a: integer (nullable = false)\n         |-- b: string (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b')))\n        &gt;&gt;&gt; res.printSchema()\n        root\n         |-- a: integer (nullable = true)\n         |-- b: string (nullable = true)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when(~col.isNull(), col)\n</code></pre>"},{"location":"reference/graph/","title":"spark_frame.graph","text":"<p>This module contains implementations of graph algorithms and related methods.</p>"},{"location":"reference/graph/#spark_frame.graph_impl.ascending_forest_traversal.ascending_forest_traversal","title":"<code>ascending_forest_traversal(input_df: DataFrame, node_id: str, parent_id: str, keep_labels: bool = False) -&gt; DataFrame</code>","text":"<p>Given a DataFrame representing a labeled forest with columns <code>id</code>, <code>parent_id</code> and other label columns, performs a graph traversal that will return a DataFrame with the same schema that gives for each node the labels of it's furthest ancestor.</p> <p>In the input DataFrame, a node is considered to have no parent if its parent_id is null or equal to its node_id. In the output DataFrame, a node that has no parent will have its parent_id equal to its node_id. Cycle protection: If the graph contains any cycle, the nodes in that cycle will have a NULL parent_id.</p> <p>It has a security against dependency cycles, but no security preventing a combinatorial explosion if some nodes have more than one parent.</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>node_id</code> <code>str</code> <p>Name of the column that represent the node's ids</p> required <code>parent_id</code> <code>str</code> <p>Name of the column that represent the parent node's ids</p> required <code>keep_labels</code> <code>bool</code> <p>If set to true, add two structs column called \"node\" and \"furthest_ancestor\" containing the content of the row from the input DataFrame for the corresponding nodes and their furthest ancestor</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with two columns named according to <code>node_id</code> and <code>parent_id</code> that gives for each node</p> <code>DataFrame</code> <p>the id of it's furthest ancestor (in the <code>parent_id</code> column).</p> <code>DataFrame</code> <p>If the option <code>keep_labels</code> is used, two extra columns of type STRUCT are a added to the output DataFrame,</p> <code>DataFrame</code> <p>they represent the content of the rows in the input DataFrame corresponding to the node and its furthest</p> <code>DataFrame</code> <p>ancestor, respectively.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n</code></pre> <p>Given a DataFrame with pokemon attributes and evolution links</p> <pre><code>&gt;&gt;&gt; input_df = spark.sql('''\n...     SELECT\n...       col1 as `pokemon.id`,\n...       col2 as `pokemon.evolve_to_id`,\n...       col3 as `pokemon.name`,\n...       col4 as `pokemon.types`\n...     FROM VALUES\n...       (4, 5, 'Charmander', ARRAY('Fire')),\n...       (5, 6, 'Charmeleon', ARRAY('Fire')),\n...       (6, NULL, 'Charizard', ARRAY('Fire', 'Flying'))\n... ''')\n&gt;&gt;&gt; input_df.show()\n+----------+--------------------+------------+--------------+\n|pokemon.id|pokemon.evolve_to_id|pokemon.name| pokemon.types|\n+----------+--------------------+------------+--------------+\n|         4|                   5|  Charmander|        [Fire]|\n|         5|                   6|  Charmeleon|        [Fire]|\n|         6|                NULL|   Charizard|[Fire, Flying]|\n+----------+--------------------+------------+--------------+\n</code></pre> <p>We compute a DataFrame that for each pokemon.id gives the attributes of its highest level of evolution</p> <pre><code>&gt;&gt;&gt; ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\").orderBy(\"`pokemon.id`\").show()\n+----------+--------------------+\n|pokemon.id|pokemon.evolve_to_id|\n+----------+--------------------+\n|         4|                   6|\n|         5|                   6|\n|         6|                   6|\n+----------+--------------------+\n</code></pre> <p>With the <code>keep_label</code> option extra joins are performed at the end of the algorithm to add two struct columns containing the corresponding row for the original node and the furthest ancestor.</p> <pre><code>&gt;&gt;&gt; ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\", keep_labels=True\n...     ).orderBy(\"`pokemon.id`\").show(10, False)\n+----------+--------------------+------------------------------------+------------------------------------+\n|pokemon.id|pokemon.evolve_to_id|node                                |furthest_ancestor                   |\n+----------+--------------------+------------------------------------+------------------------------------+\n|4         |6                   |{4, 5, Charmander, [Fire]}          |{6, NULL, Charizard, [Fire, Flying]}|\n|5         |6                   |{5, 6, Charmeleon, [Fire]}          |{6, NULL, Charizard, [Fire, Flying]}|\n|6         |6                   |{6, NULL, Charizard, [Fire, Flying]}|{6, NULL, Charizard, [Fire, Flying]}|\n+----------+--------------------+------------------------------------+------------------------------------+\n</code></pre> <p>Cycle Protection: to prevent the algorithm from looping indefinitely, cycles are detected, and the nodes that are part of cycles will end up with a NULL value as their furthest ancestor</p> <pre><code>&gt;&gt;&gt; input_df = spark.sql('''\n...     SELECT\n...       col1 as `node_id`,\n...       col2 as `parent_id`\n...     FROM VALUES (1, 2), (2, 3), (3, 1)\n... ''')\n&gt;&gt;&gt; input_df.show()\n+-------+---------+\n|node_id|parent_id|\n+-------+---------+\n|      1|        2|\n|      2|        3|\n|      3|        1|\n+-------+---------+\n\n&gt;&gt;&gt; ascending_forest_traversal(input_df, \"node_id\", \"parent_id\").orderBy(\"node_id\").show()\n+-------+---------+\n|node_id|parent_id|\n+-------+---------+\n|      1|     NULL|\n|      2|     NULL|\n|      3|     NULL|\n+-------+---------+\n</code></pre> Source code in <code>spark_frame/graph_impl/ascending_forest_traversal.py</code> <pre><code>def ascending_forest_traversal(\n    input_df: DataFrame,\n    node_id: str,\n    parent_id: str,\n    keep_labels: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Given a DataFrame representing a labeled forest with columns `id`, `parent_id` and other label columns,\n    performs a graph traversal that will return a DataFrame with the same schema that gives for each node\n    the labels of it's furthest ancestor.\n\n    In the input DataFrame, a node is considered to have no parent if its parent_id is null or equal to its node_id.\n    In the output DataFrame, a node that has no parent will have its parent_id equal to its node_id.\n    Cycle protection: If the graph contains any cycle, the nodes in that cycle will have a NULL parent_id.\n\n    It has a security against dependency cycles, but no security preventing\n    a combinatorial explosion if some nodes have more than one parent.\n\n    Args:\n        input_df: A Spark DataFrame\n        node_id: Name of the column that represent the node's ids\n        parent_id: Name of the column that represent the parent node's ids\n        keep_labels: If set to true, add two structs column called \"node\" and \"furthest_ancestor\" containing\n            the content of the row from the input DataFrame for the corresponding nodes and their furthest ancestor\n\n    Returns:\n        A DataFrame with two columns named according to `node_id` and `parent_id` that gives for each node\n        the id of it's furthest ancestor (in the `parent_id` column).\n        If the option `keep_labels` is used, two extra columns of type STRUCT are a added to the output DataFrame,\n        they represent the content of the rows in the input DataFrame corresponding to the node and its furthest\n        ancestor, respectively.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n\n        Given a DataFrame with pokemon attributes and evolution links\n\n        &gt;&gt;&gt; input_df = spark.sql('''\n        ...     SELECT\n        ...       col1 as `pokemon.id`,\n        ...       col2 as `pokemon.evolve_to_id`,\n        ...       col3 as `pokemon.name`,\n        ...       col4 as `pokemon.types`\n        ...     FROM VALUES\n        ...       (4, 5, 'Charmander', ARRAY('Fire')),\n        ...       (5, 6, 'Charmeleon', ARRAY('Fire')),\n        ...       (6, NULL, 'Charizard', ARRAY('Fire', 'Flying'))\n        ... ''')\n        &gt;&gt;&gt; input_df.show()\n        +----------+--------------------+------------+--------------+\n        |pokemon.id|pokemon.evolve_to_id|pokemon.name| pokemon.types|\n        +----------+--------------------+------------+--------------+\n        |         4|                   5|  Charmander|        [Fire]|\n        |         5|                   6|  Charmeleon|        [Fire]|\n        |         6|                NULL|   Charizard|[Fire, Flying]|\n        +----------+--------------------+------------+--------------+\n        &lt;BLANKLINE&gt;\n\n        We compute a DataFrame that for each pokemon.id gives the attributes of its highest level of evolution\n\n        &gt;&gt;&gt; ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\").orderBy(\"`pokemon.id`\").show()\n        +----------+--------------------+\n        |pokemon.id|pokemon.evolve_to_id|\n        +----------+--------------------+\n        |         4|                   6|\n        |         5|                   6|\n        |         6|                   6|\n        +----------+--------------------+\n        &lt;BLANKLINE&gt;\n\n        With the `keep_label` option extra joins are performed at the end of the algorithm to add two struct columns\n        containing the corresponding row for the original node and the furthest ancestor.\n\n        &gt;&gt;&gt; ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\", keep_labels=True\n        ...     ).orderBy(\"`pokemon.id`\").show(10, False)\n        +----------+--------------------+------------------------------------+------------------------------------+\n        |pokemon.id|pokemon.evolve_to_id|node                                |furthest_ancestor                   |\n        +----------+--------------------+------------------------------------+------------------------------------+\n        |4         |6                   |{4, 5, Charmander, [Fire]}          |{6, NULL, Charizard, [Fire, Flying]}|\n        |5         |6                   |{5, 6, Charmeleon, [Fire]}          |{6, NULL, Charizard, [Fire, Flying]}|\n        |6         |6                   |{6, NULL, Charizard, [Fire, Flying]}|{6, NULL, Charizard, [Fire, Flying]}|\n        +----------+--------------------+------------------------------------+------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        *Cycle Protection:* to prevent the algorithm from looping indefinitely, cycles are detected, and the nodes\n        that are part of cycles will end up with a NULL value as their furthest ancestor\n\n        &gt;&gt;&gt; input_df = spark.sql('''\n        ...     SELECT\n        ...       col1 as `node_id`,\n        ...       col2 as `parent_id`\n        ...     FROM VALUES (1, 2), (2, 3), (3, 1)\n        ... ''')\n        &gt;&gt;&gt; input_df.show()\n        +-------+---------+\n        |node_id|parent_id|\n        +-------+---------+\n        |      1|        2|\n        |      2|        3|\n        |      3|        1|\n        +-------+---------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; ascending_forest_traversal(input_df, \"node_id\", \"parent_id\").orderBy(\"node_id\").show()\n        +-------+---------+\n        |node_id|parent_id|\n        +-------+---------+\n        |      1|     NULL|\n        |      2|     NULL|\n        |      3|     NULL|\n        +-------+---------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    assert_true(\n        node_id in input_df.columns,\n        \"Could not find column %s in Dataframe's columns: %s\" % (node_id, input_df.columns),\n    )\n    assert_true(\n        parent_id in input_df.columns,\n        \"Could not find column %s in Dataframe's columns: %s\" % (parent_id, input_df.columns),\n    )\n    node_id_col_name = \"node_id\"\n    parent_id_col_name = \"parent_id\"\n    status_col_name = \"status\"\n    df = input_df.select(\n        f.col(quote(node_id)).alias(node_id_col_name),\n        f.col(quote(parent_id)).alias(parent_id_col_name),\n    )\n\n    res_df = _ascending_forest_traversal(\n        df,\n        node_id_col_name=node_id_col_name,\n        parent_id_col_name=parent_id_col_name,\n        status_col_name=status_col_name,\n    )\n    res_df = res_df.select(\n        f.col(node_id_col_name).alias(node_id),\n        f.col(parent_id_col_name).alias(parent_id),\n    )\n\n    if keep_labels:\n        res_df = res_df.join(input_df, node_id).select(\n            res_df[\"*\"],\n            f.struct(*[input_df[quote(col)] for col in input_df.columns]).alias(\"node\"),\n        )\n        res_df = res_df.join(input_df, res_df[quote(parent_id)] == input_df[quote(node_id)]).select(\n            res_df[quote(node_id)],\n            res_df[quote(parent_id)],\n            res_df[\"node\"],\n            f.struct(*[input_df[quote(col)] for col in input_df.columns]).alias(\"furthest_ancestor\"),\n        )\n\n    return res_df\n</code></pre>"},{"location":"reference/nested/","title":"spark_frame.nested","text":""},{"location":"reference/nested/#please-read-this-before-using-the-spark_framenested-module","title":"Please read this before using the <code>spark_frame.nested</code> module","text":"<p>The <code>spark_frame.nested</code> module contains several methods that make the manipulation of deeply nested data structures  much easier. Before diving into it, it is important to explicit the concept of <code>Field</code> in the context of this library.</p> <p>First, let's distinguish the notion of <code>Column</code> and <code>Field</code>. Both terms are already used in Spark, but we chose here to make the following distinction:</p> <ul> <li>A <code>Column</code> is a root-level column of a DataFrame.</li> <li>A <code>Field</code> is any column or sub-column inside a struct of the DataFrame.</li> </ul> <p>Example: let's consider the following DataFrame</p> <pre><code>&gt;&gt;&gt; from spark_frame.examples.reference_nested import _get_sample_data\n&gt;&gt;&gt; df = _get_sample_data()\n&gt;&gt;&gt; df.show(truncate=False)  # noqa: E501\n+---+-----------------------+---------------+\n|id |name                   |types          |\n+---+-----------------------+---------------+\n|1  |{Bulbasaur, Bulbizarre}|[Grass, Poison]|\n+---+-----------------------+---------------+\n\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- name: struct (nullable = false)\n |    |-- english: string (nullable = false)\n |    |-- french: string (nullable = false)\n |-- types: array (nullable = false)\n |    |-- element: string (containsNull = false)\n</code></pre> <p>This DataFrame has 3 columns:</p> <pre><code>id\nname\ntypes\n</code></pre> <p>But it has 4 fields:</p> <pre><code>id\nname.english\nname.french\ntypes!\n</code></pre> <p>This can be seen by using the method <code>spark_frame.nested.print_schema</code></p> <pre><code>&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- name.english: string (nullable = false)\n |-- name.french: string (nullable = false)\n |-- types!: string (nullable = false)\n</code></pre> <p>As we can see, some field names contain dots <code>.</code> or exclamation marks <code>!</code>, they convey the following meaning:</p> <ul> <li>A dot <code>.</code> represents a struct.</li> <li>An exclamation mark <code>!</code> represents an array.</li> </ul> <p>While the dot syntax for structs should feel familiar to users, the exclamation mark <code>!</code> should feel new. It is used as a repetition marker indicating that this field is repeated.</p> <p>Tip</p> <p>It is important to not forget to use exclamation marks <code>!</code> when mentionning a field. For instance:</p> <ul> <li><code>types</code> designates the root-level field which is of type <code>ARRAY&lt;STRING&gt;</code></li> <li><code>types!</code> designates the elements inside this array</li> </ul> <p>In particular, if a field <code>\"my_field\"</code> is of type <code>ARRAY&lt;ARRAY&lt;STRING&gt;&gt;</code>, the innermost elements of the arrays will be designated as <code>\"my_field!!\"</code> with two exclamation marks.</p> <p>Limitation: Do not use dots, exclamation marks or percents in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, exclamation mark <code>!</code> or percents <code>%</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p>"},{"location":"reference/nested/#spark_frame.nested_impl.print_schema.print_schema","title":"<code>print_schema(df: DataFrame) -&gt; None</code>","text":"<p>Print the DataFrame's flattened schema to the standard output.</p> <ul> <li>Structs are flattened with a <code>.</code> after their name.</li> <li>Arrays are flattened with a <code>!</code> character after their name.</li> <li>Maps are flattened with a <code>%key</code> and '%value' after their name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     1 as id,\n...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n...     STRUCT(7 as f) as s2,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4,\n...     MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1\n... ''')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s1: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- a: integer (nullable = false)\n |    |    |-- b: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- c: integer (nullable = false)\n |    |    |    |    |-- d: integer (nullable = false)\n |    |    |-- e: array (nullable = false)\n |    |    |    |-- element: integer (containsNull = false)\n |-- s2: struct (nullable = false)\n |    |-- f: integer (nullable = false)\n |-- s3: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: integer (containsNull = false)\n |-- s4: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- e: integer (nullable = false)\n |    |    |    |-- f: integer (nullable = false)\n |-- m1: map (nullable = false)\n |    |-- key: struct\n |    |    |-- a: integer (nullable = false)\n |    |-- value: struct (valueContainsNull = false)\n |    |    |-- b: integer (nullable = false)\n\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s1!.b!.c: integer (nullable = false)\n |-- s1!.b!.d: integer (nullable = false)\n |-- s1!.e!: integer (nullable = false)\n |-- s2.f: integer (nullable = false)\n |-- s3!!: integer (nullable = false)\n |-- s4!!.e: integer (nullable = false)\n |-- s4!!.f: integer (nullable = false)\n |-- m1%key.a: integer (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n</code></pre> Source code in <code>spark_frame/nested_impl/print_schema.py</code> <pre><code>def print_schema(df: DataFrame) -&gt; None:\n    \"\"\"Print the DataFrame's flattened schema to the standard output.\n\n    - Structs are flattened with a `.` after their name.\n    - Arrays are flattened with a `!` character after their name.\n    - Maps are flattened with a `%key` and '%value' after their name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     1 as id,\n        ...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n        ...     STRUCT(7 as f) as s2,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n        ...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4,\n        ...     MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1\n        ... ''')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s1: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- a: integer (nullable = false)\n         |    |    |-- b: array (nullable = false)\n         |    |    |    |-- element: struct (containsNull = false)\n         |    |    |    |    |-- c: integer (nullable = false)\n         |    |    |    |    |-- d: integer (nullable = false)\n         |    |    |-- e: array (nullable = false)\n         |    |    |    |-- element: integer (containsNull = false)\n         |-- s2: struct (nullable = false)\n         |    |-- f: integer (nullable = false)\n         |-- s3: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: integer (containsNull = false)\n         |-- s4: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: struct (containsNull = false)\n         |    |    |    |-- e: integer (nullable = false)\n         |    |    |    |-- f: integer (nullable = false)\n         |-- m1: map (nullable = false)\n         |    |-- key: struct\n         |    |    |-- a: integer (nullable = false)\n         |    |-- value: struct (valueContainsNull = false)\n         |    |    |-- b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s1!.b!.c: integer (nullable = false)\n         |-- s1!.b!.d: integer (nullable = false)\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2.f: integer (nullable = false)\n         |-- s3!!: integer (nullable = false)\n         |-- s4!!.e: integer (nullable = false)\n         |-- s4!!.f: integer (nullable = false)\n         |-- m1%key.a: integer (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    print(schema_string(df))\n</code></pre>"},{"location":"reference/nested/#spark_frame.nested_impl.select_impl.select","title":"<code>select(df: DataFrame, fields: Mapping[str, ColumnTransformation]) -&gt; DataFrame</code>","text":"<p>Project a set of expressions and returns a new DataFrame.</p> <p>This method is similar to the DataFrame.select method, with the extra capability of working on nested and repeated fields (structs and arrays).</p> <p>The syntax for field names works as follows:</p> <ul> <li>\".\" is the separator for struct elements</li> <li>\"!\" must be appended at the end of fields that are repeated (arrays)</li> <li>Map keys are appended with <code>%key</code></li> <li>Map values are appended with <code>%value</code></li> </ul> <p>The following types of transformation are allowed:</p> <ul> <li>String and column expressions can be used on any non-repeated field, even nested ones.</li> <li>When working on repeated fields, transformations must be expressed as higher order functions   (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,   but their value will be repeated multiple times.</li> <li>When working on multiple levels of nested arrays, higher order functions may take multiple arguments,   corresponding to each level of repetition (See Example 5.).</li> <li><code>None</code> can also be used to represent the identity transformation, this is useful to select a field without    changing and without having to repeat its name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>fields</code> <code>Mapping[str, ColumnTransformation]</code> <p>A Dict(field_name, transformation_to_apply)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame where only the specified field have been selected and the corresponding</p> <code>DataFrame</code> <p>transformations were applied to each of them.</p> <p>Example 1: non-repeated fields</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s: struct (nullable = false)\n |    |-- a: integer (nullable = false)\n |    |-- b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+------+\n| id|     s|\n+---+------+\n|  1|{2, 3}|\n+---+------+\n</code></pre> <p>Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected)</p> <pre><code>&gt;&gt;&gt; new_df = nested.select(df, {\n...     \"s.a\": \"s.a\",                        # Column name (string)\n...     \"s.b\": None,                         # None: use to keep a column without having to repeat its name\n...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")   # Column expression\n... })\n&gt;&gt;&gt; new_df.printSchema()\nroot\n |-- s: struct (nullable = false)\n |    |-- a: integer (nullable = false)\n |    |-- b: integer (nullable = false)\n |    |-- c: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---------+\n|        s|\n+---------+\n|{2, 3, 5}|\n+---------+\n</code></pre> <p>Example 2: repeated fields</p> <pre><code>&gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.a: integer (nullable = false)\n |-- s!.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+----------------+\n| id|               s|\n+---+----------------+\n|  1|[{1, 2}, {3, 4}]|\n+---+----------------+\n</code></pre> <p>Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.select, {\n...     \"s!.a\": lambda s: s[\"a\"],\n...     \"s!.b\": None,\n...     \"s!.c\": lambda s: s[\"a\"] + s[\"b\"]\n... }).show(truncate=False)\n+----------------------+\n|s                     |\n+----------------------+\n|[{1, 2, 3}, {3, 4, 7}]|\n+----------------------+\n</code></pre> <p>String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.select, {\n...     \"id\": None,\n...     \"s!.a\": \"id\",\n...     \"s!.b\": f.lit(2)\n... }).show(truncate=False)\n+---+----------------+\n|id |s               |\n+---+----------------+\n|1  |[{1, 2}, {1, 2}]|\n+---+----------------+\n</code></pre> <p>Example 3: field repeated twice</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1,\n...         ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.e!: integer (nullable = false)\n |-- s2!.e!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+-------------+\n| id|           s1|           s2|\n+---+-------------+-------------+\n|  1|[{[1, 2, 3]}]|[{[4, 5, 6]}]|\n+---+-------------+-------------+\n</code></pre> <p>Here, the lambda expression will be applied to the last repeated element <code>e</code>.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"s1!.e!\": None,\n...  \"s2!.e!\": lambda e : e.cast(\"DOUBLE\")\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- s1!.e!: integer (nullable = false)\n |-- s2!.e!: double (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+-------------+-------------------+\n|           s1|                 s2|\n+-------------+-------------------+\n|[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]|\n+-------------+-------------------+\n</code></pre> <p>Example 4: Dataframe with maps</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: integer (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+\n| id|           m1|\n+---+-------------+\n|  1|{a -&gt; {2, 3}}|\n+---+-------------+\n</code></pre> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"id\": None,\n...  \"m1%key\": lambda key : f.upper(key),\n...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: double (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---+------------+\n| id|          m1|\n+---+------------+\n|  1|{A -&gt; {2.0}}|\n+---+------------+\n</code></pre> <p>Example 5: Accessing multiple repetition levels</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(\n...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n...         ) as s1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.average: integer (nullable = false)\n |-- s1!.values!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+---+--------------------------------------+\n|id |s1                                    |\n+---+--------------------------------------+\n|1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n+---+--------------------------------------+\n</code></pre> <p>Here, the transformation applied to \"s1!.values!\" takes two arguments.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"id\": None,\n...  \"s1!.average\": None,\n...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+-----------------------------------------+\n|id |s1                                       |\n+---+-----------------------------------------+\n|1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n+---+-----------------------------------------+\n</code></pre> <p>Extra arguments can be added to the left for each repetition level, up to the root level.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"id\": None,\n...  \"s1!.average\": None,\n...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+---------------------------------------+\n|id |s1                                     |\n+---+---------------------------------------+\n|1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n+---+---------------------------------------+\n</code></pre> Source code in <code>spark_frame/nested_impl/select_impl.py</code> <pre><code>def select(df: DataFrame, fields: Mapping[str, ColumnTransformation]) -&gt; DataFrame:\n    \"\"\"Project a set of expressions and returns a new [DataFrame][pyspark.sql.DataFrame].\n\n    This method is similar to the [DataFrame.select][pyspark.sql.DataFrame.select] method, with the extra\n    capability of working on nested and repeated fields (structs and arrays).\n\n    The syntax for field names works as follows:\n\n    - \".\" is the separator for struct elements\n    - \"!\" must be appended at the end of fields that are repeated (arrays)\n    - Map keys are appended with `%key`\n    - Map values are appended with `%value`\n\n    The following types of transformation are allowed:\n\n    - String and column expressions can be used on any non-repeated field, even nested ones.\n    - When working on repeated fields, transformations must be expressed as higher order functions\n      (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,\n      but their value will be repeated multiple times.\n    - When working on multiple levels of nested arrays, higher order functions may take multiple arguments,\n      corresponding to each level of repetition (See Example 5.).\n    - `None` can also be used to represent the identity transformation, this is useful to select a field without\n       changing and without having to repeat its name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n        fields: A Dict(field_name, transformation_to_apply)\n\n    Returns:\n        A new DataFrame where only the specified field have been selected and the corresponding\n        transformations were applied to each of them.\n\n    Examples: Example 1: non-repeated fields\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s: struct (nullable = false)\n         |    |-- a: integer (nullable = false)\n         |    |-- b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+------+\n        | id|     s|\n        +---+------+\n        |  1|{2, 3}|\n        +---+------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on non-repeated fields may be expressed as a string representing a column name,\n        a Column expression or None.\n        (In this example the column \"id\" will be dropped because it was not selected)\n        &gt;&gt;&gt; new_df = nested.select(df, {\n        ...     \"s.a\": \"s.a\",                        # Column name (string)\n        ...     \"s.b\": None,                         # None: use to keep a column without having to repeat its name\n        ...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")   # Column expression\n        ... })\n        &gt;&gt;&gt; new_df.printSchema()\n        root\n         |-- s: struct (nullable = false)\n         |    |-- a: integer (nullable = false)\n         |    |-- b: integer (nullable = false)\n         |    |-- c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---------+\n        |        s|\n        +---------+\n        |{2, 3, 5}|\n        +---------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2: repeated fields\n        &gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.a: integer (nullable = false)\n         |-- s!.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+----------------+\n        | id|               s|\n        +---+----------------+\n        |  1|[{1, 2}, {3, 4}]|\n        +---+----------------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on repeated fields must be expressed as higher-order\n        functions (lambda expressions or named functions).\n        The value passed to this function will correspond to the last repeated element.\n        &gt;&gt;&gt; df.transform(nested.select, {\n        ...     \"s!.a\": lambda s: s[\"a\"],\n        ...     \"s!.b\": None,\n        ...     \"s!.c\": lambda s: s[\"a\"] + s[\"b\"]\n        ... }).show(truncate=False)\n        +----------------------+\n        |s                     |\n        +----------------------+\n        |[{1, 2, 3}, {3, 4, 7}]|\n        +----------------------+\n        &lt;BLANKLINE&gt;\n\n        String and column expressions can be used on repeated fields as well,\n        but their value will be repeated multiple times.\n        &gt;&gt;&gt; df.transform(nested.select, {\n        ...     \"id\": None,\n        ...     \"s!.a\": \"id\",\n        ...     \"s!.b\": f.lit(2)\n        ... }).show(truncate=False)\n        +---+----------------+\n        |id |s               |\n        +---+----------------+\n        |1  |[{1, 2}, {1, 2}]|\n        +---+----------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 3: field repeated twice\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1,\n        ...         ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2!.e!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+-------------+\n        | id|           s1|           s2|\n        +---+-------------+-------------+\n        |  1|[{[1, 2, 3]}]|[{[4, 5, 6]}]|\n        +---+-------------+-------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the lambda expression will be applied to the last repeated element `e`.\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"s1!.e!\": None,\n        ...  \"s2!.e!\": lambda e : e.cast(\"DOUBLE\")\n        ... })\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2!.e!: double (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +-------------+-------------------+\n        |           s1|                 s2|\n        +-------------+-------------------+\n        |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]|\n        +-------------+-------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 4: Dataframe with maps\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: integer (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+\n        | id|           m1|\n        +---+-------------+\n        |  1|{a -&gt; {2, 3}}|\n        +---+-------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"id\": None,\n        ...  \"m1%key\": lambda key : f.upper(key),\n        ...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n        ... })\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: double (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---+------------+\n        | id|          m1|\n        +---+------------+\n        |  1|{A -&gt; {2.0}}|\n        +---+------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 5: Accessing multiple repetition levels\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(\n        ...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n        ...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n        ...         ) as s1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.average: integer (nullable = false)\n         |-- s1!.values!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+--------------------------------------+\n        |id |s1                                    |\n        +---+--------------------------------------+\n        |1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n        +---+--------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the transformation applied to \"s1!.values!\" takes two arguments.\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"id\": None,\n        ...  \"s1!.average\": None,\n        ...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+-----------------------------------------+\n        |id |s1                                       |\n        +---+-----------------------------------------+\n        |1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n        +---+-----------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Extra arguments can be added to the left for each repetition level, up to the root level.\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"id\": None,\n        ...  \"s1!.average\": None,\n        ...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+---------------------------------------+\n        |id |s1                                     |\n        +---+---------------------------------------+\n        |1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n        +---+---------------------------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return df.select(*resolve_nested_fields(fields, starting_level=df))\n</code></pre>"},{"location":"reference/nested/#spark_frame.nested_impl.schema_string.schema_string","title":"<code>schema_string(df: DataFrame) -&gt; str</code>","text":"<p>Write the DataFrame's flattened schema to a string.</p> <ul> <li>Structs are flattened with a <code>.</code> after their name.</li> <li>Arrays are flattened with a <code>!</code> character after their name.</li> <li>Maps are flattened with a <code>%key</code> and '%value' after their name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Type Description <code>str</code> <p>a string representing the flattened schema</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     1 as id,\n...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n...     STRUCT(7 as f) as s2,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4\n... ''')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s1: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- a: integer (nullable = false)\n |    |    |-- b: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- c: integer (nullable = false)\n |    |    |    |    |-- d: integer (nullable = false)\n |    |    |-- e: array (nullable = false)\n |    |    |    |-- element: integer (containsNull = false)\n |-- s2: struct (nullable = false)\n |    |-- f: integer (nullable = false)\n |-- s3: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: integer (containsNull = false)\n |-- s4: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- e: integer (nullable = false)\n |    |    |    |-- f: integer (nullable = false)\n\n&gt;&gt;&gt; print(nested.schema_string(df))\nroot\n |-- id: integer (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s1!.b!.c: integer (nullable = false)\n |-- s1!.b!.d: integer (nullable = false)\n |-- s1!.e!: integer (nullable = false)\n |-- s2.f: integer (nullable = false)\n |-- s3!!: integer (nullable = false)\n |-- s4!!.e: integer (nullable = false)\n |-- s4!!.f: integer (nullable = false)\n</code></pre> Source code in <code>spark_frame/nested_impl/schema_string.py</code> <pre><code>def schema_string(df: DataFrame) -&gt; str:\n    \"\"\"Write the DataFrame's flattened schema to a string.\n\n    - Structs are flattened with a `.` after their name.\n    - Arrays are flattened with a `!` character after their name.\n    - Maps are flattened with a `%key` and '%value' after their name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n\n    Returns:\n        a string representing the flattened schema\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     1 as id,\n        ...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n        ...     STRUCT(7 as f) as s2,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n        ...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4\n        ... ''')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s1: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- a: integer (nullable = false)\n         |    |    |-- b: array (nullable = false)\n         |    |    |    |-- element: struct (containsNull = false)\n         |    |    |    |    |-- c: integer (nullable = false)\n         |    |    |    |    |-- d: integer (nullable = false)\n         |    |    |-- e: array (nullable = false)\n         |    |    |    |-- element: integer (containsNull = false)\n         |-- s2: struct (nullable = false)\n         |    |-- f: integer (nullable = false)\n         |-- s3: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: integer (containsNull = false)\n         |-- s4: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: struct (containsNull = false)\n         |    |    |    |-- e: integer (nullable = false)\n         |    |    |    |-- f: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; print(nested.schema_string(df))\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s1!.b!.c: integer (nullable = false)\n         |-- s1!.b!.d: integer (nullable = false)\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2.f: integer (nullable = false)\n         |-- s3!!: integer (nullable = false)\n         |-- s4!!.e: integer (nullable = false)\n         |-- s4!!.f: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    flat_schema = flatten_schema(df.schema, explode=True)\n    return _flat_schema_to_tree_string(flat_schema.fields)\n</code></pre>"},{"location":"reference/nested/#spark_frame.nested_impl.unnest_all_fields.unnest_all_fields","title":"<code>unnest_all_fields(df: DataFrame, keep_columns: Optional[List[str]] = None) -&gt; Dict[str, DataFrame]</code>","text":"<p>Given a DataFrame, return a dict of {granularity: DataFrame} where all arrays have been recursively unnested (a.k.a. exploded). This produce one DataFrame for each possible granularity.</p> <p>For instance, given a DataFrame with the following flattened schema:     id     s1.a     s2!.b     s2!.c     s2!.s3!.d     s4!.e     s4!.f</p> This will produce a dict with four granularity - DataFrames entries <ul> <li>'': DataFrame[id, s1.a] ('' corresponds to the root granularity)</li> <li>'s2': DataFrame[s2!.b, s2!.c]</li> <li>'s2!.s3': DataFrame[s2!.s3!.d]</li> <li>'s4': DataFrame[s4!.e, s4!.f]</li> </ul> <p>Limitation: Maps are not unnested</p> <ul> <li>Fields of type Maps are not unnested by this method.</li> <li>A possible workaround is to first use the transformation <code>spark_frame.transformations.convert_all_maps_to_arrays</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>keep_columns</code> <code>Optional[List[str]]</code> <p>Names of columns that should be kept while unnesting</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, DataFrame]</code> <p>A list of DataFrames</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         STRUCT(2 as a) as s1,\n...         ARRAY(STRUCT(3 as b, 4 as c, ARRAY(STRUCT(5 as d), STRUCT(6 as d)) as s3)) as s2,\n...         ARRAY(STRUCT(7 as e, 8 as f), STRUCT(9 as e, 10 as f)) as s4\n... ''')\n&gt;&gt;&gt; df.show(truncate=False)\n+---+---+--------------------+-----------------+\n|id |s1 |s2                  |s4               |\n+---+---+--------------------+-----------------+\n|1  |{2}|[{3, 4, [{5}, {6}]}]|[{7, 8}, {9, 10}]|\n+---+---+--------------------+-----------------+\n\n&gt;&gt;&gt; nested.fields(df)\n['id', 's1.a', 's2!.b', 's2!.c', 's2!.s3!.d', 's4!.e', 's4!.f']\n&gt;&gt;&gt; result_df_list = nested.unnest_all_fields(df, keep_columns=[\"id\"])\n&gt;&gt;&gt; for cols, result_df in result_df_list.items():\n...     print(cols)\n...     result_df.show()\n\n+---+----+\n| id|s1.a|\n+---+----+\n|  1|   2|\n+---+----+\n\ns2\n+---+-----+-----+\n| id|s2!.b|s2!.c|\n+---+-----+-----+\n|  1|    3|    4|\n+---+-----+-----+\n\ns2!.s3\n+---+---------+\n| id|s2!.s3!.d|\n+---+---------+\n|  1|        5|\n|  1|        6|\n+---+---------+\n\ns4\n+---+-----+-----+\n| id|s4!.e|s4!.f|\n+---+-----+-----+\n|  1|    7|    8|\n|  1|    9|   10|\n+---+-----+-----+\n</code></pre> Source code in <code>spark_frame/nested_impl/unnest_all_fields.py</code> <pre><code>def unnest_all_fields(df: DataFrame, keep_columns: Optional[List[str]] = None) -&gt; Dict[str, DataFrame]:\n    \"\"\"Given a DataFrame, return a dict of {granularity: DataFrame} where all arrays have been recursively\n    unnested (a.k.a. exploded).\n    This produce one DataFrame for each possible granularity.\n\n    For instance, given a DataFrame with the following flattened schema:\n        id\n        s1.a\n        s2!.b\n        s2!.c\n        s2!.s3!.d\n        s4!.e\n        s4!.f\n\n    This will produce a dict with four granularity - DataFrames entries:\n        - '': DataFrame[id, s1.a] ('' corresponds to the root granularity)\n        - 's2': DataFrame[s2!.b, s2!.c]\n        - 's2!.s3': DataFrame[s2!.s3!.d]\n        - 's4': DataFrame[s4!.e, s4!.f]\n\n    !!! warning \"Limitation: Maps are not unnested\"\n        - Fields of type Maps are not unnested by this method.\n        - A possible workaround is to first use the transformation\n        [`spark_frame.transformations.convert_all_maps_to_arrays`]\n        [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays]\n\n    Args:\n        df: A Spark DataFrame\n        keep_columns: Names of columns that should be kept while unnesting\n\n    Returns:\n        A list of DataFrames\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         STRUCT(2 as a) as s1,\n        ...         ARRAY(STRUCT(3 as b, 4 as c, ARRAY(STRUCT(5 as d), STRUCT(6 as d)) as s3)) as s2,\n        ...         ARRAY(STRUCT(7 as e, 8 as f), STRUCT(9 as e, 10 as f)) as s4\n        ... ''')\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+---+--------------------+-----------------+\n        |id |s1 |s2                  |s4               |\n        +---+---+--------------------+-----------------+\n        |1  |{2}|[{3, 4, [{5}, {6}]}]|[{7, 8}, {9, 10}]|\n        +---+---+--------------------+-----------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.fields(df)\n        ['id', 's1.a', 's2!.b', 's2!.c', 's2!.s3!.d', 's4!.e', 's4!.f']\n        &gt;&gt;&gt; result_df_list = nested.unnest_all_fields(df, keep_columns=[\"id\"])\n        &gt;&gt;&gt; for cols, result_df in result_df_list.items():\n        ...     print(cols)\n        ...     result_df.show()\n        &lt;BLANKLINE&gt;\n        +---+----+\n        | id|s1.a|\n        +---+----+\n        |  1|   2|\n        +---+----+\n        &lt;BLANKLINE&gt;\n        s2\n        +---+-----+-----+\n        | id|s2!.b|s2!.c|\n        +---+-----+-----+\n        |  1|    3|    4|\n        +---+-----+-----+\n        &lt;BLANKLINE&gt;\n        s2!.s3\n        +---+---------+\n        | id|s2!.s3!.d|\n        +---+---------+\n        |  1|        5|\n        |  1|        6|\n        +---+---------+\n        &lt;BLANKLINE&gt;\n        s4\n        +---+-----+-----+\n        | id|s4!.e|s4!.f|\n        +---+-----+-----+\n        |  1|    7|    8|\n        |  1|    9|   10|\n        +---+-----+-----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    if keep_columns is None:\n        keep_columns = []\n    fields_to_unnest = [field for field in nested.fields(df) if not is_sub_field_or_equal_to_any(field, keep_columns)]\n    return unnest_fields(df, fields_to_unnest, keep_fields=keep_columns)\n</code></pre>"},{"location":"reference/nested/#spark_frame.nested_impl.unnest_field.unnest_field","title":"<code>unnest_field(df: DataFrame, field_name: str, keep_columns: Optional[List[str]] = None) -&gt; DataFrame</code>","text":"<p>Given a DataFrame, return a new DataFrame where the specified column has been recursively unnested (a.k.a. exploded).</p> <p>Limitation: Maps are not unnested</p> <ul> <li>Fields of type Maps are not unnested by this method.</li> <li>A possible workaround is to first use the transformation <code>spark_frame.transformations.convert_all_maps_to_arrays</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>field_name</code> <code>str</code> <p>The name of a nested column to unnest</p> required <code>keep_columns</code> <code>Optional[List[str]]</code> <p>List of column names to keep while unnesting</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as arr\n... ''')\n&gt;&gt;&gt; df.show(truncate=False)\n+---+----------------+\n|id |arr             |\n+---+----------------+\n|1  |[[1, 2], [3, 4]]|\n+---+----------------+\n\n&gt;&gt;&gt; nested.fields(df)\n['id', 'arr!!']\n&gt;&gt;&gt; nested.unnest_field(df, 'arr!').show(truncate=False)\n+------+\n|arr!  |\n+------+\n|[1, 2]|\n|[3, 4]|\n+------+\n\n&gt;&gt;&gt; nested.unnest_field(df, 'arr!!').show(truncate=False)\n+-----+\n|arr!!|\n+-----+\n|1    |\n|2    |\n|3    |\n|4    |\n+-----+\n\n&gt;&gt;&gt; nested.unnest_field(df, 'arr!!', keep_columns=[\"id\"]).show(truncate=False)\n+---+-----+\n|id |arr!!|\n+---+-----+\n|1  |1    |\n|1  |2    |\n|1  |3    |\n|1  |4    |\n+---+-----+\n</code></pre> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(\n...             STRUCT(ARRAY(STRUCT(\"a1\" as a, \"b1\" as b), STRUCT(\"a2\" as a, \"b1\" as b)) as s2),\n...             STRUCT(ARRAY(STRUCT(\"a3\" as a, \"b3\" as b)) as s2)\n...         ) as s1\n... ''')\n&gt;&gt;&gt; df.show(truncate=False)\n+---+--------------------------------------+\n|id |s1                                    |\n+---+--------------------------------------+\n|1  |[{[{a1, b1}, {a2, b1}]}, {[{a3, b3}]}]|\n+---+--------------------------------------+\n\n&gt;&gt;&gt; nested.fields(df)\n['id', 's1!.s2!.a', 's1!.s2!.b']\n&gt;&gt;&gt; nested.unnest_field(df, 's1!.s2!').show(truncate=False)\n+--------+\n|s1!.s2! |\n+--------+\n|{a1, b1}|\n|{a2, b1}|\n|{a3, b3}|\n+--------+\n</code></pre> Source code in <code>spark_frame/nested_impl/unnest_field.py</code> <pre><code>def unnest_field(df: DataFrame, field_name: str, keep_columns: Optional[List[str]] = None) -&gt; DataFrame:\n    \"\"\"Given a DataFrame, return a new DataFrame where the specified column has been recursively\n    unnested (a.k.a. exploded).\n\n    !!! warning \"Limitation: Maps are not unnested\"\n        - Fields of type Maps are not unnested by this method.\n        - A possible workaround is to first use the transformation\n        [`spark_frame.transformations.convert_all_maps_to_arrays`]\n        [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays]\n\n    Args:\n        df: A Spark DataFrame\n        field_name: The name of a nested column to unnest\n        keep_columns: List of column names to keep while unnesting\n\n    Returns:\n        A new DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as arr\n        ... ''')\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+----------------+\n        |id |arr             |\n        +---+----------------+\n        |1  |[[1, 2], [3, 4]]|\n        +---+----------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.fields(df)\n        ['id', 'arr!!']\n        &gt;&gt;&gt; nested.unnest_field(df, 'arr!').show(truncate=False)\n        +------+\n        |arr!  |\n        +------+\n        |[1, 2]|\n        |[3, 4]|\n        +------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.unnest_field(df, 'arr!!').show(truncate=False)\n        +-----+\n        |arr!!|\n        +-----+\n        |1    |\n        |2    |\n        |3    |\n        |4    |\n        +-----+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.unnest_field(df, 'arr!!', keep_columns=[\"id\"]).show(truncate=False)\n        +---+-----+\n        |id |arr!!|\n        +---+-----+\n        |1  |1    |\n        |1  |2    |\n        |1  |3    |\n        |1  |4    |\n        +---+-----+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(\n        ...             STRUCT(ARRAY(STRUCT(\"a1\" as a, \"b1\" as b), STRUCT(\"a2\" as a, \"b1\" as b)) as s2),\n        ...             STRUCT(ARRAY(STRUCT(\"a3\" as a, \"b3\" as b)) as s2)\n        ...         ) as s1\n        ... ''')\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+--------------------------------------+\n        |id |s1                                    |\n        +---+--------------------------------------+\n        |1  |[{[{a1, b1}, {a2, b1}]}, {[{a3, b3}]}]|\n        +---+--------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.fields(df)\n        ['id', 's1!.s2!.a', 's1!.s2!.b']\n        &gt;&gt;&gt; nested.unnest_field(df, 's1!.s2!').show(truncate=False)\n        +--------+\n        |s1!.s2! |\n        +--------+\n        |{a1, b1}|\n        |{a2, b1}|\n        |{a3, b3}|\n        +--------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    if keep_columns is None:\n        keep_columns = []\n    return next(iter(unnest_fields(df, field_name, keep_fields=keep_columns).values()))\n</code></pre>"},{"location":"reference/nested/#spark_frame.nested_impl.with_fields.with_fields","title":"<code>with_fields(df: DataFrame, fields: Mapping[str, AnyKindOfTransformation]) -&gt; DataFrame</code>","text":"<p>Return a new DataFrame by adding or replacing (when they already exist) columns.</p> <p>This method is similar to the DataFrame.withColumn method, with the extra capability of working on nested and repeated fields (structs and arrays).</p> <p>The syntax for field names works as follows:</p> <ul> <li>\".\" is the separator for struct elements</li> <li>\"!\" must be appended at the end of fields that are repeated (arrays)</li> <li>Map keys are appended with <code>%key</code></li> <li>Map values are appended with <code>%value</code></li> </ul> <p>The following types of transformation are allowed:</p> <ul> <li>String and column expressions can be used on any non-repeated field, even nested ones.</li> <li>When working on repeated fields, transformations must be expressed as higher order functions   (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,   but their value will be repeated multiple times.</li> <li>When working on multiple levels of nested arrays, higher order functions may take multiple arguments,   corresponding to each level of repetition (See Example 5.).</li> <li><code>None</code> can also be used to represent the identity transformation, this is useful to select a field without    changing and without having to repeat its name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>fields</code> <code>Mapping[str, AnyKindOfTransformation]</code> <p>A Dict(field_name, transformation_to_apply)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been</p> <code>DataFrame</code> <p>applied to the corresponding fields. If a field name did not exist in the input DataFrame,</p> <code>DataFrame</code> <p>it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one.</p> <p>Example 1: non-repeated fields</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s.a: integer (nullable = false)\n |-- s.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+------+\n| id|     s|\n+---+------+\n|  1|{2, 3}|\n+---+------+\n</code></pre> <p>Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression.</p> <pre><code>&gt;&gt;&gt; new_df = nested.with_fields(df, {\n...     \"s.id\": \"id\",                                 # column name (string)\n...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")            # Column expression\n... })\n&gt;&gt;&gt; new_df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s: struct (nullable = false)\n |    |-- a: integer (nullable = false)\n |    |-- b: integer (nullable = false)\n |    |-- id: integer (nullable = false)\n |    |-- c: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---+------------+\n| id|           s|\n+---+------------+\n|  1|{2, 3, 1, 5}|\n+---+------------+\n</code></pre> <p>Example 2: repeated fields</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.a: integer (nullable = false)\n |-- s!.b.c: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+--------------------+\n| id|                   s|\n+---+--------------------+\n|  1|[{1, {2}}, {3, {4}}]|\n+---+--------------------+\n</code></pre> <p>Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...     \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]}\n... )\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.a: integer (nullable = false)\n |-- s!.b.c: integer (nullable = false)\n |-- s!.b.d: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+--------------------------+\n|id |s                         |\n+---+--------------------------+\n|1  |[{1, {2, 3}}, {3, {4, 7}}]|\n+---+--------------------------+\n</code></pre> <p>String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.with_fields, {\n...     \"id\": None,\n...     \"s!.a\": \"id\",\n...     \"s!.b.c\": f.lit(2)\n... }).show(truncate=False)\n+---+--------------------+\n|id |s                   |\n+---+--------------------+\n|1  |[{1, {2}}, {1, {2}}]|\n+---+--------------------+\n</code></pre> <p>Example 3: field repeated twice</p> <pre><code>&gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.e!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+\n| id|            s|\n+---+-------------+\n|  1|[{[1, 2, 3]}]|\n+---+-------------+\n</code></pre> <p>Here, the lambda expression will be applied to the last repeated element <code>e</code>.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show()\n+---+-------------------+\n| id|                  s|\n+---+-------------------+\n|  1|[{[1.0, 2.0, 3.0]}]|\n+---+-------------------+\n</code></pre> <p>Example 4: Dataframe with maps</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: integer (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+\n| id|           m1|\n+---+-------------+\n|  1|{a -&gt; {2, 3}}|\n+---+-------------+\n</code></pre> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...  \"m1%key\": lambda key : f.upper(key),\n...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: double (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---+---------------+\n| id|             m1|\n+---+---------------+\n|  1|{A -&gt; {2.0, 3}}|\n+---+---------------+\n</code></pre> <p>Example 5: Accessing multiple repetition levels</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(\n...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n...         ) as s1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.average: integer (nullable = false)\n |-- s1!.values!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+---+--------------------------------------+\n|id |s1                                    |\n+---+--------------------------------------+\n|1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n+---+--------------------------------------+\n</code></pre> <p>Here, the transformation applied to \"s1!.values!\" takes two arguments.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+-----------------------------------------+\n|id |s1                                       |\n+---+-----------------------------------------+\n|1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n+---+-----------------------------------------+\n</code></pre> <p>Extra arguments can be added to the left for each repetition level, up to the root level.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+---------------------------------------+\n|id |s1                                     |\n+---+---------------------------------------+\n|1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n+---+---------------------------------------+\n</code></pre> Source code in <code>spark_frame/nested_impl/with_fields.py</code> <pre><code>def with_fields(df: DataFrame, fields: Mapping[str, AnyKindOfTransformation]) -&gt; DataFrame:\n    \"\"\"Return a new [DataFrame][pyspark.sql.DataFrame] by adding or replacing (when they already exist) columns.\n\n    This method is similar to the [DataFrame.withColumn][pyspark.sql.DataFrame.withColumn] method, with the extra\n    capability of working on nested and repeated fields (structs and arrays).\n\n    The syntax for field names works as follows:\n\n    - \".\" is the separator for struct elements\n    - \"!\" must be appended at the end of fields that are repeated (arrays)\n    - Map keys are appended with `%key`\n    - Map values are appended with `%value`\n\n    The following types of transformation are allowed:\n\n    - String and column expressions can be used on any non-repeated field, even nested ones.\n    - When working on repeated fields, transformations must be expressed as higher order functions\n      (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,\n      but their value will be repeated multiple times.\n    - When working on multiple levels of nested arrays, higher order functions may take multiple arguments,\n      corresponding to each level of repetition (See Example 5.).\n    - `None` can also be used to represent the identity transformation, this is useful to select a field without\n       changing and without having to repeat its name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n        fields: A Dict(field_name, transformation_to_apply)\n\n    Returns:\n        A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been\n        applied to the corresponding fields. If a field name did not exist in the input DataFrame,\n        it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one.\n\n    Examples: Example 1: non-repeated fields\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s.a: integer (nullable = false)\n         |-- s.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+------+\n        | id|     s|\n        +---+------+\n        |  1|{2, 3}|\n        +---+------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on non-repeated fields may be expressed as a string representing a column name\n        or a Column expression.\n        &gt;&gt;&gt; new_df = nested.with_fields(df, {\n        ...     \"s.id\": \"id\",                                 # column name (string)\n        ...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")            # Column expression\n        ... })\n        &gt;&gt;&gt; new_df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s: struct (nullable = false)\n         |    |-- a: integer (nullable = false)\n         |    |-- b: integer (nullable = false)\n         |    |-- id: integer (nullable = false)\n         |    |-- c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---+------------+\n        | id|           s|\n        +---+------------+\n        |  1|{2, 3, 1, 5}|\n        +---+------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2: repeated fields\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.a: integer (nullable = false)\n         |-- s!.b.c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+--------------------+\n        | id|                   s|\n        +---+--------------------+\n        |  1|[{1, {2}}, {3, {4}}]|\n        +---+--------------------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on repeated fields must be expressed as\n        higher-order functions (lambda expressions or named functions).\n        The value passed to this function will correspond to the last repeated element.\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...     \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]}\n        ... )\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.a: integer (nullable = false)\n         |-- s!.b.c: integer (nullable = false)\n         |-- s!.b.d: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+--------------------------+\n        |id |s                         |\n        +---+--------------------------+\n        |1  |[{1, {2, 3}}, {3, {4, 7}}]|\n        +---+--------------------------+\n        &lt;BLANKLINE&gt;\n\n        String and column expressions can be used on repeated fields as well,\n        but their value will be repeated multiple times.\n        &gt;&gt;&gt; df.transform(nested.with_fields, {\n        ...     \"id\": None,\n        ...     \"s!.a\": \"id\",\n        ...     \"s!.b.c\": f.lit(2)\n        ... }).show(truncate=False)\n        +---+--------------------+\n        |id |s                   |\n        +---+--------------------+\n        |1  |[{1, {2}}, {1, {2}}]|\n        +---+--------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 3: field repeated twice\n        &gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.e!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+\n        | id|            s|\n        +---+-------------+\n        |  1|[{[1, 2, 3]}]|\n        +---+-------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the lambda expression will be applied to the last repeated element `e`.\n        &gt;&gt;&gt; df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show()\n        +---+-------------------+\n        | id|                  s|\n        +---+-------------------+\n        |  1|[{[1.0, 2.0, 3.0]}]|\n        +---+-------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 4: Dataframe with maps\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: integer (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+\n        | id|           m1|\n        +---+-------------+\n        |  1|{a -&gt; {2, 3}}|\n        +---+-------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...  \"m1%key\": lambda key : f.upper(key),\n        ...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n        ... })\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: double (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---+---------------+\n        | id|             m1|\n        +---+---------------+\n        |  1|{A -&gt; {2.0, 3}}|\n        +---+---------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 5: Accessing multiple repetition levels\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(\n        ...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n        ...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n        ...         ) as s1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.average: integer (nullable = false)\n         |-- s1!.values!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+--------------------------------------+\n        |id |s1                                    |\n        +---+--------------------------------------+\n        |1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n        +---+--------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the transformation applied to \"s1!.values!\" takes two arguments.\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+-----------------------------------------+\n        |id |s1                                       |\n        +---+-----------------------------------------+\n        |1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n        +---+-----------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Extra arguments can be added to the left for each repetition level, up to the root level.\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+---------------------------------------+\n        |id |s1                                     |\n        +---+---------------------------------------+\n        |1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n        +---+---------------------------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    default_columns = {field: None for field in nested.fields(df)}\n    fields = {**default_columns, **fields}\n    return df.select(*resolve_nested_fields(fields, starting_level=df))\n</code></pre>"},{"location":"reference/nested_functions/","title":"spark_frame.nested_functions","text":"<p>Like with pyspark.sql.functions,  the methods in this module all return Column expressions and can be used to build operations on Spark DataFrames using <code>select</code>, <code>withColumn</code>, etc.</p>"},{"location":"reference/nested_functions/#spark_frame.nested_functions_impl.aggregate.aggregate","title":"<code>aggregate(field_name: str, initial_value: StringOrColumn, merge: Callable[[Column, Column], Column], start: Optional[Callable[[Column], Column]] = None, finish: Optional[Callable[[Column], Column]] = None, starting_level: Union[Column, DataFrame, None] = None) -&gt; Column</code>","text":"<p>Recursively compute an aggregation of all elements in the given repeated field.</p> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the repeated field to sum. It may be repeated multiple times.</p> required <code>initial_value</code> <code>StringOrColumn</code> <p>Name of column or Column expression.</p> required <code>merge</code> <code>Callable[[Column, Column], Column]</code> <p>A binary function <code>(acc: Column, x: Column[) -&gt; Column</code> returning an expression of the same type as <code>initial_value</code>.</p> required <code>start</code> <code>Optional[Callable[[Column], Column]]</code> <p>An optional unary function <code>(x: Column) -&gt; Column</code> that transforms the values to aggregate into the same type as <code>initial_value</code>.</p> <code>None</code> <code>finish</code> <code>Optional[Callable[[Column], Column]]</code> <p>An optional unary function <code>(x: Column) -&gt; Column</code> used to convert accumulated value into the final result.</p> <code>None</code> <code>starting_level</code> <code>Union[Column, DataFrame, None]</code> <p>Nesting level from which the aggregation is started</p> <code>None</code> <p>Returns:</p> Type Description <code>Column</code> <p>A Column expression</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_frame.nested_functions_impl.aggregate import _get_sample_data\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; from spark_frame import nested_functions as nf\n&gt;&gt;&gt; employee_df = _get_sample_data()\n&gt;&gt;&gt; nested.print_schema(employee_df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.client: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n |-- projects!.tasks!.estimate: long (nullable = true)\n\n&gt;&gt;&gt; employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False)\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n|employee_id|name      |age|projects                                                                                                                           |\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n|1          |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] |\n|1          |Jane Doe  |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]|\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n\n&gt;&gt;&gt; employee_df.transform(nested.select, {\n...     \"employee_id\": None,\n...     \"name\": None,\n...     \"age\": None,\n...     \"projects!.tasks!.estimate\": None\n... }).show(truncate=False)\n+-----------+----------+---+------------------------------+\n|employee_id|name      |age|projects                      |\n+-----------+----------+---+------------------------------+\n|1          |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] |\n|1          |Jane Doe  |25 |[{[{20}, {13}]}, {[{8}, {5}]}]|\n+-----------+----------+---+------------------------------+\n\n&gt;&gt;&gt; employee_df.transform(\n...     nested.select,\n...     {\n...         \"employee_id\": None,\n...         \"name\": None,\n...         \"age\": None,\n...         \"total_task_estimate\": nf.aggregate(\n...             field_name=\"projects!.tasks!.estimate\",\n...             initial_value=f.lit(0).cast(\"BIGINT\"),\n...             merge=lambda acc, x: acc + x\n...         ),\n...         \"projects!.task_estimate_per_project\": lambda project: nf.aggregate(\n...             field_name=\"tasks!.estimate\",\n...             initial_value=f.lit(0).cast(\"BIGINT\"),\n...             merge=lambda acc, x: acc + x,\n...             starting_level=project,\n...         ),\n...     },\n... ).show(truncate=False)\n+-----------+----------+---+-------------------+------------+\n|employee_id|name      |age|total_task_estimate|projects    |\n+-----------+----------+---+-------------------+------------+\n|1          |John Smith|30 |29                 |[{13}, {16}]|\n|1          |Jane Doe  |25 |46                 |[{33}, {13}]|\n+-----------+----------+---+-------------------+------------+\n</code></pre> Source code in <code>spark_frame/nested_functions_impl/aggregate.py</code> <pre><code>def aggregate(\n    field_name: str,\n    initial_value: StringOrColumn,\n    merge: Callable[[Column, Column], Column],\n    start: Optional[Callable[[Column], Column]] = None,\n    finish: Optional[Callable[[Column], Column]] = None,\n    starting_level: Union[Column, DataFrame, None] = None,\n) -&gt; Column:\n    \"\"\"Recursively compute an aggregation of all elements in the given repeated field.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        field_name: Name of the repeated field to sum. It may be repeated multiple times.\n        initial_value: Name of column or Column expression.\n        merge: A binary function `(acc: Column, x: Column[) -&gt; Column` returning an expression\n            of the same type as `initial_value`.\n        start: An optional unary function `(x: Column) -&gt; Column` that transforms the values to aggregate into the\n            same type as `initial_value`.\n        finish: An optional unary function `(x: Column) -&gt; Column` used to convert accumulated value into the final\n            result.\n        starting_level: Nesting level from which the aggregation is started\n\n    Returns:\n        A Column expression\n\n    Examples:\n        &gt;&gt;&gt; from spark_frame.nested_functions_impl.aggregate import _get_sample_data\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; from spark_frame import nested_functions as nf\n        &gt;&gt;&gt; employee_df = _get_sample_data()\n        &gt;&gt;&gt; nested.print_schema(employee_df)\n        root\n         |-- employee_id: integer (nullable = true)\n         |-- name: string (nullable = true)\n         |-- age: long (nullable = true)\n         |-- projects!.name: string (nullable = true)\n         |-- projects!.client: string (nullable = true)\n         |-- projects!.tasks!.name: string (nullable = true)\n         |-- projects!.tasks!.estimate: long (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False)\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        |employee_id|name      |age|projects                                                                                                                           |\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        |1          |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] |\n        |1          |Jane Doe  |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]|\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.transform(nested.select, {\n        ...     \"employee_id\": None,\n        ...     \"name\": None,\n        ...     \"age\": None,\n        ...     \"projects!.tasks!.estimate\": None\n        ... }).show(truncate=False)\n        +-----------+----------+---+------------------------------+\n        |employee_id|name      |age|projects                      |\n        +-----------+----------+---+------------------------------+\n        |1          |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] |\n        |1          |Jane Doe  |25 |[{[{20}, {13}]}, {[{8}, {5}]}]|\n        +-----------+----------+---+------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.transform(\n        ...     nested.select,\n        ...     {\n        ...         \"employee_id\": None,\n        ...         \"name\": None,\n        ...         \"age\": None,\n        ...         \"total_task_estimate\": nf.aggregate(\n        ...             field_name=\"projects!.tasks!.estimate\",\n        ...             initial_value=f.lit(0).cast(\"BIGINT\"),\n        ...             merge=lambda acc, x: acc + x\n        ...         ),\n        ...         \"projects!.task_estimate_per_project\": lambda project: nf.aggregate(\n        ...             field_name=\"tasks!.estimate\",\n        ...             initial_value=f.lit(0).cast(\"BIGINT\"),\n        ...             merge=lambda acc, x: acc + x,\n        ...             starting_level=project,\n        ...         ),\n        ...     },\n        ... ).show(truncate=False)\n        +-----------+----------+---+-------------------+------------+\n        |employee_id|name      |age|total_task_estimate|projects    |\n        +-----------+----------+---+-------------------+------------+\n        |1          |John Smith|30 |29                 |[{13}, {16}]|\n        |1          |Jane Doe  |25 |46                 |[{33}, {13}]|\n        +-----------+----------+---+-------------------+------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"  # noqa: E501\n    validate_nested_field_names(field_name, allow_maps=False)\n    agg_merge = PrintableFunction(\n        lambda a: f.aggregate(a, initial_value, merge),\n        lambda s: f\"f.aggregate({s}, initial_value, merge)\",\n    )\n    if finish is not None:\n        agg_finish = PrintableFunction(\n            lambda a: f.aggregate(f.array(a), initial_value, merge, finish),\n            lambda s: f\"f.aggregate(f.array({s}), initial_value, merge, finish))\",\n        )\n    else:\n        agg_finish = higher_order.identity\n    if start is not None:\n        agg_start = PrintableFunction(start, lambda s: f\"start({s})\")\n    else:\n        agg_start = higher_order.identity\n\n    field_parts = _split_field_name(field_name)\n\n    def recurse_item(parts: List[str], prefix: str = \"\") -&gt; PrintableFunction:\n        key = parts[0]\n        is_struct = key == STRUCT_SEPARATOR\n        is_repeated = key == REPETITION_MARKER\n        has_children = len(parts) &gt; 1\n        if has_children:\n            child_transformation = recurse_item(parts[1:], prefix + key)\n        else:\n            child_transformation = agg_start\n        if is_struct:\n            assert_true(\n                has_children,\n                \"Error, this should not happen: struct without children\",\n            )\n            return child_transformation\n        elif is_repeated:\n            return fp.compose(agg_merge, higher_order.transform(child_transformation))\n        else:\n            return fp.compose(child_transformation, higher_order.struct_get(key))\n\n    root_transformation = recurse_item(field_parts)\n    root_transformation = fp.compose(agg_finish, root_transformation)\n    return root_transformation(starting_level)\n</code></pre>"},{"location":"reference/nested_functions/#spark_frame.nested_functions_impl.average.average","title":"<code>average(field_name: str, starting_level: Union[Column, DataFrame, None] = None) -&gt; Column</code>","text":"<p>Recursively compute the average of all elements in the given repeated field.</p> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the repeated field to sum. It may be repeated multiple times.</p> required <code>starting_level</code> <code>Union[Column, DataFrame, None]</code> <p>Nesting level from which the aggregation is started.</p> <code>None</code> <p>Returns:</p> Type Description <code>Column</code> <p>A Column expression</p> <p>Example 1</p> <pre><code>&gt;&gt;&gt; from spark_frame.nested_functions_impl.aggregate import _get_sample_data\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; from spark_frame import nested_functions as nf\n&gt;&gt;&gt; employee_df = _get_sample_data()\n&gt;&gt;&gt; nested.print_schema(employee_df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.client: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n |-- projects!.tasks!.estimate: long (nullable = true)\n\n&gt;&gt;&gt; employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False)\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n|employee_id|name      |age|projects                                                                                                                           |\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n|1          |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] |\n|1          |Jane Doe  |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]|\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n\n&gt;&gt;&gt; employee_df.transform(nested.select, {\n...     \"employee_id\": None,\n...     \"name\": None,\n...     \"age\": None,\n...     \"projects!.tasks!.estimate\": None\n... }).show(truncate=False)\n+-----------+----------+---+------------------------------+\n|employee_id|name      |age|projects                      |\n+-----------+----------+---+------------------------------+\n|1          |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] |\n|1          |Jane Doe  |25 |[{[{20}, {13}]}, {[{8}, {5}]}]|\n+-----------+----------+---+------------------------------+\n\n&gt;&gt;&gt; employee_df.transform(nested.select, {\n...     \"employee_id\": None,\n...     \"name\": None,\n...     \"age\": None,\n...     \"average_task_estimate\": nf.average(\"projects!.tasks!.estimate\"),\n...     \"projects!.average_task_estimate_per_project\":\n...         lambda project: nf.average(\"tasks!.estimate\", starting_level=project),\n... }).show(truncate=False)\n+-----------+----------+---+---------------------+---------------+\n|employee_id|name      |age|average_task_estimate|projects       |\n+-----------+----------+---+---------------------+---------------+\n|1          |John Smith|30 |7.25                 |[{6.5}, {8.0}] |\n|1          |Jane Doe  |25 |11.5                 |[{16.5}, {6.5}]|\n+-----------+----------+---+---------------------+---------------+\n</code></pre> <p>Example 2 : with all kind of nested structures</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     1 as id,\n...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n...     ARRAY(\n...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n...     ) as s5\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s2!!: integer (nullable = false)\n |-- s3!!.a: integer (nullable = false)\n |-- s4!.a!: integer (nullable = false)\n |-- s5!.a!.b.c: integer (nullable = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+---+----------+----------------+--------------+--------------------+------------------------------------+\n|id |s1        |s2              |s3            |s4                  |s5                                  |\n+---+----------+----------------+--------------+--------------------+------------------------------------+\n|1  |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]|\n+---+----------+----------------+--------------+--------------------+------------------------------------+\n\n&gt;&gt;&gt; df.select(nf.average(\"s1!.a\").alias(\"average\")).show()\n+-------+\n|average|\n+-------+\n|    1.5|\n+-------+\n\n&gt;&gt;&gt; df.select(nf.average(\"s2!!\").alias(\"average\")).show()\n+-------+\n|average|\n+-------+\n|    2.5|\n+-------+\n\n&gt;&gt;&gt; df.select(nf.average(\"s3!!.a\").alias(\"average\")).show()\n+-------+\n|average|\n+-------+\n|    1.5|\n+-------+\n\n&gt;&gt;&gt; df.select(nf.average(\"s4!.a!\").alias(\"average\")).show()\n+-------+\n|average|\n+-------+\n|    2.5|\n+-------+\n\n&gt;&gt;&gt; df.select(nf.average(\"s5!.a!.b.c\").alias(\"average\")).show()\n+-------+\n|average|\n+-------+\n|    2.5|\n+-------+\n</code></pre> Source code in <code>spark_frame/nested_functions_impl/average.py</code> <pre><code>def average(\n    field_name: str,\n    starting_level: Union[Column, DataFrame, None] = None,\n) -&gt; Column:\n    \"\"\"Recursively compute the average of all elements in the given repeated field.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        field_name: Name of the repeated field to sum. It may be repeated multiple times.\n        starting_level: Nesting level from which the aggregation is started.\n\n    Returns:\n        A Column expression\n\n    Examples: Example 1\n        &gt;&gt;&gt; from spark_frame.nested_functions_impl.aggregate import _get_sample_data\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; from spark_frame import nested_functions as nf\n        &gt;&gt;&gt; employee_df = _get_sample_data()\n        &gt;&gt;&gt; nested.print_schema(employee_df)\n        root\n         |-- employee_id: integer (nullable = true)\n         |-- name: string (nullable = true)\n         |-- age: long (nullable = true)\n         |-- projects!.name: string (nullable = true)\n         |-- projects!.client: string (nullable = true)\n         |-- projects!.tasks!.name: string (nullable = true)\n         |-- projects!.tasks!.estimate: long (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False)\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        |employee_id|name      |age|projects                                                                                                                           |\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        |1          |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] |\n        |1          |Jane Doe  |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]|\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.transform(nested.select, {\n        ...     \"employee_id\": None,\n        ...     \"name\": None,\n        ...     \"age\": None,\n        ...     \"projects!.tasks!.estimate\": None\n        ... }).show(truncate=False)\n        +-----------+----------+---+------------------------------+\n        |employee_id|name      |age|projects                      |\n        +-----------+----------+---+------------------------------+\n        |1          |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] |\n        |1          |Jane Doe  |25 |[{[{20}, {13}]}, {[{8}, {5}]}]|\n        +-----------+----------+---+------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.transform(nested.select, {\n        ...     \"employee_id\": None,\n        ...     \"name\": None,\n        ...     \"age\": None,\n        ...     \"average_task_estimate\": nf.average(\"projects!.tasks!.estimate\"),\n        ...     \"projects!.average_task_estimate_per_project\":\n        ...         lambda project: nf.average(\"tasks!.estimate\", starting_level=project),\n        ... }).show(truncate=False)\n        +-----------+----------+---+---------------------+---------------+\n        |employee_id|name      |age|average_task_estimate|projects       |\n        +-----------+----------+---+---------------------+---------------+\n        |1          |John Smith|30 |7.25                 |[{6.5}, {8.0}] |\n        |1          |Jane Doe  |25 |11.5                 |[{16.5}, {6.5}]|\n        +-----------+----------+---+---------------------+---------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2 : with all kind of nested structures\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     1 as id,\n        ...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n        ...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n        ...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n        ...     ARRAY(\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n        ...     ) as s5\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s2!!: integer (nullable = false)\n         |-- s3!!.a: integer (nullable = false)\n         |-- s4!.a!: integer (nullable = false)\n         |-- s5!.a!.b.c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+----------+----------------+--------------+--------------------+------------------------------------+\n        |id |s1        |s2              |s3            |s4                  |s5                                  |\n        +---+----------+----------------+--------------+--------------------+------------------------------------+\n        |1  |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]|\n        +---+----------+----------------+--------------+--------------------+------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.average(\"s1!.a\").alias(\"average\")).show()\n        +-------+\n        |average|\n        +-------+\n        |    1.5|\n        +-------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.average(\"s2!!\").alias(\"average\")).show()\n        +-------+\n        |average|\n        +-------+\n        |    2.5|\n        +-------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.average(\"s3!!.a\").alias(\"average\")).show()\n        +-------+\n        |average|\n        +-------+\n        |    1.5|\n        +-------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.average(\"s4!.a!\").alias(\"average\")).show()\n        +-------+\n        |average|\n        +-------+\n        |    2.5|\n        +-------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.average(\"s5!.a!.b.c\").alias(\"average\")).show()\n        +-------+\n        |average|\n        +-------+\n        |    2.5|\n        +-------+\n        &lt;BLANKLINE&gt;\n    \"\"\"  # noqa: E501\n    initial_value = f.struct(\n        f.lit(0).cast(\"BIGINT\").alias(\"sum\"),\n        f.lit(0).cast(\"BIGINT\").alias(\"count\"),\n    )\n\n    def start(x: Column) -&gt; Column:\n        return f.struct(x.alias(\"sum\"), f.lit(1).alias(\"count\"))\n\n    def merge(acc: Column, x: Column) -&gt; Column:\n        return f.struct(\n            (acc[\"sum\"] + x[\"sum\"]).alias(\"sum\"),\n            (acc[\"count\"] + x[\"count\"]).alias(\"count\"),\n        )\n\n    def finish(acc: Column) -&gt; Column:\n        return f.when(acc[\"count\"] &gt; 0, acc[\"sum\"] / acc[\"count\"])\n\n    return aggregate(\n        field_name,\n        initial_value=initial_value,\n        merge=merge,\n        start=start,\n        finish=finish,\n        starting_level=starting_level,\n    )\n</code></pre>"},{"location":"reference/nested_functions/#spark_frame.nested_functions_impl.sum.sum","title":"<code>sum(field_name: str, starting_level: Union[Column, DataFrame, None] = None) -&gt; Column</code>","text":"<p>Recursively compute the sum of all elements in the given repeated field.</p> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the repeated field to sum. It may be repeated multiple times.</p> required <code>starting_level</code> <code>Union[Column, DataFrame, None]</code> <p>Nesting level from which the aggregation is started.</p> <code>None</code> <p>Returns:</p> Type Description <code>Column</code> <p>A Column expression</p> <p>Example 1</p> <pre><code>&gt;&gt;&gt; from spark_frame.nested_functions_impl.aggregate import _get_sample_data\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; from spark_frame import nested_functions as nf\n&gt;&gt;&gt; employee_df = _get_sample_data()\n&gt;&gt;&gt; nested.print_schema(employee_df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.client: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n |-- projects!.tasks!.estimate: long (nullable = true)\n\n&gt;&gt;&gt; employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False)\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n|employee_id|name      |age|projects                                                                                                                           |\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n|1          |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] |\n|1          |Jane Doe  |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]|\n+-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n\n&gt;&gt;&gt; employee_df.transform(nested.select, {\n...     \"employee_id\": None,\n...     \"name\": None,\n...     \"age\": None,\n...     \"projects!.tasks!.estimate\": None\n... }).show(truncate=False)\n+-----------+----------+---+------------------------------+\n|employee_id|name      |age|projects                      |\n+-----------+----------+---+------------------------------+\n|1          |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] |\n|1          |Jane Doe  |25 |[{[{20}, {13}]}, {[{8}, {5}]}]|\n+-----------+----------+---+------------------------------+\n\n&gt;&gt;&gt; employee_df.transform(nested.select, {\n...     \"employee_id\": None,\n...     \"name\": None,\n...     \"age\": None,\n...     \"total_task_estimate\": nf.sum(\"projects!.tasks!.estimate\"),\n...     \"projects!.task_estimate_per_project\":\n...         lambda project: nf.sum(\"tasks!.estimate\", starting_level=project),\n... }).show(truncate=False)\n+-----------+----------+---+-------------------+------------+\n|employee_id|name      |age|total_task_estimate|projects    |\n+-----------+----------+---+-------------------+------------+\n|1          |John Smith|30 |29                 |[{13}, {16}]|\n|1          |Jane Doe  |25 |46                 |[{33}, {13}]|\n+-----------+----------+---+-------------------+------------+\n</code></pre> <p>Example 2 : with all kind of nested structures*</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     1 as id,\n...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n...     ARRAY(\n...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n...     ) as s5\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s2!!: integer (nullable = false)\n |-- s3!!.a: integer (nullable = false)\n |-- s4!.a!: integer (nullable = false)\n |-- s5!.a!.b.c: integer (nullable = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+---+----------+----------------+--------------+--------------------+------------------------------------+\n|id |s1        |s2              |s3            |s4                  |s5                                  |\n+---+----------+----------------+--------------+--------------------+------------------------------------+\n|1  |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]|\n+---+----------+----------------+--------------+--------------------+------------------------------------+\n\n&gt;&gt;&gt; df.select(nf.sum(\"s1!.a\").alias(\"sum\")).show()\n+---+\n|sum|\n+---+\n|  3|\n+---+\n\n&gt;&gt;&gt; df.select(nf.sum(\"s2!!\").alias(\"sum\")).show()\n+---+\n|sum|\n+---+\n| 10|\n+---+\n\n&gt;&gt;&gt; df.select(nf.sum(\"s3!!.a\").alias(\"sum\")).show()\n+---+\n|sum|\n+---+\n|  3|\n+---+\n\n&gt;&gt;&gt; df.select(nf.sum(\"s4!.a!\").alias(\"sum\")).show()\n+---+\n|sum|\n+---+\n| 10|\n+---+\n\n&gt;&gt;&gt; df.select(nf.sum(\"s5!.a!.b.c\").alias(\"sum\")).show()\n+---+\n|sum|\n+---+\n| 10|\n+---+\n</code></pre> Source code in <code>spark_frame/nested_functions_impl/sum.py</code> <pre><code>def sum(  # noqa: A001\n    field_name: str,\n    starting_level: Union[Column, DataFrame, None] = None,\n) -&gt; Column:\n    \"\"\"Recursively compute the sum of all elements in the given repeated field.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        field_name: Name of the repeated field to sum. It may be repeated multiple times.\n        starting_level: Nesting level from which the aggregation is started.\n\n    Returns:\n        A Column expression\n\n    Examples: Example 1\n        &gt;&gt;&gt; from spark_frame.nested_functions_impl.aggregate import _get_sample_data\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; from spark_frame import nested_functions as nf\n        &gt;&gt;&gt; employee_df = _get_sample_data()\n        &gt;&gt;&gt; nested.print_schema(employee_df)\n        root\n         |-- employee_id: integer (nullable = true)\n         |-- name: string (nullable = true)\n         |-- age: long (nullable = true)\n         |-- projects!.name: string (nullable = true)\n         |-- projects!.client: string (nullable = true)\n         |-- projects!.tasks!.name: string (nullable = true)\n         |-- projects!.tasks!.estimate: long (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False)\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        |employee_id|name      |age|projects                                                                                                                           |\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        |1          |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] |\n        |1          |Jane Doe  |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]|\n        +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.transform(nested.select, {\n        ...     \"employee_id\": None,\n        ...     \"name\": None,\n        ...     \"age\": None,\n        ...     \"projects!.tasks!.estimate\": None\n        ... }).show(truncate=False)\n        +-----------+----------+---+------------------------------+\n        |employee_id|name      |age|projects                      |\n        +-----------+----------+---+------------------------------+\n        |1          |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] |\n        |1          |Jane Doe  |25 |[{[{20}, {13}]}, {[{8}, {5}]}]|\n        +-----------+----------+---+------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; employee_df.transform(nested.select, {\n        ...     \"employee_id\": None,\n        ...     \"name\": None,\n        ...     \"age\": None,\n        ...     \"total_task_estimate\": nf.sum(\"projects!.tasks!.estimate\"),\n        ...     \"projects!.task_estimate_per_project\":\n        ...         lambda project: nf.sum(\"tasks!.estimate\", starting_level=project),\n        ... }).show(truncate=False)\n        +-----------+----------+---+-------------------+------------+\n        |employee_id|name      |age|total_task_estimate|projects    |\n        +-----------+----------+---+-------------------+------------+\n        |1          |John Smith|30 |29                 |[{13}, {16}]|\n        |1          |Jane Doe  |25 |46                 |[{33}, {13}]|\n        +-----------+----------+---+-------------------+------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2 : with all kind of nested structures*\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     1 as id,\n        ...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n        ...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n        ...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n        ...     ARRAY(\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n        ...     ) as s5\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s2!!: integer (nullable = false)\n         |-- s3!!.a: integer (nullable = false)\n         |-- s4!.a!: integer (nullable = false)\n         |-- s5!.a!.b.c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+----------+----------------+--------------+--------------------+------------------------------------+\n        |id |s1        |s2              |s3            |s4                  |s5                                  |\n        +---+----------+----------------+--------------+--------------------+------------------------------------+\n        |1  |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]|\n        +---+----------+----------------+--------------+--------------------+------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.sum(\"s1!.a\").alias(\"sum\")).show()\n        +---+\n        |sum|\n        +---+\n        |  3|\n        +---+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.sum(\"s2!!\").alias(\"sum\")).show()\n        +---+\n        |sum|\n        +---+\n        | 10|\n        +---+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.sum(\"s3!!.a\").alias(\"sum\")).show()\n        +---+\n        |sum|\n        +---+\n        |  3|\n        +---+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.sum(\"s4!.a!\").alias(\"sum\")).show()\n        +---+\n        |sum|\n        +---+\n        | 10|\n        +---+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.select(nf.sum(\"s5!.a!.b.c\").alias(\"sum\")).show()\n        +---+\n        |sum|\n        +---+\n        | 10|\n        +---+\n        &lt;BLANKLINE&gt;\n    \"\"\"  # noqa: E501\n    initial_value = f.lit(0).cast(\"BIGINT\")\n\n    def merge(acc: Column, x: Column) -&gt; Column:\n        return acc + x\n\n    return aggregate(\n        field_name,\n        initial_value=initial_value,\n        merge=merge,\n        starting_level=starting_level,\n    )\n</code></pre>"},{"location":"reference/schema_utils/","title":"spark_frame.schema_utils","text":"<p>This module contains methods useful for manipulating DataFrame schemas.</p>"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_from_json","title":"<code>schema_from_json(json_string: str) -&gt; StructType</code>","text":"<p>Parses the given json string representing a Spark :class:<code>StructType</code>.</p> <p>Only schema representing StructTypes can be parsed, this means that <code>schema_from_json(schema_to_json(data_type))</code> will crash if <code>data_type</code> is not a StructType.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>A string representation of a DataFrame schema.</p> required <p>Returns:</p> Type Description <code>StructType</code> <p>A StructType object representing the DataFrame schema</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; schema_from_json('''{\"fields\":[\n...     {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},\n...     {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}\n... ],\"type\":\"struct\"}''')\nStructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])\n&gt;&gt;&gt; schema_from_json('''{\"fields\":[\n...     {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},\n...     {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}\n... ],\"type\":\"struct\"}''')\nStructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])\n&gt;&gt;&gt; schema_from_json('''{\"fields\":[\n... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\n...     \"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"\n... }}\n... ],\"type\":\"struct\"}''')\nStructType([StructField('a', ArrayType(ShortType(), True), True)])\n</code></pre> <p>Error cases:</p> <pre><code>&gt;&gt;&gt; schema_from_json('\"integer\"')\nTraceback (most recent call last):\n  ...\nTypeError: string indices must be integers\n&gt;&gt;&gt; schema_from_json('''{\"keyType\":\"string\",\"type\":\"map\",\n... \"valueContainsNull\":true,\"valueType\":\"string\"}''')\nTraceback (most recent call last):\n  ...\nKeyError: 'fields'\n</code></pre> Source code in <code>spark_frame/schema_utils.py</code> <pre><code>def schema_from_json(json_string: str) -&gt; StructType:\n    \"\"\"Parses the given json string representing a Spark :class:`StructType`.\n\n    Only schema representing StructTypes can be parsed, this means that\n    `schema_from_json(schema_to_json(data_type))` will crash if `data_type` is not a StructType.\n\n    Args:\n        json_string: A string representation of a DataFrame schema.\n\n    Returns:\n        A StructType object representing the DataFrame schema\n\n    Examples:\n        &gt;&gt;&gt; schema_from_json('''{\"fields\":[\n        ...     {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},\n        ...     {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}\n        ... ],\"type\":\"struct\"}''')\n        StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])\n        &gt;&gt;&gt; schema_from_json('''{\"fields\":[\n        ...     {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},\n        ...     {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}\n        ... ],\"type\":\"struct\"}''')\n        StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])\n        &gt;&gt;&gt; schema_from_json('''{\"fields\":[\n        ... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\n        ...     \"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"\n        ... }}\n        ... ],\"type\":\"struct\"}''')\n        StructType([StructField('a', ArrayType(ShortType(), True), True)])\n\n        **Error cases:**\n\n        &gt;&gt;&gt; schema_from_json('\"integer\"') # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        TypeError: string indices must be integers\n        &gt;&gt;&gt; schema_from_json('''{\"keyType\":\"string\",\"type\":\"map\",\n        ... \"valueContainsNull\":true,\"valueType\":\"string\"}''') # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        KeyError: 'fields'\n\n    \"\"\"\n    return StructType.fromJson(json.loads(json_string))\n</code></pre>"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_from_simple_string","title":"<code>schema_from_simple_string(schema_string: str) -&gt; DataType</code>","text":"<p>Parses the given data type string to a :class:<code>DataType</code>. The data type string format equals pyspark.sql.types.DataType.simpleString, except that the top level struct type can omit the <code>struct&lt;&gt;</code>. This method requires the SparkSession to have already been instantiated.</p> <p>Parameters:</p> Name Type Description Default <code>schema_string</code> <code>str</code> <p>A simpleString representing a DataFrame schema.</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>A DataType object representing the DataFrame schema.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If no SparkContext has been instantiated first.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; schema_from_simple_string(\"int \")\nIntegerType()\n&gt;&gt;&gt; schema_from_simple_string(\"INT \")\nIntegerType()\n&gt;&gt;&gt; schema_from_simple_string(\"a: byte, b: decimal(  16 , 8   ) \")\nStructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])\n&gt;&gt;&gt; schema_from_simple_string(\"a DOUBLE, b STRING\")\nStructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])\n&gt;&gt;&gt; schema_from_simple_string(\"a: array&lt; short&gt;\")\nStructType([StructField('a', ArrayType(ShortType(), True), True)])\n&gt;&gt;&gt; schema_from_simple_string(\" map&lt;string , string &gt; \")\nMapType(StringType(), StringType(), True)\n</code></pre> <p>Error cases:</p> <pre><code>&gt;&gt;&gt; schema_from_simple_string(\"blabla\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n&gt;&gt;&gt; schema_from_simple_string(\"a: int,\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n&gt;&gt;&gt; schema_from_simple_string(\"array&lt;int\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n&gt;&gt;&gt; schema_from_simple_string(\"map&lt;int, boolean&gt;&gt;\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n</code></pre> Source code in <code>spark_frame/schema_utils.py</code> <pre><code>def schema_from_simple_string(schema_string: str) -&gt; DataType:\n    \"\"\"Parses the given data type string to a :class:`DataType`. The data type string format equals\n    [pyspark.sql.types.DataType.simpleString][], except that the top level struct type can omit\n    the ``struct&lt;&gt;``.\n    This method requires the SparkSession to have already been instantiated.\n\n    Args:\n        schema_string: A simpleString representing a DataFrame schema.\n\n    Returns:\n        A DataType object representing the DataFrame schema.\n\n    Raises:\n        AssertionError: If no SparkContext has been instantiated first.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; schema_from_simple_string(\"int \")\n        IntegerType()\n        &gt;&gt;&gt; schema_from_simple_string(\"INT \")\n        IntegerType()\n        &gt;&gt;&gt; schema_from_simple_string(\"a: byte, b: decimal(  16 , 8   ) \")\n        StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])\n        &gt;&gt;&gt; schema_from_simple_string(\"a DOUBLE, b STRING\")\n        StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])\n        &gt;&gt;&gt; schema_from_simple_string(\"a: array&lt; short&gt;\")\n        StructType([StructField('a', ArrayType(ShortType(), True), True)])\n        &gt;&gt;&gt; schema_from_simple_string(\" map&lt;string , string &gt; \")\n        MapType(StringType(), StringType(), True)\n\n        **Error cases:**\n\n        &gt;&gt;&gt; schema_from_simple_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n        &gt;&gt;&gt; schema_from_simple_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n        &gt;&gt;&gt; schema_from_simple_string(\"array&lt;int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n        &gt;&gt;&gt; schema_from_simple_string(\"map&lt;int, boolean&gt;&gt;\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n\n    \"\"\"\n    sc = SparkContext._active_spark_context  # noqa: SLF001\n    assert_true(sc is not None, \"No SparkContext has been instantiated yet\")\n    return _parse_datatype_string(schema_string)\n</code></pre>"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_to_json","title":"<code>schema_to_json(schema: DataType) -&gt; str</code>","text":"<p>Convert the given datatype into a json string.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>DataType</code> <p>A DataFrame schema.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A single-line json string representing the DataFrame schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql.types import *\n&gt;&gt;&gt; schema_to_json(IntegerType())\n'\"integer\"'\n&gt;&gt;&gt; schema_to_json(StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]))\n'{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}],\"type\":\"struct\"}'\n&gt;&gt;&gt; schema_to_json(StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]))\n'{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'\n&gt;&gt;&gt; schema_to_json(StructType([StructField('a', ArrayType(ShortType(), True), True)]))\n'{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"}}],\"type\":\"struct\"}'\n&gt;&gt;&gt; schema_to_json(MapType(StringType(), StringType(), True))\n'{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}'\n</code></pre> Source code in <code>spark_frame/schema_utils.py</code> <pre><code>def schema_to_json(schema: DataType) -&gt; str:\n    \"\"\"Convert the given datatype into a json string.\n\n    Args:\n        schema: A DataFrame schema.\n\n    Returns:\n        A single-line json string representing the DataFrame schema.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql.types import *\n        &gt;&gt;&gt; schema_to_json(IntegerType())\n        '\"integer\"'\n        &gt;&gt;&gt; schema_to_json(StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]))\n        '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}],\"type\":\"struct\"}'\n        &gt;&gt;&gt; schema_to_json(StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]))\n        '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'\n        &gt;&gt;&gt; schema_to_json(StructType([StructField('a', ArrayType(ShortType(), True), True)]))\n        '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"}}],\"type\":\"struct\"}'\n        &gt;&gt;&gt; schema_to_json(MapType(StringType(), StringType(), True))\n        '{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}'\n\n    \"\"\"\n    return schema.json()\n</code></pre>"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_to_pretty_json","title":"<code>schema_to_pretty_json(schema: DataType) -&gt; str</code>","text":"<p>Convert the given datatype into a pretty (indented) json string.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>DataType</code> <p>A DataFrame schema.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-line indented json string representing the DataFrame schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql.types import *\n&gt;&gt;&gt; print(schema_to_pretty_json(IntegerType()))\n\"integer\"\n&gt;&gt;&gt; print(schema_to_pretty_json(StructType([StructField('a', ArrayType(ShortType(), True), True)])))\n{\n  \"fields\": [\n    {\n      \"metadata\": {},\n      \"name\": \"a\",\n      \"nullable\": true,\n      \"type\": {\n        \"containsNull\": true,\n        \"elementType\": \"short\",\n        \"type\": \"array\"\n      }\n    }\n  ],\n  \"type\": \"struct\"\n}\n&gt;&gt;&gt; print(schema_to_pretty_json(MapType(StringType(), StringType(), True)))\n{\n  \"keyType\": \"string\",\n  \"type\": \"map\",\n  \"valueContainsNull\": true,\n  \"valueType\": \"string\"\n}\n</code></pre> <p>:param schema: :return:</p> Source code in <code>spark_frame/schema_utils.py</code> <pre><code>def schema_to_pretty_json(schema: DataType) -&gt; str:\n    \"\"\"Convert the given datatype into a pretty (indented) json string.\n\n    Args:\n        schema: A DataFrame schema.\n\n    Returns:\n        A multi-line indented json string representing the DataFrame schema.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql.types import *\n        &gt;&gt;&gt; print(schema_to_pretty_json(IntegerType()))\n        \"integer\"\n        &gt;&gt;&gt; print(schema_to_pretty_json(StructType([StructField('a', ArrayType(ShortType(), True), True)])))\n        {\n          \"fields\": [\n            {\n              \"metadata\": {},\n              \"name\": \"a\",\n              \"nullable\": true,\n              \"type\": {\n                \"containsNull\": true,\n                \"elementType\": \"short\",\n                \"type\": \"array\"\n              }\n            }\n          ],\n          \"type\": \"struct\"\n        }\n        &gt;&gt;&gt; print(schema_to_pretty_json(MapType(StringType(), StringType(), True)))\n        {\n          \"keyType\": \"string\",\n          \"type\": \"map\",\n          \"valueContainsNull\": true,\n          \"valueType\": \"string\"\n        }\n\n    :param schema:\n    :return:\n    \"\"\"\n    schema_dict = json.loads(schema.json())\n    return json.dumps(schema_dict, indent=2, sort_keys=True)\n</code></pre>"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_to_simple_string","title":"<code>schema_to_simple_string(schema: DataType) -&gt; str</code>","text":"<p>Convert the given datatype into a simple sql string. This method is equivalent to calling <code>schema.simpleString()</code> directly.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>DataType</code> <p>A DataFrame schema.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A simpleString representing the DataFrame schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql.types import *\n&gt;&gt;&gt; schema_to_simple_string(IntegerType())\n'int'\n&gt;&gt;&gt; schema_to_simple_string(StructType([\n...     StructField('a', ByteType(), True),\n...     StructField('b', DecimalType(16,8), True)\n... ]))\n'struct&lt;a:tinyint,b:decimal(16,8)&gt;'\n&gt;&gt;&gt; schema_to_simple_string(StructType([\n...     StructField('a', DoubleType(), True),\n...     StructField('b', StringType(), True)\n... ]))\n'struct&lt;a:double,b:string&gt;'\n&gt;&gt;&gt; schema_to_simple_string(StructType([StructField('a', ArrayType(ShortType(), True), True)]))\n'struct&lt;a:array&lt;smallint&gt;&gt;'\n&gt;&gt;&gt; schema_to_simple_string(MapType(StringType(), StringType(), True))\n'map&lt;string,string&gt;'\n</code></pre> Source code in <code>spark_frame/schema_utils.py</code> <pre><code>def schema_to_simple_string(schema: DataType) -&gt; str:\n    \"\"\"Convert the given datatype into a simple sql string.\n    This method is equivalent to calling [`schema.simpleString()`][pyspark.sql.types.DataType.simpleString] directly.\n\n    Args:\n        schema: A DataFrame schema.\n\n    Returns:\n        A simpleString representing the DataFrame schema.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql.types import *\n        &gt;&gt;&gt; schema_to_simple_string(IntegerType())\n        'int'\n        &gt;&gt;&gt; schema_to_simple_string(StructType([\n        ...     StructField('a', ByteType(), True),\n        ...     StructField('b', DecimalType(16,8), True)\n        ... ]))\n        'struct&lt;a:tinyint,b:decimal(16,8)&gt;'\n        &gt;&gt;&gt; schema_to_simple_string(StructType([\n        ...     StructField('a', DoubleType(), True),\n        ...     StructField('b', StringType(), True)\n        ... ]))\n        'struct&lt;a:double,b:string&gt;'\n        &gt;&gt;&gt; schema_to_simple_string(StructType([StructField('a', ArrayType(ShortType(), True), True)]))\n        'struct&lt;a:array&lt;smallint&gt;&gt;'\n        &gt;&gt;&gt; schema_to_simple_string(MapType(StringType(), StringType(), True))\n        'map&lt;string,string&gt;'\n    \"\"\"\n    return schema.simpleString()\n</code></pre>"},{"location":"reference/transformations/","title":"spark_frame.transformations","text":"<p>Unlike those in spark_frame.functions, the methods in this module all take at least one DataFrame as argument and return a new transformed DataFrame. These methods generally offer higher order transformation that requires to inspect the schema or event the content of the input DataFrame(s) before generating the next transformation. Those are typically generic operations  that cannot be implemented with one single SQL query.</p> <p>Tip</p> <p>Since Spark 3.3.0, all transformations can be inlined using  DataFrame.transform, like this:</p> <p><pre><code>df.transform(flatten).withColumn(\n    \"base_stats.Total\",\n    f.col(\"`base_stats.Attack`\") + f.col(\"`base_stats.Defense`\") + f.col(\"`base_stats.HP`\") +\n    f.col(\"`base_stats.Sp Attack`\") + f.col(\"`base_stats.Sp Defense`\") + f.col(\"`base_stats.Speed`\")\n).transform(unflatten).show(vertical=True, truncate=False)\n</code></pre> This example is taken</p>"},{"location":"reference/transformations/#spark_frame.transformations_impl.analyze.analyze","title":"<code>analyze(df: DataFrame, group_by: Optional[Union[str, List[str]]] = None, group_alias: str = 'group', _aggs: Optional[List[Callable[[str, StructField, int], Column]]] = None) -&gt; DataFrame</code>","text":"<p>Analyze a DataFrame by computing various stats for each column.</p> <p>By default, it returns a DataFrame with one row per column and the following columns (but the columns computed can be customized, see the Customization section below):</p> <ul> <li><code>column_number</code>: Number of the column (useful for sorting)</li> <li><code>column_name</code>: Name of the column</li> <li><code>column_type</code>: Type of the column</li> <li><code>count</code>: Number of rows in the column, it is equal to the number of rows in the table, except for columns nested   <code>inside</code> arrays for which it may be different</li> <li><code>count_distinct</code>: Number of distinct values</li> <li><code>count_null</code>: Number of null values</li> <li><code>min</code>: smallest value</li> <li><code>max</code>: largest value</li> </ul>"},{"location":"reference/transformations/#spark_frame.transformations_impl.analyze.analyze--implementation-details","title":"Implementation details","text":"<ul> <li>Structs are flattened with a <code>.</code> after their name.</li> <li>Arrays are unnested with a <code>!</code> character after their name, which is why they may have a different count.</li> <li>Null values are not counted in the count_distinct column.</li> </ul> <p>Limitation: Map type is not supported</p> <p>This method currently does not work on columns of type Map. A possible workaround is to use <code>spark_frame.transformations.convert_all_maps_to_arrays</code> before using it.</p>"},{"location":"reference/transformations/#spark_frame.transformations_impl.analyze.analyze--grouping","title":"Grouping","text":"<p>With the <code>group_by</code> option, users can specify one or multiple columns for which the statistics will be grouped. If this option is used, an extra column \"group\" of type struct will be added to output DataFrame. See the examples below.</p> <p>Limitation: group_by only works on non-repeated fields</p> <p>Currently, the <code>group_by</code> option only works with non-repeated fields. Using it on repeated fields will lead to an unspecified error.</p>"},{"location":"reference/transformations/#spark_frame.transformations_impl.analyze.analyze--customization","title":"Customization","text":"<p>By default, this method will compute for each column the aggregations listed in <code>spark_frame.transformation_impl.analyze.default_aggs</code>, but users can change this and even add their own custom aggregation by passing the argument <code>_agg</code>, a list of aggregation functions with the following signature: <code>(col: str, schema_field: StructField, col_num: int) -&gt; Column</code></p> <p>Examples of aggregation methods can be found in the module <code>spark_frame.transformation_impl.analyze_aggs</code></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>group_by</code> <code>Optional[Union[str, List[str]]]</code> <p>A list of column names on which the aggregations will be grouped</p> <code>None</code> <code>group_alias</code> <code>str</code> <p>The alias to use for the struct column that will contain the <code>group_by</code> columns, if any.</p> <code>'group'</code> <code>_aggs</code> <code>Optional[List[Callable[[str, StructField, int], Column]]]</code> <p>A list of aggregation to override the default aggregation made by the function</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame containing descriptive statistics about the input DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_frame.transformations_impl.analyze import __get_test_df\n</code></pre> <pre><code>&gt;&gt;&gt; df = __get_test_df()\n&gt;&gt;&gt; df.show()\n+---+----------+---------------+------------+\n| id|      name|          types|   evolution|\n+---+----------+---------------+------------+\n|  1| Bulbasaur|[Grass, Poison]|{true, NULL}|\n|  2|   Ivysaur|[Grass, Poison]|   {true, 1}|\n|  3|  Venusaur|[Grass, Poison]|  {false, 2}|\n|  4|Charmander|         [Fire]|{true, NULL}|\n|  5|Charmeleon|         [Fire]|   {true, 4}|\n|  6| Charizard| [Fire, Flying]|  {false, 5}|\n|  7|  Squirtle|        [Water]|{true, NULL}|\n|  8| Wartortle|        [Water]|   {true, 7}|\n|  9| Blastoise|        [Water]|  {false, 8}|\n+---+----------+---------------+------------+\n</code></pre> <pre><code>&gt;&gt;&gt; analyzed_df = analyze(df)\nAnalyzing 5 columns ...\n&gt;&gt;&gt; analyzed_df.show(truncate=False)  # noqa: E501\n+-------------+----------------------+-----------+-----+--------------+----------+---------+---------+\n|column_number|column_name           |column_type|count|count_distinct|count_null|min      |max      |\n+-------------+----------------------+-----------+-----+--------------+----------+---------+---------+\n|0            |id                    |INTEGER    |9    |9             |0         |1        |9        |\n|1            |name                  |STRING     |9    |9             |0         |Blastoise|Wartortle|\n|2            |types!                |STRING     |13   |5             |0         |Fire     |Water    |\n|3            |evolution.can_evolve  |BOOLEAN    |9    |2             |0         |false    |true     |\n|4            |evolution.evolves_from|INTEGER    |9    |6             |3         |1        |8        |\n+-------------+----------------------+-----------+-----+--------------+----------+---------+---------+\n</code></pre> <p>Analyze a DataFrame with custom aggregation methods</p> <p>Custom aggregation methods can be defined as method functions that take three arguments:     - <code>col</code>: the name of the Column that will be analyzed     - <code>struct_field</code>: a Column name     - <code>col_num</code>: the number of the column</p> <pre><code>&gt;&gt;&gt; from spark_frame.transformations_impl import analyze_aggs\n&gt;&gt;&gt; from pyspark.sql.types import IntegerType\n&gt;&gt;&gt; def total(col: str, struct_field: StructField, _: int) -&gt; Column:\n...     if struct_field.dataType == IntegerType():\n...         return f.sum(col).alias(\"total\")\n...     else:\n...         return f.lit(None).alias(\"total\")\n&gt;&gt;&gt; aggs = [\n...     analyze_aggs.column_number,\n...     analyze_aggs.column_name,\n...     analyze_aggs.count,\n...     analyze_aggs.count_distinct,\n...     analyze_aggs.count_null,\n...     total\n... ]\n&gt;&gt;&gt; analyzed_df = analyze(df, _aggs=aggs)\nAnalyzing 5 columns ...\n&gt;&gt;&gt; analyzed_df.show(truncate=False)  # noqa: E501\n+-------------+----------------------+-----+--------------+----------+-----+\n|column_number|column_name           |count|count_distinct|count_null|total|\n+-------------+----------------------+-----+--------------+----------+-----+\n|0            |id                    |9    |9             |0         |45   |\n|1            |name                  |9    |9             |0         |NULL |\n|2            |types!                |13   |5             |0         |NULL |\n|3            |evolution.can_evolve  |9    |2             |0         |NULL |\n|4            |evolution.evolves_from|9    |6             |3         |27   |\n+-------------+----------------------+-----+--------------+----------+-----+\n</code></pre> <p>Analyze a DataFrame grouped by a specific column</p> <p>Use the <code>group_by</code> to group the result by one or multiple columns</p> <pre><code>&gt;&gt;&gt; df = __get_test_df().withColumn(\"main_type\", f.expr(\"types[0]\"))\n&gt;&gt;&gt; df.show()\n+---+----------+---------------+------------+---------+\n| id|      name|          types|   evolution|main_type|\n+---+----------+---------------+------------+---------+\n|  1| Bulbasaur|[Grass, Poison]|{true, NULL}|    Grass|\n|  2|   Ivysaur|[Grass, Poison]|   {true, 1}|    Grass|\n|  3|  Venusaur|[Grass, Poison]|  {false, 2}|    Grass|\n|  4|Charmander|         [Fire]|{true, NULL}|     Fire|\n|  5|Charmeleon|         [Fire]|   {true, 4}|     Fire|\n|  6| Charizard| [Fire, Flying]|  {false, 5}|     Fire|\n|  7|  Squirtle|        [Water]|{true, NULL}|    Water|\n|  8| Wartortle|        [Water]|   {true, 7}|    Water|\n|  9| Blastoise|        [Water]|  {false, 8}|    Water|\n+---+----------+---------------+------------+---------+\n\n&gt;&gt;&gt; analyzed_df = analyze(df, group_by=\"main_type\", _aggs=aggs)\nAnalyzing 5 columns ...\n&gt;&gt;&gt; analyzed_df.orderBy(\"`group`.main_type\", \"column_number\").show(truncate=False)\n+-------+-------------+----------------------+-----+--------------+----------+-----+\n|group  |column_number|column_name           |count|count_distinct|count_null|total|\n+-------+-------------+----------------------+-----+--------------+----------+-----+\n|{Fire} |0            |id                    |3    |3             |0         |15   |\n|{Fire} |1            |name                  |3    |3             |0         |NULL |\n|{Fire} |2            |types!                |4    |2             |0         |NULL |\n|{Fire} |3            |evolution.can_evolve  |3    |2             |0         |NULL |\n|{Fire} |4            |evolution.evolves_from|3    |2             |1         |9    |\n|{Grass}|0            |id                    |3    |3             |0         |6    |\n|{Grass}|1            |name                  |3    |3             |0         |NULL |\n|{Grass}|2            |types!                |6    |2             |0         |NULL |\n|{Grass}|3            |evolution.can_evolve  |3    |2             |0         |NULL |\n|{Grass}|4            |evolution.evolves_from|3    |2             |1         |3    |\n|{Water}|0            |id                    |3    |3             |0         |24   |\n|{Water}|1            |name                  |3    |3             |0         |NULL |\n|{Water}|2            |types!                |3    |1             |0         |NULL |\n|{Water}|3            |evolution.can_evolve  |3    |2             |0         |NULL |\n|{Water}|4            |evolution.evolves_from|3    |2             |1         |15   |\n+-------+-------------+----------------------+-----+--------------+----------+-----+\n</code></pre> Source code in <code>spark_frame/transformations_impl/analyze.py</code> <pre><code>def analyze(\n    df: DataFrame,\n    group_by: Optional[Union[str, List[str]]] = None,\n    group_alias: str = \"group\",\n    _aggs: Optional[List[Callable[[str, StructField, int], Column]]] = None,\n) -&gt; DataFrame:\n    \"\"\"Analyze a DataFrame by computing various stats for each column.\n\n    By default, it returns a DataFrame with one row per column and the following columns\n    (but the columns computed can be customized, see the Customization section below):\n\n    - `column_number`: Number of the column (useful for sorting)\n    - `column_name`: Name of the column\n    - `column_type`: Type of the column\n    - `count`: Number of rows in the column, it is equal to the number of rows in the table, except for columns nested\n      `inside` arrays for which it may be different\n    - `count_distinct`: Number of distinct values\n    - `count_null`: Number of null values\n    - `min`: smallest value\n    - `max`: largest value\n\n    Implementation details\n    ----------------------\n    - Structs are flattened with a `.` after their name.\n    - Arrays are unnested with a `!` character after their name, which is why they may have a different count.\n    - Null values are not counted in the count_distinct column.\n\n    !!! warning \"Limitation: Map type is not supported\"\n        This method currently does not work on columns of type Map.\n        A possible workaround is to use [`spark_frame.transformations.convert_all_maps_to_arrays`]\n        [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays]\n        before using it.\n\n    Grouping\n    --------\n    With the `group_by` option, users can specify one or multiple columns for which the statistics will be grouped.\n    If this option is used, an extra column \"group\" of type struct will be added to output DataFrame.\n    See the examples below.\n\n    !!! warning \"Limitation: group_by only works on non-repeated fields\"\n        Currently, the `group_by` option only works with non-repeated fields.\n        Using it on repeated fields will lead to an unspecified error.\n\n    Customization\n    -------------\n    By default, this method will compute for each column the aggregations listed in\n    `spark_frame.transformation_impl.analyze.default_aggs`, but users can change this and even add their\n    own custom aggregation by passing the argument `_agg`, a list of aggregation functions with the following\n    signature: `(col: str, schema_field: StructField, col_num: int) -&gt; Column`\n\n    Examples of aggregation methods can be found in the module `spark_frame.transformation_impl.analyze_aggs`\n\n    Args:\n        df: A Spark DataFrame\n        group_by: A list of column names on which the aggregations will be grouped\n        group_alias: The alias to use for the struct column that will contain the `group_by` columns, if any.\n        _aggs: A list of aggregation to override the default aggregation made by the function\n\n    Returns:\n        A new DataFrame containing descriptive statistics about the input DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from spark_frame.transformations_impl.analyze import __get_test_df\n\n        &gt;&gt;&gt; df = __get_test_df()\n        &gt;&gt;&gt; df.show()\n        +---+----------+---------------+------------+\n        | id|      name|          types|   evolution|\n        +---+----------+---------------+------------+\n        |  1| Bulbasaur|[Grass, Poison]|{true, NULL}|\n        |  2|   Ivysaur|[Grass, Poison]|   {true, 1}|\n        |  3|  Venusaur|[Grass, Poison]|  {false, 2}|\n        |  4|Charmander|         [Fire]|{true, NULL}|\n        |  5|Charmeleon|         [Fire]|   {true, 4}|\n        |  6| Charizard| [Fire, Flying]|  {false, 5}|\n        |  7|  Squirtle|        [Water]|{true, NULL}|\n        |  8| Wartortle|        [Water]|   {true, 7}|\n        |  9| Blastoise|        [Water]|  {false, 8}|\n        +---+----------+---------------+------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; analyzed_df = analyze(df)\n        Analyzing 5 columns ...\n        &gt;&gt;&gt; analyzed_df.show(truncate=False)  # noqa: E501\n        +-------------+----------------------+-----------+-----+--------------+----------+---------+---------+\n        |column_number|column_name           |column_type|count|count_distinct|count_null|min      |max      |\n        +-------------+----------------------+-----------+-----+--------------+----------+---------+---------+\n        |0            |id                    |INTEGER    |9    |9             |0         |1        |9        |\n        |1            |name                  |STRING     |9    |9             |0         |Blastoise|Wartortle|\n        |2            |types!                |STRING     |13   |5             |0         |Fire     |Water    |\n        |3            |evolution.can_evolve  |BOOLEAN    |9    |2             |0         |false    |true     |\n        |4            |evolution.evolves_from|INTEGER    |9    |6             |3         |1        |8        |\n        +-------------+----------------------+-----------+-----+--------------+----------+---------+---------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Analyze a DataFrame with custom aggregation methods\n        Custom aggregation methods can be defined as method functions that take three arguments:\n            - `col`: the name of the Column that will be analyzed\n            - `struct_field`: a Column name\n            - `col_num`: the number of the column\n        &gt;&gt;&gt; from spark_frame.transformations_impl import analyze_aggs\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; def total(col: str, struct_field: StructField, _: int) -&gt; Column:\n        ...     if struct_field.dataType == IntegerType():\n        ...         return f.sum(col).alias(\"total\")\n        ...     else:\n        ...         return f.lit(None).alias(\"total\")\n        &gt;&gt;&gt; aggs = [\n        ...     analyze_aggs.column_number,\n        ...     analyze_aggs.column_name,\n        ...     analyze_aggs.count,\n        ...     analyze_aggs.count_distinct,\n        ...     analyze_aggs.count_null,\n        ...     total\n        ... ]\n        &gt;&gt;&gt; analyzed_df = analyze(df, _aggs=aggs)\n        Analyzing 5 columns ...\n        &gt;&gt;&gt; analyzed_df.show(truncate=False)  # noqa: E501\n        +-------------+----------------------+-----+--------------+----------+-----+\n        |column_number|column_name           |count|count_distinct|count_null|total|\n        +-------------+----------------------+-----+--------------+----------+-----+\n        |0            |id                    |9    |9             |0         |45   |\n        |1            |name                  |9    |9             |0         |NULL |\n        |2            |types!                |13   |5             |0         |NULL |\n        |3            |evolution.can_evolve  |9    |2             |0         |NULL |\n        |4            |evolution.evolves_from|9    |6             |3         |27   |\n        +-------------+----------------------+-----+--------------+----------+-----+\n        &lt;BLANKLINE&gt;\n\n    Examples: Analyze a DataFrame grouped by a specific column\n        Use the `group_by` to group the result by one or multiple columns\n        &gt;&gt;&gt; df = __get_test_df().withColumn(\"main_type\", f.expr(\"types[0]\"))\n        &gt;&gt;&gt; df.show()\n        +---+----------+---------------+------------+---------+\n        | id|      name|          types|   evolution|main_type|\n        +---+----------+---------------+------------+---------+\n        |  1| Bulbasaur|[Grass, Poison]|{true, NULL}|    Grass|\n        |  2|   Ivysaur|[Grass, Poison]|   {true, 1}|    Grass|\n        |  3|  Venusaur|[Grass, Poison]|  {false, 2}|    Grass|\n        |  4|Charmander|         [Fire]|{true, NULL}|     Fire|\n        |  5|Charmeleon|         [Fire]|   {true, 4}|     Fire|\n        |  6| Charizard| [Fire, Flying]|  {false, 5}|     Fire|\n        |  7|  Squirtle|        [Water]|{true, NULL}|    Water|\n        |  8| Wartortle|        [Water]|   {true, 7}|    Water|\n        |  9| Blastoise|        [Water]|  {false, 8}|    Water|\n        +---+----------+---------------+------------+---------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; analyzed_df = analyze(df, group_by=\"main_type\", _aggs=aggs)\n        Analyzing 5 columns ...\n        &gt;&gt;&gt; analyzed_df.orderBy(\"`group`.main_type\", \"column_number\").show(truncate=False)\n        +-------+-------------+----------------------+-----+--------------+----------+-----+\n        |group  |column_number|column_name           |count|count_distinct|count_null|total|\n        +-------+-------------+----------------------+-----+--------------+----------+-----+\n        |{Fire} |0            |id                    |3    |3             |0         |15   |\n        |{Fire} |1            |name                  |3    |3             |0         |NULL |\n        |{Fire} |2            |types!                |4    |2             |0         |NULL |\n        |{Fire} |3            |evolution.can_evolve  |3    |2             |0         |NULL |\n        |{Fire} |4            |evolution.evolves_from|3    |2             |1         |9    |\n        |{Grass}|0            |id                    |3    |3             |0         |6    |\n        |{Grass}|1            |name                  |3    |3             |0         |NULL |\n        |{Grass}|2            |types!                |6    |2             |0         |NULL |\n        |{Grass}|3            |evolution.can_evolve  |3    |2             |0         |NULL |\n        |{Grass}|4            |evolution.evolves_from|3    |2             |1         |3    |\n        |{Water}|0            |id                    |3    |3             |0         |24   |\n        |{Water}|1            |name                  |3    |3             |0         |NULL |\n        |{Water}|2            |types!                |3    |1             |0         |NULL |\n        |{Water}|3            |evolution.can_evolve  |3    |2             |0         |NULL |\n        |{Water}|4            |evolution.evolves_from|3    |2             |1         |15   |\n        +-------+-------------+----------------------+-----+--------------+----------+-----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    if _aggs is None:\n        _aggs = default_aggs\n    if group_by is None:\n        group_by = []\n    if isinstance(group_by, str):\n        group_by = [group_by]\n\n    flat_fields = nested.fields(df)\n    fields_to_drop = [field for field in flat_fields if is_sub_field_or_equal_to_any(field, group_by)]\n    nb_cols = len(flat_fields) - len(fields_to_drop)\n    print(f\"Analyzing {nb_cols} columns ...\")\n\n    if len(group_by) &gt; 0:\n        df = df.withColumn(group_alias, f.struct(*group_by))\n        group = [group_alias]\n    else:\n        group = []\n\n    flattened_dfs = unnest_all_fields(df, keep_columns=group)\n    index_by_field = {field: index for index, field in enumerate(flat_fields)}\n    analyzed_dfs = [\n        _analyze_flat_df(flat_df.drop(*fields_to_drop), index_by_field, group_by=group, aggs=_aggs)\n        for flat_df in flattened_dfs.values()\n    ]\n\n    union_df = union_dataframes(*analyzed_dfs)\n    return union_df.orderBy(\"column_number\")\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays","title":"<code>convert_all_maps_to_arrays(df: DataFrame) -&gt; DataFrame</code>","text":"<p>Transform all columns of type <code>Map&lt;K,V&gt;</code> inside the given DataFrame into <code>ARRAY&lt;STRUCT&lt;key: K, value: V&gt;&gt;</code>. This transformation works recursively on every nesting level.</p> <p>Info</p> <p>This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame in which all maps have been replaced with arrays of entries.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(MAP(1, STRUCT(MAP(1, \"a\") as m2))) as m1')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- m1: array (nullable = false)\n |    |-- element: map (containsNull = false)\n |    |    |-- key: integer\n |    |    |-- value: struct (valueContainsNull = false)\n |    |    |    |-- m2: map (nullable = false)\n |    |    |    |    |-- key: integer\n |    |    |    |    |-- value: string (valueContainsNull = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------------+\n| id|                 m1|\n+---+-------------------+\n|  1|[{1 -&gt; {{1 -&gt; a}}}]|\n+---+-------------------+\n\n&gt;&gt;&gt; res_df = convert_all_maps_to_arrays(df)\n&gt;&gt;&gt; res_df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- m1: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- key: integer (nullable = false)\n |    |    |    |-- value: struct (nullable = false)\n |    |    |    |    |-- m2: array (nullable = false)\n |    |    |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |    |    |-- key: integer (nullable = false)\n |    |    |    |    |    |    |-- value: string (nullable = false)\n\n&gt;&gt;&gt; res_df.show()\n+---+-------------------+\n| id|                 m1|\n+---+-------------------+\n|  1|[[{1, {[{1, a}]}}]]|\n+---+-------------------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/convert_all_maps_to_arrays.py</code> <pre><code>def convert_all_maps_to_arrays(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Transform all columns of type `Map&lt;K,V&gt;` inside the given DataFrame into `ARRAY&lt;STRUCT&lt;key: K, value: V&gt;&gt;`.\n    This transformation works recursively on every nesting level.\n\n    !!! info\n        This method is compatible with any schema. It recursively applies on structs, arrays and maps\n        and is compatible with field names containing special characters.\n\n    Args:\n        df: A Spark DataFrame\n\n    Returns:\n        A new DataFrame in which all maps have been replaced with arrays of entries.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(MAP(1, STRUCT(MAP(1, \"a\") as m2))) as m1')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- m1: array (nullable = false)\n         |    |-- element: map (containsNull = false)\n         |    |    |-- key: integer\n         |    |    |-- value: struct (valueContainsNull = false)\n         |    |    |    |-- m2: map (nullable = false)\n         |    |    |    |    |-- key: integer\n         |    |    |    |    |-- value: string (valueContainsNull = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------------+\n        | id|                 m1|\n        +---+-------------------+\n        |  1|[{1 -&gt; {{1 -&gt; a}}}]|\n        +---+-------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res_df = convert_all_maps_to_arrays(df)\n        &gt;&gt;&gt; res_df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- m1: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: struct (containsNull = false)\n         |    |    |    |-- key: integer (nullable = false)\n         |    |    |    |-- value: struct (nullable = false)\n         |    |    |    |    |-- m2: array (nullable = false)\n         |    |    |    |    |    |-- element: struct (containsNull = false)\n         |    |    |    |    |    |    |-- key: integer (nullable = false)\n         |    |    |    |    |    |    |-- value: string (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res_df.show()\n        +---+-------------------+\n        | id|                 m1|\n        +---+-------------------+\n        |  1|[[{1, {[{1, a}]}}]]|\n        +---+-------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n\n    def map_to_arrays(col: Column, data_type: DataType) -&gt; Optional[Column]:\n        if isinstance(data_type, MapType):\n            return f.map_entries(col)\n        else:\n            return None\n\n    return transform_all_fields(df, map_to_arrays)\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.flatten.flatten","title":"<code>flatten(df: DataFrame, struct_separator: str = '.') -&gt; DataFrame</code>","text":"<p>Flatten all the struct columns of a Spark DataFrame. Nested fields names will be joined together using the specified separator</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>struct_separator</code> <code>str</code> <p>A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots</p> <code>'.'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A flattened DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...         [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})],\n...         \"id INT, s STRUCT&lt;a:INT, b:STRUCT&lt;c:INT, d:INT&gt;&gt;\"\n...      )\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s: struct (nullable = true)\n |    |-- a: integer (nullable = true)\n |    |-- b: struct (nullable = true)\n |    |    |-- c: integer (nullable = true)\n |    |    |-- d: integer (nullable = true)\n\n&gt;&gt;&gt; flatten(df).printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.a: integer (nullable = true)\n |-- s.b.c: integer (nullable = true)\n |-- s.b.d: integer (nullable = true)\n\n&gt;&gt;&gt; df = spark.createDataFrame(\n...         [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})],\n...         \"id INT, `s.s1` STRUCT&lt;`a.a1`:INT, `b.b1`:STRUCT&lt;`c.c1`:INT, `d.d1`:INT&gt;&gt;\"\n... )\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1: struct (nullable = true)\n |    |-- a.a1: integer (nullable = true)\n |    |-- b.b1: struct (nullable = true)\n |    |    |-- c.c1: integer (nullable = true)\n |    |    |-- d.d1: integer (nullable = true)\n\n&gt;&gt;&gt; flatten(df, \"?\").printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1?a.a1: integer (nullable = true)\n |-- s.s1?b.b1?c.c1: integer (nullable = true)\n |-- s.s1?b.b1?d.d1: integer (nullable = true)\n</code></pre> Source code in <code>spark_frame/transformations_impl/flatten.py</code> <pre><code>def flatten(df: DataFrame, struct_separator: str = \".\") -&gt; DataFrame:\n    \"\"\"Flatten all the struct columns of a Spark [DataFrame][pyspark.sql.DataFrame].\n    Nested fields names will be joined together using the specified separator\n\n    Args:\n        df: A Spark DataFrame\n        struct_separator: A string used to separate the structs names from their elements.\n            It might be useful to change the separator when some DataFrame's column names already contain dots\n\n    Returns:\n        A flattened DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...         [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})],\n        ...         \"id INT, s STRUCT&lt;a:INT, b:STRUCT&lt;c:INT, d:INT&gt;&gt;\"\n        ...      )\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s: struct (nullable = true)\n         |    |-- a: integer (nullable = true)\n         |    |-- b: struct (nullable = true)\n         |    |    |-- c: integer (nullable = true)\n         |    |    |-- d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; flatten(df).printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.a: integer (nullable = true)\n         |-- s.b.c: integer (nullable = true)\n         |-- s.b.d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...         [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})],\n        ...         \"id INT, `s.s1` STRUCT&lt;`a.a1`:INT, `b.b1`:STRUCT&lt;`c.c1`:INT, `d.d1`:INT&gt;&gt;\"\n        ... )\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1: struct (nullable = true)\n         |    |-- a.a1: integer (nullable = true)\n         |    |-- b.b1: struct (nullable = true)\n         |    |    |-- c.c1: integer (nullable = true)\n         |    |    |-- d.d1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; flatten(df, \"?\").printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1?a.a1: integer (nullable = true)\n         |-- s.s1?b.b1?c.c1: integer (nullable = true)\n         |-- s.s1?b.b1?d.d1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    # The idea is to recursively write a \"SELECT s.b.c as `s.b.c`\" for each nested column.\n    cols = []\n\n    def expand_struct(struct: StructType, col_stack: List[str]) -&gt; None:\n        for field in struct:\n            if isinstance(field.dataType, StructType):\n                struct_field = field.dataType\n                expand_struct(struct_field, [*col_stack, field.name])\n            else:\n                column = f.col(\".\".join(quote_columns([*col_stack, field.name])))\n                cols.append(column.alias(struct_separator.join([*col_stack, field.name])))\n\n    expand_struct(df.schema, col_stack=[])\n    return df.select(cols)\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.flatten_all_arrays.flatten_all_arrays","title":"<code>flatten_all_arrays(df: DataFrame) -&gt; DataFrame</code>","text":"<p>Flatten all columns of type <code>ARRAY&lt;ARRAY&lt;T&gt;&gt;</code> inside the given DataFrame into <code>ARRAY&lt;&lt;T&gt;&gt;&gt;</code>. This transformation works recursively on every nesting level.</p> <p>Info</p> <p>This method is compatible with any schema. It recursively applies on structs, arrays and maps and accepts field names containing dots (<code>.</code>), exclamation marks (<code>!</code>) or percentage (<code>%</code>).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame in which all arrays of array have been flattened</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(ARRAY(ARRAY(1, 2), ARRAY(3)), ARRAY(ARRAY(4), ARRAY(5))) as a')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- a: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: array (containsNull = false)\n |    |    |    |-- element: integer (containsNull = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+---+---------------------------+\n|id |a                          |\n+---+---------------------------+\n|1  |[[[1, 2], [3]], [[4], [5]]]|\n+---+---------------------------+\n\n&gt;&gt;&gt; res_df = flatten_all_arrays(df)\n&gt;&gt;&gt; res_df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- a: array (nullable = false)\n |    |-- element: integer (containsNull = false)\n\n&gt;&gt;&gt; res_df.show(truncate=False)\n+---+---------------+\n|id |a              |\n+---+---------------+\n|1  |[1, 2, 3, 4, 5]|\n+---+---------------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/flatten_all_arrays.py</code> <pre><code>def flatten_all_arrays(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Flatten all columns of type `ARRAY&lt;ARRAY&lt;T&gt;&gt;` inside the given DataFrame into `ARRAY&lt;&lt;T&gt;&gt;&gt;`.\n    This transformation works recursively on every nesting level.\n\n    !!! info\n        This method is compatible with any schema. It recursively applies on structs, arrays and maps\n        and accepts field names containing dots (`.`), exclamation marks (`!`) or percentage (`%`).\n\n    Args:\n        df: A Spark DataFrame\n\n    Returns:\n        A new DataFrame in which all arrays of array have been flattened\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(ARRAY(ARRAY(1, 2), ARRAY(3)), ARRAY(ARRAY(4), ARRAY(5))) as a')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- a: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: array (containsNull = false)\n         |    |    |    |-- element: integer (containsNull = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+---------------------------+\n        |id |a                          |\n        +---+---------------------------+\n        |1  |[[[1, 2], [3]], [[4], [5]]]|\n        +---+---------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res_df = flatten_all_arrays(df)\n        &gt;&gt;&gt; res_df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- a: array (nullable = false)\n         |    |-- element: integer (containsNull = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res_df.show(truncate=False)\n        +---+---------------+\n        |id |a              |\n        +---+---------------+\n        |1  |[1, 2, 3, 4, 5]|\n        +---+---------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n\n    def flatten_array(col: Column, data_type: DataType) -&gt; Optional[Column]:\n        if isinstance(data_type, ArrayType) and isinstance(data_type.elementType, ArrayType):\n            return f.flatten(col)\n        else:\n            return None\n\n    return transform_all_fields(df, flatten_array)\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.harmonize_dataframes.harmonize_dataframes","title":"<code>harmonize_dataframes(left_df: DataFrame, right_df: DataFrame, common_columns: Optional[Dict[str, Optional[str]]] = None, keep_missing_columns: bool = False) -&gt; Tuple[DataFrame, DataFrame]</code>","text":"<p>Given two DataFrames, returns two new corresponding DataFrames with the same schemas by applying the following changes:</p> <ul> <li>Only common columns are kept</li> <li>Columns of type MAP are cast into ARRAY&gt; <li>Columns are re-ordered to have the same ordering in both DataFrames</li> <li>When matching columns have different types, their type is widened to their most narrow common type. This transformation is applied recursively on nested columns, including those inside repeated records (a.k.a. ARRAY&lt;STRUCT&lt;&gt;&gt;).</li> <p>Parameters:</p> Name Type Description Default <code>left_df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>right_df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>common_columns</code> <code>Optional[Dict[str, Optional[str]]]</code> <p>A dict of (column name, type). Column names must appear in both DataFrames, and each column will be cast into the corresponding type.</p> <code>None</code> <code>keep_missing_columns</code> <code>bool</code> <p>If set to true, the root columns of each DataFrames that do not exist in the other one are kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Two new Spark DataFrames with the same schema</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df1 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3 as b, 4 as c)) as s2) as s1')\n&gt;&gt;&gt; df2 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3.0 as b, \"4\" as c, 5 as d)) as s2) as s1')\n&gt;&gt;&gt; df1.union(df2).show(truncate=False)\nTraceback (most recent call last):\n    ...\nAnalysisException: ... UNION can only be performed on tables with compatible column types.\n&gt;&gt;&gt; df1, df2 = harmonize_dataframes(df1, df2)\n&gt;&gt;&gt; df1.union(df2).show()\n+---+---------------+\n| id|             s1|\n+---+---------------+\n|  1|{2, [{3.0, 4}]}|\n|  1|{2, [{3.0, 4}]}|\n+---+---------------+\n\n&gt;&gt;&gt; df1, df2 = harmonize_dataframes(df1, df2, common_columns={\"id\": None, \"s1.s2!.b\": \"int\"})\n&gt;&gt;&gt; df1.union(df2).show()\n+---+-------+\n| id|     s1|\n+---+-------+\n|  1|{[{3}]}|\n|  1|{[{3}]}|\n+---+-------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/harmonize_dataframes.py</code> <pre><code>def harmonize_dataframes(\n    left_df: DataFrame,\n    right_df: DataFrame,\n    common_columns: Optional[Dict[str, Optional[str]]] = None,\n    keep_missing_columns: bool = False,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"Given two DataFrames, returns two new corresponding DataFrames with the same schemas by applying the following\n    changes:\n\n    - Only common columns are kept\n    - Columns of type MAP&lt;key, value&gt; are cast into ARRAY&lt;STRUCT&lt;key, value&gt;&gt;\n    - Columns are re-ordered to have the same ordering in both DataFrames\n    - When matching columns have different types, their type is widened to their most narrow common type.\n    This transformation is applied recursively on nested columns, including those inside\n    repeated records (a.k.a. ARRAY&lt;STRUCT&lt;&gt;&gt;).\n\n    Args:\n        left_df: A Spark DataFrame\n        right_df: A Spark DataFrame\n        common_columns: A dict of (column name, type).\n            Column names must appear in both DataFrames, and each column will be cast into the corresponding type.\n        keep_missing_columns: If set to true, the root columns of each DataFrames that do not exist in the other\n            one are kept.\n\n    Returns:\n        Two new Spark DataFrames with the same schema\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df1 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3 as b, 4 as c)) as s2) as s1')\n        &gt;&gt;&gt; df2 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3.0 as b, \"4\" as c, 5 as d)) as s2) as s1')\n        &gt;&gt;&gt; df1.union(df2).show(truncate=False) # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n            ...\n        AnalysisException: ... UNION can only be performed on tables with compatible column types.\n        &gt;&gt;&gt; df1, df2 = harmonize_dataframes(df1, df2)\n        &gt;&gt;&gt; df1.union(df2).show()\n        +---+---------------+\n        | id|             s1|\n        +---+---------------+\n        |  1|{2, [{3.0, 4}]}|\n        |  1|{2, [{3.0, 4}]}|\n        +---+---------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df1, df2 = harmonize_dataframes(df1, df2, common_columns={\"id\": None, \"s1.s2!.b\": \"int\"})\n        &gt;&gt;&gt; df1.union(df2).show()\n        +---+-------+\n        | id|     s1|\n        +---+-------+\n        |  1|{[{3}]}|\n        |  1|{[{3}]}|\n        +---+-------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    left_schema_flat = flatten_schema(left_df.schema, explode=True)\n    right_schema_flat = flatten_schema(right_df.schema, explode=True)\n    if common_columns is None:\n        common_columns = get_common_columns(left_schema_flat, right_schema_flat)\n\n    left_only_columns = {}\n    right_only_columns = {}\n    if keep_missing_columns:\n        left_cols = [field.name for field in left_schema_flat.fields]\n        right_cols = [field.name for field in right_schema_flat.fields]\n        left_cols_set = set(left_cols)\n        right_cols_set = set(right_cols)\n        left_only_columns = {col: None for col in left_cols if col not in right_cols_set}\n        right_only_columns = {col: None for col in right_cols if col not in left_cols_set}\n\n    def build_col(col_name: str, col_type: Optional[str]) -&gt; PrintableFunction:\n        parent_structs = _deepest_granularity(col_name)\n        if col_type is not None:\n            tpe = col_type\n            f1 = PrintableFunction(lambda s: s.cast(tpe), lambda s: f\"{s}.cast({tpe})\")\n        else:\n            f1 = higher_order.identity\n        f2 = higher_order.recursive_struct_get(parent_structs)\n        return fp.compose(f1, f2)\n\n    left_columns = {**common_columns, **left_only_columns}\n    right_columns = {**common_columns, **right_only_columns}\n    left_columns_dict = {col_name: build_col(col_name, col_type) for (col_name, col_type) in left_columns.items()}\n    right_columns_dict = {col_name: build_col(col_name, col_type) for (col_name, col_type) in right_columns.items()}\n    left_tree = _build_nested_struct_tree(left_columns_dict)\n    right_tree = _build_nested_struct_tree(right_columns_dict)\n    left_root_transformation = _build_transformation_from_tree(left_tree)\n    right_root_transformation = _build_transformation_from_tree(right_tree)\n    return (\n        left_df.select(*left_root_transformation([left_df])),\n        right_df.select(*right_root_transformation([right_df])),\n    )\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.parse_json_columns.parse_json_columns","title":"<code>parse_json_columns(df: DataFrame, columns: Union[str, List[str], Dict[str, str]]) -&gt; DataFrame</code>","text":"<p>Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information.</p> <p>This method is similar to Spark's <code>from_json</code> function, with one main difference: <code>from_json</code> requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column.</p> <p>By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names.</p> <p>Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism.</p> <p>WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \"<code>a.b.c</code>\") (See Example 2).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>columns</code> <code>Union[str, List[str], Dict[str, str]]</code> <p>A column name, list of column names, or dict(column_name, parsed_column_name)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame</p> <p>Example 1</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([\n...         (1, '[{\"a\": 1}, {\"a\": 2}]'),\n...         (1, '[{\"a\": 2}, {\"a\": 4}]'),\n...         (2, None)\n...     ], \"id INT, json1 STRING\"\n... )\n&gt;&gt;&gt; df.show()\n+---+--------------------+\n| id|               json1|\n+---+--------------------+\n|  1|[{\"a\": 1}, {\"a\": 2}]|\n|  1|[{\"a\": 2}, {\"a\": 4}]|\n|  2|                NULL|\n+---+--------------------+\n\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- json1: string (nullable = true)\n\n&gt;&gt;&gt; parse_json_columns(df, 'json1').printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- json1: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: long (nullable = true)\n</code></pre> <p>Example 2 : different output column name</p> <pre><code>&gt;&gt;&gt; parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- json1: string (nullable = true)\n |-- parsed_json1: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: long (nullable = true)\n</code></pre> <p>Example 3 : json inside a struct</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([\n...         (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}),\n...         (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}),\n...         (2, None)\n...     ], \"id INT, struct STRUCT&lt;json1: STRING&gt;\"\n... )\n&gt;&gt;&gt; df.show(10, False)\n+---+----------------------+\n|id |struct                |\n+---+----------------------+\n|1  |{[{\"a\": 1}, {\"a\": 2}]}|\n|1  |{[{\"a\": 2}, {\"a\": 4}]}|\n|2  |NULL                  |\n+---+----------------------+\n\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- struct: struct (nullable = true)\n |    |-- json1: string (nullable = true)\n\n&gt;&gt;&gt; res = parse_json_columns(df, 'struct.json1')\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- struct: struct (nullable = true)\n |    |-- json1: string (nullable = true)\n |-- struct.json1: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: long (nullable = true)\n\n&gt;&gt;&gt; res.show(10, False)\n+---+----------------------+------------+\n|id |struct                |struct.json1|\n+---+----------------------+------------+\n|1  |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}]  |\n|1  |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}]  |\n|2  |NULL                  |NULL        |\n+---+----------------------+------------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/parse_json_columns.py</code> <pre><code>def parse_json_columns(df: DataFrame, columns: Union[str, List[str], Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Transform the specified columns containing json strings in the given DataFrame into structs containing\n    the equivalent parsed information.\n\n    This method is similar to Spark's `from_json` function, with one main difference: `from_json` requires the user\n    to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically\n    the json schema of each column.\n\n    By default, the output columns will have the same name as the input columns, but if you want to keep the input\n    columns you can pass a dict(input_col_name, output_col_name) to specify different output column names.\n\n    Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful\n    for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema\n    of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism.\n\n    WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"),\n    instead of replacing that column it will create a new column outside the struct (e.g. \"`a.b.c`\") (See Example 2).\n\n    Args:\n        df: A Spark DataFrame\n        columns: A column name, list of column names, or dict(column_name, parsed_column_name)\n\n    Returns:\n        A new DataFrame\n\n    Examples: Example 1\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...         (1, '[{\"a\": 1}, {\"a\": 2}]'),\n        ...         (1, '[{\"a\": 2}, {\"a\": 4}]'),\n        ...         (2, None)\n        ...     ], \"id INT, json1 STRING\"\n        ... )\n        &gt;&gt;&gt; df.show()\n        +---+--------------------+\n        | id|               json1|\n        +---+--------------------+\n        |  1|[{\"a\": 1}, {\"a\": 2}]|\n        |  1|[{\"a\": 2}, {\"a\": 4}]|\n        |  2|                NULL|\n        +---+--------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- json1: string (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; parse_json_columns(df, 'json1').printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- json1: array (nullable = true)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- a: long (nullable = true)\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2 : different output column name\n        &gt;&gt;&gt; parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- json1: string (nullable = true)\n         |-- parsed_json1: array (nullable = true)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- a: long (nullable = true)\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 3 : json inside a struct\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...         (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}),\n        ...         (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}),\n        ...         (2, None)\n        ...     ], \"id INT, struct STRUCT&lt;json1: STRING&gt;\"\n        ... )\n        &gt;&gt;&gt; df.show(10, False)\n        +---+----------------------+\n        |id |struct                |\n        +---+----------------------+\n        |1  |{[{\"a\": 1}, {\"a\": 2}]}|\n        |1  |{[{\"a\": 2}, {\"a\": 4}]}|\n        |2  |NULL                  |\n        +---+----------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- struct: struct (nullable = true)\n         |    |-- json1: string (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res = parse_json_columns(df, 'struct.json1')\n        &gt;&gt;&gt; res.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- struct: struct (nullable = true)\n         |    |-- json1: string (nullable = true)\n         |-- struct.json1: array (nullable = true)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- a: long (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res.show(10, False)\n        +---+----------------------+------------+\n        |id |struct                |struct.json1|\n        +---+----------------------+------------+\n        |1  |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}]  |\n        |1  |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}]  |\n        |2  |NULL                  |NULL        |\n        +---+----------------------+------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    if isinstance(columns, str):\n        columns = [columns]\n    if isinstance(columns, list):\n        columns = {col: col for col in columns}\n\n    wrapped_df = __wrap_json_columns(df, columns)\n    schema_per_col = __infer_schema_per_column(wrapped_df, list(columns.values()))\n    res = __parse_json_columns(wrapped_df, schema_per_col)\n    return res\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.sort_all_arrays.sort_all_arrays","title":"<code>sort_all_arrays(df: DataFrame) -&gt; DataFrame</code>","text":"<p>Given a DataFrame, sort all fields of type <code>ARRAY</code> in a canonical order, making them comparable. This also applies to nested fields, even those inside other arrays.</p> <p>Limitation</p> <ul> <li>Arrays containing sub-fields of type Map cannot be sorted, as the Map type is not comparable.</li> <li>A possible workaround is to first use the transformation <code>spark_frame.transformations.convert_all_maps_to_arrays</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame where all arrays have been sorted.</p> <p>Example 1: with a simple `ARRAY&lt;INT&gt;`</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(3, 2, 1) as a')\n&gt;&gt;&gt; df.show()\n+---+---------+\n| id|        a|\n+---+---------+\n|  1|[3, 2, 1]|\n+---+---------+\n\n&gt;&gt;&gt; sort_all_arrays(df).show()\n+---+---------+\n| id|        a|\n+---+---------+\n|  1|[1, 2, 3]|\n+---+---------+\n</code></pre> <p>Example 2: with an `ARRAY&lt;STRUCT&lt;...&gt;&gt;`</p> <pre><code>&gt;&gt;&gt; df = spark.sql('SELECT ARRAY(STRUCT(2 as a, 1 as b), STRUCT(1 as a, 2 as b), STRUCT(1 as a, 1 as b)) as s')\n&gt;&gt;&gt; df.show(truncate=False)\n+------------------------+\n|s                       |\n+------------------------+\n|[{2, 1}, {1, 2}, {1, 1}]|\n+------------------------+\n\n&gt;&gt;&gt; df.transform(sort_all_arrays).show(truncate=False)\n+------------------------+\n|s                       |\n+------------------------+\n|[{1, 1}, {1, 2}, {2, 1}]|\n+------------------------+\n</code></pre> <p>Example 3: with an `ARRAY&lt;STRUCT&lt;STRUCT&lt;...&gt;&gt;&gt;`</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''SELECT ARRAY(\n...         STRUCT(STRUCT(2 as a, 2 as b) as s),\n...         STRUCT(STRUCT(1 as a, 2 as b) as s)\n...     ) as l1\n... ''')\n&gt;&gt;&gt; df.show(truncate=False)\n+--------------------+\n|l1                  |\n+--------------------+\n|[{{2, 2}}, {{1, 2}}]|\n+--------------------+\n\n&gt;&gt;&gt; df.transform(sort_all_arrays).show(truncate=False)\n+--------------------+\n|l1                  |\n+--------------------+\n|[{{1, 2}}, {{2, 2}}]|\n+--------------------+\n</code></pre> <p>Example 4: with an `ARRAY&lt;ARRAY&lt;ARRAY&lt;INT&gt;&gt;&gt;`</p> <p>As this example shows, the innermost arrays are sorted before the outermost arrays.</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''SELECT ARRAY(\n...         ARRAY(ARRAY(4, 1), ARRAY(3, 2)),\n...         ARRAY(ARRAY(2, 2), ARRAY(2, 1))\n...     ) as l1\n... ''')\n&gt;&gt;&gt; df.show(truncate=False)\n+------------------------------------+\n|l1                                  |\n+------------------------------------+\n|[[[4, 1], [3, 2]], [[2, 2], [2, 1]]]|\n+------------------------------------+\n\n&gt;&gt;&gt; df.transform(sort_all_arrays).show(truncate=False)\n+------------------------------------+\n|l1                                  |\n+------------------------------------+\n|[[[1, 2], [2, 2]], [[1, 4], [2, 3]]]|\n+------------------------------------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/sort_all_arrays.py</code> <pre><code>def sort_all_arrays(df: DataFrame) -&gt; DataFrame:\n    \"\"\"Given a DataFrame, sort all fields of type `ARRAY` in a canonical order, making them comparable.\n    This also applies to nested fields, even those inside other arrays.\n\n    !!! warning \"Limitation\"\n        - Arrays containing sub-fields of type Map cannot be sorted, as the Map type is not comparable.\n        - A possible workaround is to first use the transformation\n        [`spark_frame.transformations.convert_all_maps_to_arrays`]\n        [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays]\n\n    Args:\n        df: A Spark DataFrame\n\n    Returns:\n        A new DataFrame where all arrays have been sorted.\n\n    Examples: Example 1: with a simple `ARRAY&lt;INT&gt;`\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(3, 2, 1) as a')\n        &gt;&gt;&gt; df.show()\n        +---+---------+\n        | id|        a|\n        +---+---------+\n        |  1|[3, 2, 1]|\n        +---+---------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; sort_all_arrays(df).show()\n        +---+---------+\n        | id|        a|\n        +---+---------+\n        |  1|[1, 2, 3]|\n        +---+---------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2: with an `ARRAY&lt;STRUCT&lt;...&gt;&gt;`\n        &gt;&gt;&gt; df = spark.sql('SELECT ARRAY(STRUCT(2 as a, 1 as b), STRUCT(1 as a, 2 as b), STRUCT(1 as a, 1 as b)) as s')\n        &gt;&gt;&gt; df.show(truncate=False)\n        +------------------------+\n        |s                       |\n        +------------------------+\n        |[{2, 1}, {1, 2}, {1, 1}]|\n        +------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.transform(sort_all_arrays).show(truncate=False)\n        +------------------------+\n        |s                       |\n        +------------------------+\n        |[{1, 1}, {1, 2}, {2, 1}]|\n        +------------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 3: with an `ARRAY&lt;STRUCT&lt;STRUCT&lt;...&gt;&gt;&gt;`\n        &gt;&gt;&gt; df = spark.sql('''SELECT ARRAY(\n        ...         STRUCT(STRUCT(2 as a, 2 as b) as s),\n        ...         STRUCT(STRUCT(1 as a, 2 as b) as s)\n        ...     ) as l1\n        ... ''')\n        &gt;&gt;&gt; df.show(truncate=False)\n        +--------------------+\n        |l1                  |\n        +--------------------+\n        |[{{2, 2}}, {{1, 2}}]|\n        +--------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.transform(sort_all_arrays).show(truncate=False)\n        +--------------------+\n        |l1                  |\n        +--------------------+\n        |[{{1, 2}}, {{2, 2}}]|\n        +--------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 4: with an `ARRAY&lt;ARRAY&lt;ARRAY&lt;INT&gt;&gt;&gt;`\n        As this example shows, the innermost arrays are sorted before the outermost arrays.\n        &gt;&gt;&gt; df = spark.sql('''SELECT ARRAY(\n        ...         ARRAY(ARRAY(4, 1), ARRAY(3, 2)),\n        ...         ARRAY(ARRAY(2, 2), ARRAY(2, 1))\n        ...     ) as l1\n        ... ''')\n        &gt;&gt;&gt; df.show(truncate=False)\n        +------------------------------------+\n        |l1                                  |\n        +------------------------------------+\n        |[[[4, 1], [3, 2]], [[2, 2], [2, 1]]]|\n        +------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.transform(sort_all_arrays).show(truncate=False)\n        +------------------------------------+\n        |l1                                  |\n        +------------------------------------+\n        |[[[1, 2], [2, 2]], [[1, 4], [2, 3]]]|\n        +------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n\n    def sort_array(col: Column, data_type: DataType) -&gt; Optional[Column]:\n        if isinstance(data_type, ArrayType):\n            return f.sort_array(col)\n        else:\n            return None\n\n    return transform_all_fields(df, sort_array)\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names","title":"<code>transform_all_field_names(df: DataFrame, transformation: Callable[[str], str]) -&gt; DataFrame</code>","text":"<p>Apply a transformation to all nested field names of a DataFrame.</p> <p>Info</p> <p>This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>transformation</code> <code>Callable[[str], str]</code> <p>Transformation to apply to all field names in the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame</p> <p>Example 1: with a nested schema structure</p> <p>In this example we cast all the field names of the schema to uppercase:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     \"John\" as name,\n...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n...     ARRAY(\n...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n...     ) as s5\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- name: string (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s2!!: integer (nullable = false)\n |-- s3!!.a: integer (nullable = false)\n |-- s4!.a!: integer (nullable = false)\n |-- s5!.a!.b.c: integer (nullable = false)\n</code></pre> <pre><code>&gt;&gt;&gt; new_df = df.transform(transform_all_field_names, lambda s: s.upper())\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- NAME: string (nullable = false)\n |-- S1!.A: integer (nullable = false)\n |-- S2!!: integer (nullable = false)\n |-- S3!!.A: integer (nullable = false)\n |-- S4!.A!: integer (nullable = false)\n |-- S5!.A!.B.C: integer (nullable = false)\n</code></pre> <p>Example 2: sanitizing field names</p> <p>In this example we replace all dots and exclamation marks in field names with underscores. This is useful to make a DataFrame compatible with the spark_frame.nested module.</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''SELECT\n...     ARRAY(STRUCT(\n...         ARRAY(STRUCT(\n...             STRUCT(1 as `d.d!d`) as `c.c!c`\n...         )) as `b.b!b`\n...    )) as `a.a!a`\n... ''')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- a.a!a: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- b.b!b: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- c.c!c: struct (nullable = false)\n |    |    |    |    |    |-- d.d!d: integer (nullable = false)\n\n&gt;&gt;&gt; new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\"))\n&gt;&gt;&gt; new_df.printSchema()\nroot\n |-- a_a_a: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- b_b_b: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- c_c_c: struct (nullable = false)\n |    |    |    |    |    |-- d_d_d: integer (nullable = false)\n</code></pre> <p>This also works on fields of type <code>MAP&lt;K,V&gt;</code>.</p> <pre><code>&gt;&gt;&gt; df = spark.sql('SELECT MAP(STRUCT(1 as `a.a!a`), STRUCT(2 as `b.b!b`)) as `m.m!m`')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- m.m!m: map (nullable = false)\n |    |-- key: struct\n |    |    |-- a.a!a: integer (nullable = false)\n |    |-- value: struct (valueContainsNull = false)\n |    |    |-- b.b!b: integer (nullable = false)\n\n&gt;&gt;&gt; new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\"))\n&gt;&gt;&gt; new_df.printSchema()\nroot\n |-- m_m_m: map (nullable = false)\n |    |-- key: struct\n |    |    |-- a_a_a: integer (nullable = false)\n |    |-- value: struct (valueContainsNull = false)\n |    |    |-- b_b_b: integer (nullable = false)\n</code></pre> Source code in <code>spark_frame/transformations_impl/transform_all_field_names.py</code> <pre><code>def transform_all_field_names(df: DataFrame, transformation: Callable[[str], str]) -&gt; DataFrame:\n    \"\"\"Apply a transformation to all nested field names of a DataFrame.\n\n    !!! info\n        This method is compatible with any schema. It recursively applies on structs, arrays and maps\n        and is compatible with field names containing special characters.\n\n    Args:\n        df: A Spark DataFrame\n        transformation: Transformation to apply to all field names in the DataFrame.\n\n    Returns:\n        A new DataFrame\n\n    Examples: Example 1: with a nested schema structure\n        In this example we cast all the field names of the schema to uppercase:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     \"John\" as name,\n        ...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n        ...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n        ...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n        ...     ARRAY(\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n        ...     ) as s5\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- name: string (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s2!!: integer (nullable = false)\n         |-- s3!!.a: integer (nullable = false)\n         |-- s4!.a!: integer (nullable = false)\n         |-- s5!.a!.b.c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; new_df = df.transform(transform_all_field_names, lambda s: s.upper())\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- NAME: string (nullable = false)\n         |-- S1!.A: integer (nullable = false)\n         |-- S2!!: integer (nullable = false)\n         |-- S3!!.A: integer (nullable = false)\n         |-- S4!.A!: integer (nullable = false)\n         |-- S5!.A!.B.C: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2: sanitizing field names\n        In this example we replace all dots and exclamation marks in field names with underscores.\n        This is useful to make a DataFrame compatible with the [spark_frame.nested](/spark-frame/reference/nested)\n        module.\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     ARRAY(STRUCT(\n        ...         ARRAY(STRUCT(\n        ...             STRUCT(1 as `d.d!d`) as `c.c!c`\n        ...         )) as `b.b!b`\n        ...    )) as `a.a!a`\n        ... ''')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- a.a!a: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- b.b!b: array (nullable = false)\n         |    |    |    |-- element: struct (containsNull = false)\n         |    |    |    |    |-- c.c!c: struct (nullable = false)\n         |    |    |    |    |    |-- d.d!d: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\"))\n        &gt;&gt;&gt; new_df.printSchema()\n        root\n         |-- a_a_a: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- b_b_b: array (nullable = false)\n         |    |    |    |-- element: struct (containsNull = false)\n         |    |    |    |    |-- c_c_c: struct (nullable = false)\n         |    |    |    |    |    |-- d_d_d: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n\n        This also works on fields of type `MAP&lt;K,V&gt;`.\n        &gt;&gt;&gt; df = spark.sql('SELECT MAP(STRUCT(1 as `a.a!a`), STRUCT(2 as `b.b!b`)) as `m.m!m`')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- m.m!m: map (nullable = false)\n         |    |-- key: struct\n         |    |    |-- a.a!a: integer (nullable = false)\n         |    |-- value: struct (valueContainsNull = false)\n         |    |    |-- b.b!b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\"))\n        &gt;&gt;&gt; new_df.printSchema()\n        root\n         |-- m_m_m: map (nullable = false)\n         |    |-- key: struct\n         |    |    |-- a_a_a: integer (nullable = false)\n         |    |-- value: struct (valueContainsNull = false)\n         |    |    |-- b_b_b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    root_transformation = build_transformation_from_schema(df.schema, name_transformation=transformation)\n    return df.select(*root_transformation(df))\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.transform_all_fields.transform_all_fields","title":"<code>transform_all_fields(df: DataFrame, transformation: Callable[[Column, DataType], Optional[Column]]) -&gt; DataFrame</code>","text":"<p>Apply a transformation to all nested fields of a DataFrame.</p> <p>Info</p> <p>This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>transformation</code> <code>Callable[[Column, DataType], Optional[Column]]</code> <p>Transformation to apply to all fields of the DataFrame. The transformation must take as input a Column expression and the DataType of the corresponding expression.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     \"John\" as name,\n...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n...     ARRAY(\n...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n...     ) as s5\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- name: string (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s2!!: integer (nullable = false)\n |-- s3!!.a: integer (nullable = false)\n |-- s4!.a!: integer (nullable = false)\n |-- s5!.a!.b.c: integer (nullable = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+----+----------+----------------+--------------+--------------------+------------------------------------+\n|name|s1        |s2              |s3            |s4                  |s5                                  |\n+----+----------+----------------+--------------+--------------------+------------------------------------+\n|John|[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]|\n+----+----------+----------------+--------------+--------------------+------------------------------------+\n\n&gt;&gt;&gt; from pyspark.sql.types import IntegerType\n&gt;&gt;&gt; def cast_int_as_double(col: Column, data_type: DataType):\n...     if isinstance(data_type, IntegerType):\n...         return col.cast(\"DOUBLE\")\n&gt;&gt;&gt; new_df = df.transform(transform_all_fields, cast_int_as_double)\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- name: string (nullable = false)\n |-- s1!.a: double (nullable = false)\n |-- s2!!: double (nullable = false)\n |-- s3!!.a: double (nullable = false)\n |-- s4!.a!: double (nullable = false)\n |-- s5!.a!.b.c: double (nullable = false)\n\n&gt;&gt;&gt; new_df.show(truncate=False)\n+----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+\n|name|s1            |s2                      |s3                |s4                          |s5                                          |\n+----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+\n|John|[{1.0}, {2.0}]|[[1.0, 2.0], [3.0, 4.0]]|[[{1.0}], [{2.0}]]|[{[1.0, 2.0]}, {[3.0, 4.0]}]|[{[{{1.0}}, {{2.0}}]}, {[{{3.0}}, {{4.0}}]}]|\n+----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/transform_all_fields.py</code> <pre><code>def transform_all_fields(\n    df: DataFrame,\n    transformation: Callable[[Column, DataType], Optional[Column]],\n) -&gt; DataFrame:\n    \"\"\"Apply a transformation to all nested fields of a DataFrame.\n\n    !!! info\n        This method is compatible with any schema. It recursively applies on structs, arrays and maps\n        and is compatible with field names containing special characters.\n\n    Args:\n        df: A Spark DataFrame\n        transformation: Transformation to apply to all fields of the DataFrame. The transformation must take as input\n            a Column expression and the DataType of the corresponding expression.\n\n    Returns:\n        A new DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     \"John\" as name,\n        ...     ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2,\n        ...     ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3,\n        ...     ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4,\n        ...     ARRAY(\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a),\n        ...         STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a)\n        ...     ) as s5\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- name: string (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s2!!: integer (nullable = false)\n         |-- s3!!.a: integer (nullable = false)\n         |-- s4!.a!: integer (nullable = false)\n         |-- s5!.a!.b.c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +----+----------+----------------+--------------+--------------------+------------------------------------+\n        |name|s1        |s2              |s3            |s4                  |s5                                  |\n        +----+----------+----------------+--------------+--------------------+------------------------------------+\n        |John|[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]|\n        +----+----------+----------------+--------------+--------------------+------------------------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; def cast_int_as_double(col: Column, data_type: DataType):\n        ...     if isinstance(data_type, IntegerType):\n        ...         return col.cast(\"DOUBLE\")\n        &gt;&gt;&gt; new_df = df.transform(transform_all_fields, cast_int_as_double)\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- name: string (nullable = false)\n         |-- s1!.a: double (nullable = false)\n         |-- s2!!: double (nullable = false)\n         |-- s3!!.a: double (nullable = false)\n         |-- s4!.a!: double (nullable = false)\n         |-- s5!.a!.b.c: double (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+\n        |name|s1            |s2                      |s3                |s4                          |s5                                          |\n        +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+\n        |John|[{1.0}, {2.0}]|[[1.0, 2.0], [3.0, 4.0]]|[[{1.0}], [{2.0}]]|[{[1.0, 2.0]}, {[3.0, 4.0]}]|[{[{{1.0}}, {{2.0}}]}, {[{{3.0}}, {{4.0}}]}]|\n        +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"  # noqa: E501\n    root_transformation = build_transformation_from_schema(\n        df.schema,\n        column_transformation=transformation,\n    )\n    return df.select(*root_transformation(df))\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.unflatten.unflatten","title":"<code>unflatten(df: DataFrame, separator: str = '.') -&gt; DataFrame</code>","text":"<p>Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>separator</code> <code>str</code> <p>A string used to separate the structs names from their elements.        It might be useful to change the separator when some DataFrame's column names already contain dots</p> <code>'.'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A flattened DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\")\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.a: integer (nullable = true)\n |-- s.b.c: integer (nullable = true)\n |-- s.b.d: integer (nullable = true)\n\n&gt;&gt;&gt; unflatten(df).printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s: struct (nullable = true)\n |    |-- a: integer (nullable = true)\n |    |-- b: struct (nullable = true)\n |    |    |-- c: integer (nullable = true)\n |    |    |-- d: integer (nullable = true)\n\n&gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\")\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1?a.a1: integer (nullable = true)\n |-- s.s1?b.b1: integer (nullable = true)\n\n&gt;&gt;&gt; unflatten(df, \"?\").printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1: struct (nullable = true)\n |    |-- a.a1: integer (nullable = true)\n |    |-- b.b1: integer (nullable = true)\n</code></pre> Source code in <code>spark_frame/transformations_impl/unflatten.py</code> <pre><code>def unflatten(df: DataFrame, separator: str = \".\") -&gt; DataFrame:\n    \"\"\"Reverse of the flatten operation\n    Nested fields names will be separated from each other using the specified separator\n\n    Args:\n        df: A Spark DataFrame\n        separator: A string used to separate the structs names from their elements.\n                   It might be useful to change the separator when some DataFrame's column names already contain dots\n\n    Returns:\n        A flattened DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\")\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.a: integer (nullable = true)\n         |-- s.b.c: integer (nullable = true)\n         |-- s.b.d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; unflatten(df).printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s: struct (nullable = true)\n         |    |-- a: integer (nullable = true)\n         |    |-- b: struct (nullable = true)\n         |    |    |-- c: integer (nullable = true)\n         |    |    |-- d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\")\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1?a.a1: integer (nullable = true)\n         |-- s.s1?b.b1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; unflatten(df, \"?\").printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1: struct (nullable = true)\n         |    |-- a.a1: integer (nullable = true)\n         |    |-- b.b1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    # The idea is to recursively write a \"SELECT struct(a, struct(s.b.c, s.b.d)) as s\" for each nested column.\n    # There is a little twist as we don't want to rebuild the struct if all its fields are NULL, so we add a CASE WHEN\n\n    def has_structs(df: DataFrame) -&gt; bool:\n        struct_fields = [field for field in df.schema if is_struct(field)]\n        return len(struct_fields) &gt; 0\n\n    if has_structs(df):\n        df = flatten(df)\n\n    tree = _build_nested_struct_tree(df.columns, separator)\n    cols = _build_struct_from_tree(tree, separator)\n    return df.select(cols)\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.union_dataframes.union_dataframes","title":"<code>union_dataframes(*dfs: DataFrame) -&gt; DataFrame</code>","text":"<p>Returns the union between multiple DataFrames</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>DataFrame</code> <p>One or more Spark DataFrames</p> <code>()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame containing the union of all input DataFrames</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df1 = spark.sql('SELECT 1 as a')\n&gt;&gt;&gt; df2 = spark.sql('SELECT 2 as a')\n&gt;&gt;&gt; df3 = spark.sql('SELECT 3 as a')\n&gt;&gt;&gt; union_dataframes(df1, df2, df3).show()\n+---+\n|  a|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n\n&gt;&gt;&gt; df1.transform(union_dataframes, df2, df3).show()\n+---+\n|  a|\n+---+\n|  1|\n|  2|\n|  3|\n+---+\n</code></pre> Source code in <code>spark_frame/transformations_impl/union_dataframes.py</code> <pre><code>def union_dataframes(*dfs: DataFrame) -&gt; DataFrame:\n    \"\"\"Returns the union between multiple DataFrames\n\n    Args:\n        dfs: One or more Spark DataFrames\n\n    Returns:\n        A new DataFrame containing the union of all input DataFrames\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df1 = spark.sql('SELECT 1 as a')\n        &gt;&gt;&gt; df2 = spark.sql('SELECT 2 as a')\n        &gt;&gt;&gt; df3 = spark.sql('SELECT 3 as a')\n        &gt;&gt;&gt; union_dataframes(df1, df2, df3).show()\n        +---+\n        |  a|\n        +---+\n        |  1|\n        |  2|\n        |  3|\n        +---+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df1.transform(union_dataframes, df2, df3).show()\n        +---+\n        |  a|\n        +---+\n        |  1|\n        |  2|\n        |  3|\n        +---+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    assert_true(len(dfs) &gt; 0, ValueError(\"Input list is empty\"))\n    res = dfs[0]\n    for df in dfs[1:]:\n        res = res.union(df)\n    return res\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.unpivot.unpivot","title":"<code>unpivot(df: DataFrame, pivot_columns: List[str], key_alias: str = 'key', value_alias: str = 'value') -&gt; DataFrame</code>","text":"<p>Unpivot the given DataFrame along the specified pivot columns. All columns that are not pivot columns should have the same type.</p> <p>This is the inverse transformation of the pyspark.sql.GroupedData.pivot operation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame</p> required <code>pivot_columns</code> <code>List[str]</code> <p>The list of columns names on which to perform the pivot</p> required <code>key_alias</code> <code>str</code> <p>Alias given to the 'key' column</p> <code>'key'</code> <code>value_alias</code> <code>str</code> <p>Alias given to the 'value' column</p> <code>'value'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>An unpivotted DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([\n...    (2018, \"Orange\",  None, 4000, None),\n...    (2018, \"Beans\",   None, 1500, 2000),\n...    (2018, \"Banana\",  2000,  400, None),\n...    (2018, \"Carrots\", 2000, 1200, None),\n...    (2019, \"Orange\",  5000, None, 5000),\n...    (2019, \"Beans\",   None, 1500, 2000),\n...    (2019, \"Banana\",  None, 1400,  400),\n...    (2019, \"Carrots\", None,  200, None),\n...  ], \"year INT, product STRING, Canada INT, China INT, Mexico INT\"\n... )\n&gt;&gt;&gt; df.show()\n+----+-------+------+-----+------+\n|year|product|Canada|China|Mexico|\n+----+-------+------+-----+------+\n|2018| Orange|  NULL| 4000|  NULL|\n|2018|  Beans|  NULL| 1500|  2000|\n|2018| Banana|  2000|  400|  NULL|\n|2018|Carrots|  2000| 1200|  NULL|\n|2019| Orange|  5000| NULL|  5000|\n|2019|  Beans|  NULL| 1500|  2000|\n|2019| Banana|  NULL| 1400|   400|\n|2019|Carrots|  NULL|  200|  NULL|\n+----+-------+------+-----+------+\n\n&gt;&gt;&gt; unpivot(df, ['year', 'product'], key_alias='country', value_alias='total').show(100)\n+----+-------+-------+-----+\n|year|product|country|total|\n+----+-------+-------+-----+\n|2018| Orange| Canada| NULL|\n|2018| Orange|  China| 4000|\n|2018| Orange| Mexico| NULL|\n|2018|  Beans| Canada| NULL|\n|2018|  Beans|  China| 1500|\n|2018|  Beans| Mexico| 2000|\n|2018| Banana| Canada| 2000|\n|2018| Banana|  China|  400|\n|2018| Banana| Mexico| NULL|\n|2018|Carrots| Canada| 2000|\n|2018|Carrots|  China| 1200|\n|2018|Carrots| Mexico| NULL|\n|2019| Orange| Canada| 5000|\n|2019| Orange|  China| NULL|\n|2019| Orange| Mexico| 5000|\n|2019|  Beans| Canada| NULL|\n|2019|  Beans|  China| 1500|\n|2019|  Beans| Mexico| 2000|\n|2019| Banana| Canada| NULL|\n|2019| Banana|  China| 1400|\n|2019| Banana| Mexico|  400|\n|2019|Carrots| Canada| NULL|\n|2019|Carrots|  China|  200|\n|2019|Carrots| Mexico| NULL|\n+----+-------+-------+-----+\n</code></pre> Source code in <code>spark_frame/transformations_impl/unpivot.py</code> <pre><code>def unpivot(df: DataFrame, pivot_columns: List[str], key_alias: str = \"key\", value_alias: str = \"value\") -&gt; DataFrame:\n    \"\"\"Unpivot the given DataFrame along the specified pivot columns.\n    All columns that are not pivot columns should have the same type.\n\n    This is the inverse transformation of the [pyspark.sql.GroupedData.pivot][] operation.\n\n    Args:\n        df: A DataFrame\n        pivot_columns: The list of columns names on which to perform the pivot\n        key_alias: Alias given to the 'key' column\n        value_alias: Alias given to the 'value' column\n\n    Returns:\n        An unpivotted DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...    (2018, \"Orange\",  None, 4000, None),\n        ...    (2018, \"Beans\",   None, 1500, 2000),\n        ...    (2018, \"Banana\",  2000,  400, None),\n        ...    (2018, \"Carrots\", 2000, 1200, None),\n        ...    (2019, \"Orange\",  5000, None, 5000),\n        ...    (2019, \"Beans\",   None, 1500, 2000),\n        ...    (2019, \"Banana\",  None, 1400,  400),\n        ...    (2019, \"Carrots\", None,  200, None),\n        ...  ], \"year INT, product STRING, Canada INT, China INT, Mexico INT\"\n        ... )\n        &gt;&gt;&gt; df.show()\n        +----+-------+------+-----+------+\n        |year|product|Canada|China|Mexico|\n        +----+-------+------+-----+------+\n        |2018| Orange|  NULL| 4000|  NULL|\n        |2018|  Beans|  NULL| 1500|  2000|\n        |2018| Banana|  2000|  400|  NULL|\n        |2018|Carrots|  2000| 1200|  NULL|\n        |2019| Orange|  5000| NULL|  5000|\n        |2019|  Beans|  NULL| 1500|  2000|\n        |2019| Banana|  NULL| 1400|   400|\n        |2019|Carrots|  NULL|  200|  NULL|\n        +----+-------+------+-----+------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; unpivot(df, ['year', 'product'], key_alias='country', value_alias='total').show(100)\n        +----+-------+-------+-----+\n        |year|product|country|total|\n        +----+-------+-------+-----+\n        |2018| Orange| Canada| NULL|\n        |2018| Orange|  China| 4000|\n        |2018| Orange| Mexico| NULL|\n        |2018|  Beans| Canada| NULL|\n        |2018|  Beans|  China| 1500|\n        |2018|  Beans| Mexico| 2000|\n        |2018| Banana| Canada| 2000|\n        |2018| Banana|  China|  400|\n        |2018| Banana| Mexico| NULL|\n        |2018|Carrots| Canada| 2000|\n        |2018|Carrots|  China| 1200|\n        |2018|Carrots| Mexico| NULL|\n        |2019| Orange| Canada| 5000|\n        |2019| Orange|  China| NULL|\n        |2019| Orange| Mexico| 5000|\n        |2019|  Beans| Canada| NULL|\n        |2019|  Beans|  China| 1500|\n        |2019|  Beans| Mexico| 2000|\n        |2019| Banana| Canada| NULL|\n        |2019| Banana|  China| 1400|\n        |2019| Banana| Mexico|  400|\n        |2019|Carrots| Canada| NULL|\n        |2019|Carrots|  China|  200|\n        |2019|Carrots| Mexico| NULL|\n        +----+-------+-------+-----+\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    pivoted_columns = [(c, t) for (c, t) in df.dtypes if c not in pivot_columns]\n    cols, types = zip(*pivoted_columns)\n\n    # Check that all columns have the same type.\n    assert_true(\n        len(set(types)) == 1,\n        (\"All pivoted columns should be of the same type:\\n Pivoted columns are: %s\" % pivoted_columns),\n    )\n\n    # Create and explode an array of (column_name, column_value) structs\n    kvs = f.explode(\n        f.array(*[f.struct(f.lit(c).alias(key_alias), f.col(quote(c)).alias(value_alias)) for c in cols]),\n    ).alias(\"kvs\")\n\n    return df.select([f.col(c) for c in quote_columns(pivot_columns)] + [kvs]).select(\n        [*quote_columns(pivot_columns), \"kvs.*\"],\n    )\n</code></pre>"},{"location":"reference/transformations/#spark_frame.transformations_impl.with_generic_typed_struct.with_generic_typed_struct","title":"<code>with_generic_typed_struct(df: DataFrame, col_names: List[str]) -&gt; DataFrame</code>","text":"<p>Transform the specified struct columns of a given Dataframe into generic typed struct columns with the following generic schema (based on https://spark.apache.org/docs/latest/sql-ref-datatypes.html) :</p> <pre><code>STRUCT&lt;\n    key: STRING, -- (name of the field inside the struct)\n    type: STRING, -- (type of the field inside the struct)\n    value: STRUCT&lt; -- (all the fields will be null except for the one with the correct type)\n        date: DATE,\n        timestamp: TIMESTAMP,\n        int: LONG,\n        float: DOUBLE,\n        boolean: BOOLEAN,\n        string: STRING,\n        bytes: BINARY\n    &gt;\n&gt;\n</code></pre> <p>The following spark types will be automatically cast into the more generic following types:</p> <ul> <li><code>tinyint</code>, <code>smallint</code>, <code>int</code> -&gt; <code>bigint</code></li> <li><code>float</code>, <code>decimal</code> -&gt; <code>double</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Dataframe to transform</p> required <code>col_names</code> <code>List[str]</code> <p>A list of column names to transform</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Dataframe with the columns transformed into generic typed structs</p> <p>Limitations</p> <p>Currently, complex field types (structs, maps, arrays) are not supported. All fields of the struct columns to convert must be of basic types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...     [(1, {\"first.name\": \"Jacques\", \"age\": 25, \"is.an.adult\": True}),\n...      (2, {\"first.name\": \"Michel\", \"age\": 12, \"is.an.adult\": False}),\n...      (3, {\"first.name\": \"Marie\", \"age\": 36, \"is.an.adult\": True})],\n...     \"id INT, `person.struct` STRUCT&lt;`first.name`:STRING, age:INT, `is.an.adult`:BOOLEAN&gt;\"\n... )\n&gt;&gt;&gt; df.show(truncate=False)\n+---+-------------------+\n|id |person.struct      |\n+---+-------------------+\n|1  |{Jacques, 25, true}|\n|2  |{Michel, 12, false}|\n|3  |{Marie, 36, true}  |\n+---+-------------------+\n\n&gt;&gt;&gt; res = with_generic_typed_struct(df, [\"`person.struct`\"])\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- person.struct: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- key: string (nullable = false)\n |    |    |-- type: string (nullable = false)\n |    |    |-- value: struct (nullable = false)\n |    |    |    |-- boolean: boolean (nullable = true)\n |    |    |    |-- bytes: binary (nullable = true)\n |    |    |    |-- date: date (nullable = true)\n |    |    |    |-- float: double (nullable = true)\n |    |    |    |-- int: long (nullable = true)\n |    |    |    |-- string: string (nullable = true)\n |    |    |    |-- timestamp: timestamp (nullable = true)\n\n&gt;&gt;&gt; res.show(10, False)\n+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|id |person.struct                                                                                                                                                                                  |\n+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|1  |[{first.name, string, {NULL, NULL, NULL, NULL, NULL, Jacques, NULL}}, {age, int, {NULL, NULL, NULL, NULL, 25, NULL, NULL}}, {is.an.adult, boolean, {true, NULL, NULL, NULL, NULL, NULL, NULL}}]|\n|2  |[{first.name, string, {NULL, NULL, NULL, NULL, NULL, Michel, NULL}}, {age, int, {NULL, NULL, NULL, NULL, 12, NULL, NULL}}, {is.an.adult, boolean, {false, NULL, NULL, NULL, NULL, NULL, NULL}}]|\n|3  |[{first.name, string, {NULL, NULL, NULL, NULL, NULL, Marie, NULL}}, {age, int, {NULL, NULL, NULL, NULL, 36, NULL, NULL}}, {is.an.adult, boolean, {true, NULL, NULL, NULL, NULL, NULL, NULL}}]  |\n+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/with_generic_typed_struct.py</code> <pre><code>def with_generic_typed_struct(df: DataFrame, col_names: List[str]) -&gt; DataFrame:\n    \"\"\"Transform the specified struct columns of a given [Dataframe][pyspark.sql.DataFrame] into\n    generic typed struct columns with the following generic schema\n    (based on [https://spark.apache.org/docs/latest/sql-ref-datatypes.html](\n    https://spark.apache.org/docs/latest/sql-ref-datatypes.html)) :\n\n        STRUCT&lt;\n            key: STRING, -- (name of the field inside the struct)\n            type: STRING, -- (type of the field inside the struct)\n            value: STRUCT&lt; -- (all the fields will be null except for the one with the correct type)\n                date: DATE,\n                timestamp: TIMESTAMP,\n                int: LONG,\n                float: DOUBLE,\n                boolean: BOOLEAN,\n                string: STRING,\n                bytes: BINARY\n            &gt;\n        &gt;\n\n    The following spark types will be automatically cast into the more generic following types:\n\n    - `tinyint`, `smallint`, `int` -&gt; `bigint`\n    - `float`, `decimal` -&gt; `double`\n\n    Args:\n        df: The Dataframe to transform\n        col_names: A list of column names to transform\n\n    Returns:\n        A Dataframe with the columns transformed into generic typed structs\n\n    !!! warning \"Limitations\"\n        Currently, complex field types (structs, maps, arrays) are not supported.\n        All fields of the struct columns to convert must be of basic types.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...     [(1, {\"first.name\": \"Jacques\", \"age\": 25, \"is.an.adult\": True}),\n        ...      (2, {\"first.name\": \"Michel\", \"age\": 12, \"is.an.adult\": False}),\n        ...      (3, {\"first.name\": \"Marie\", \"age\": 36, \"is.an.adult\": True})],\n        ...     \"id INT, `person.struct` STRUCT&lt;`first.name`:STRING, age:INT, `is.an.adult`:BOOLEAN&gt;\"\n        ... )\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+-------------------+\n        |id |person.struct      |\n        +---+-------------------+\n        |1  |{Jacques, 25, true}|\n        |2  |{Michel, 12, false}|\n        |3  |{Marie, 36, true}  |\n        +---+-------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res = with_generic_typed_struct(df, [\"`person.struct`\"])\n        &gt;&gt;&gt; res.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- person.struct: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- key: string (nullable = false)\n         |    |    |-- type: string (nullable = false)\n         |    |    |-- value: struct (nullable = false)\n         |    |    |    |-- boolean: boolean (nullable = true)\n         |    |    |    |-- bytes: binary (nullable = true)\n         |    |    |    |-- date: date (nullable = true)\n         |    |    |    |-- float: double (nullable = true)\n         |    |    |    |-- int: long (nullable = true)\n         |    |    |    |-- string: string (nullable = true)\n         |    |    |    |-- timestamp: timestamp (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res.show(10, False)\n        +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        |id |person.struct                                                                                                                                                                                  |\n        +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        |1  |[{first.name, string, {NULL, NULL, NULL, NULL, NULL, Jacques, NULL}}, {age, int, {NULL, NULL, NULL, NULL, 25, NULL, NULL}}, {is.an.adult, boolean, {true, NULL, NULL, NULL, NULL, NULL, NULL}}]|\n        |2  |[{first.name, string, {NULL, NULL, NULL, NULL, NULL, Michel, NULL}}, {age, int, {NULL, NULL, NULL, NULL, 12, NULL, NULL}}, {is.an.adult, boolean, {false, NULL, NULL, NULL, NULL, NULL, NULL}}]|\n        |3  |[{first.name, string, {NULL, NULL, NULL, NULL, NULL, Marie, NULL}}, {age, int, {NULL, NULL, NULL, NULL, 36, NULL, NULL}}, {is.an.adult, boolean, {true, NULL, NULL, NULL, NULL, NULL, NULL}}]  |\n        +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        &lt;BLANKLINE&gt;\n    \"\"\"  # noqa: E501\n\n    source_to_cast = {\n        \"date\": \"date\",\n        \"timestamp\": \"timestamp\",\n        \"tinyint\": \"bigint\",\n        \"smallint\": \"bigint\",\n        \"int\": \"bigint\",\n        \"bigint\": \"bigint\",\n        \"float\": \"double\",\n        \"double\": \"double\",\n        \"boolean\": \"boolean\",\n        \"string\": \"string\",\n        \"binary\": \"binary\",\n    }\n    \"\"\"Mapping indicating for each source Spark DataTypes the type into which it will be cast.\"\"\"\n\n    cast_to_name = {\n        \"binary\": \"bytes\",\n        \"bigint\": \"int\",\n        \"double\": \"float\",\n    }\n    \"\"\"Mapping indicating for each already cast Spark DataTypes the name of the corresponding field.\n    When missing, the same name will be kept.\"\"\"\n\n    name_cast = {cast_to_name.get(value, value): value for value in source_to_cast.values()}\n    # We make sure the types are sorted\n    name_cast = dict(sorted(name_cast.items()))\n\n    def match_regex_types(source_type: str) -&gt; Optional[str]:\n        \"\"\"Matches the source types against regexes to identify more complex types (like Decimal(x, y))\"\"\"\n        regex_to_cast_types = [(re.compile(\"decimal(.*)\"), \"float\")]\n        for regex, cast_type in regex_to_cast_types:\n            if regex.match(source_type) is not None:\n                return cast_type\n        return None\n\n    def field_to_col(field: StructField, column_name: str) -&gt; Optional[Column]:\n        \"\"\"Transforms the specified field into a generic column\"\"\"\n        source_type = field.dataType.simpleString()\n        cast_type = source_to_cast.get(source_type)\n        field_name = column_name + \".\" + quote(field.name)\n        if cast_type is None:\n            cast_type = match_regex_types(source_type)\n        if cast_type is None:\n            print(\n                \"WARNING: The field {field_name} is of type {source_type} which is currently unsupported. \"\n                \"This field will be dropped.\".format(\n                    field_name=field_name,\n                    source_type=source_type,\n                ),\n            )\n            return None\n        name_type = cast_to_name.get(cast_type, cast_type)\n        return f.struct(\n            f.lit(field.name).alias(\"key\"),\n            f.lit(name_type).alias(\"type\"),\n            # In the code below, we use f.expr instead of f.col because it looks like f.col\n            # does not support column names with backquotes in them, but f.expr does :-p\n            f.struct(\n                *[\n                    (f.expr(field_name) if name_type == name_t else f.lit(None)).astype(cast_t).alias(name_t)\n                    for name_t, cast_t in name_cast.items()\n                ],\n            ).alias(\"value\"),\n        )\n\n    for col_name in col_names:\n        schema = _get_nested_col_type_from_schema(col_name, df.schema)\n        assert_true(isinstance(schema, StructType))\n        schema = cast(StructType, schema)\n        columns = [field_to_col(field, col_name) for field in schema.fields]\n        columns_2 = [col for col in columns if col is not None]\n        df = df.withColumn(unquote(col_name), f.array(*columns_2).alias(\"values\"))\n    return df\n</code></pre>"},{"location":"use_cases/comparing_dataframes/","title":"Comparing DataFrames","text":""},{"location":"use_cases/comparing_dataframes/#what-is-data-diff","title":"What is data-diff ?","text":"<p>DataFrame (or table) comparison is the most important feature that any SQL-based OLAP engine should have. I personally use it all the time whenever I work on a data pipeline, and I think it is so useful and powerful that it should be a built-in feature of any data pipeline development tool, just like <code>git diff</code> is the most important feature when you use git.</p>"},{"location":"use_cases/comparing_dataframes/#what-does-it-do","title":"What does it do ?","text":"<p>Simple: it compares two SQL tables (or DataFrames), and gives you a detailed summary of what changed between the two.</p>"},{"location":"use_cases/comparing_dataframes/#why-is-it-useful","title":"Why is it useful ?","text":"<p>During the past few decades, code diff, alongside automated testing, has become the cornerstone tool  used for implementing coding best-practices, like versioning and code reviews.  No sane developer would ever consider using versioning and code reviews if code-diff wasn't possible.</p> <p></p> Here is an example of code diff, everyone should be quite familiar with them already. <p>When manipulating complex data pipelines, in particular with code written in SQL or DataFrames, it quickly becomes extremely challenging to anticipate and make sure that a change made to the code will not have any unforeseen side effects.</p>"},{"location":"use_cases/comparing_dataframes/#how-to-get-started","title":"How to get started ?","text":"<p>Here is a minimal example of how you can use it : </p> <p>First, create a PySpark job with spark-frame and data-diff-viewer as dependencies    (check this project's README.md to know which versions of data-diff-viewer are compatible with spark-frame) </p> <p>Then run a PySpark job like this one: data_diff.py<pre><code>from pyspark.sql import SparkSession\nfrom spark_frame.data_diff import compare_dataframes\n\n# Instantiate the SparkSession the way you like\nspark = SparkSession.builder.appName(\"data-diff\").getOrCreate()\n\n# Read or compute the two DataFrames you want to compare together\ndf1 = spark.read.parquet(\"...\")\ndf2 = spark.table(\"...\")\n\n# Compare the two DataFrames\ndiff_result = compare_dataframes(df1, df2, join_cols=[...])\n# Export the diff result as HTML\ndiff_result.export_to_html(output_file_path=\"diff_report.html\")\n</code></pre></p> <p>And that's it! After the job has run, you should get an HTML report at the location specified with <code>output_file_path</code>.</p> <p>The only parameter to be wary of are:</p>"},{"location":"use_cases/comparing_dataframes/#join_cols","title":"join_cols","text":"<p><code>join_cols</code> indicates the list of column names that will be used to join the two DataFrame together for the  comparison. This set of columns should follow an unicity constraint in both DataFrames to prevent a  combinatorial explosion.</p> <p>Features</p> <ul> <li>If <code>join_cols</code> is not set, the algorithm will try to infer one column automatically, but it can only detect single    columns, if the DataFrames require multiple columns to be joined together, the automatic detection will not work.</li> <li>The algorithm comes with a safety mechanism to avoid performing joins that would lead to a combinatorial explosion   when the join_cols are incorrectly chosen.</li> </ul>"},{"location":"use_cases/comparing_dataframes/#output_file_path","title":"output_file_path","text":"<p>The path where the HTML report should be written.</p> <p>Features</p> <p>This method uses Spark's FileSystem API to write the report. This means that <code>output_file_path</code> behaves the same way as the path argument in <code>df.write.save(path)</code>:</p> <ul> <li>It can be a fully qualified URI pointing to a location on a remote filesystem   (e.g. \"hdfs://...\", \"s3://...\", etc.), provided that Spark is configured to access it</li> <li>If a relative path with no scheme is specified (e.g. <code>output_file_path=\"diff_report.html\"</code>), it will   write on Spark's default's output location. For example:<ul> <li>when running locally, it will be the process current working directory.</li> <li>when running on Hadoop, it will be the user's home directory on HDFS.</li> <li>when running on the cloud (EMR, Dataproc, Azure Synapse, Databricks), it should write on the default remote   storage linked to the cluster.</li> </ul> </li> </ul> <p>Methods used in this example</p> spark_frame.data_diff.compare_dataframes <p>Compares two DataFrames and return a <code>DiffResult</code> object.</p> <p>We first compare the DataFrame schemas. If the schemas are different, we adapt the DataFrames to make them as much comparable as possible: - If the order of the columns changed, we re-order them automatically to perform the diff - If the order of the fields inside a struct changed, we re-order them automatically to perform the diff - If a column type changed, we cast the column to the smallest common type - We don't recognize when a column is renamed, we treat it as if the old column was removed and the new column added</p> <p>If <code>join_cols</code> is specified, we will use the specified columns to perform the comparison join between the two DataFrames. Ideally, the <code>join_cols</code> should respect an unicity constraint.</p> <p>If they contain duplicates, a safety check is performed to prevent a potential combinatorial explosion: if the number of rows in the joined DataFrame would be more than twice the size of the original DataFrames, then an Exception is raised and the user will be asked to provide another set of <code>join_cols</code>.</p> <p>If no <code>join_cols</code> is specified, the algorithm will try to automatically find a single column suitable for the join. However, the automatic inference can only find join keys based on a single column. If the DataFrame's unique keys are composite (multiple columns) they must be given explicitly via <code>join_cols</code> to perform the diff analysis.</p> <p>Tips</p> <ul> <li>If you want to test a column renaming, you can temporarily add renaming steps to the DataFrame   you want to test.</li> <li>If you want to exclude columns from the diff, you can simply drop them from the DataFrames you want to   compare.</li> <li>When comparing arrays, this algorithm ignores their ordering (e.g. <code>[1, 2, 3] == [3, 2, 1]</code>).</li> <li>When dealing with a nested structure, if the struct contains a unique identifier, it can be specified   in the join_cols and the structure will be automatically unnested in the diff results.   For instance, if we have a structure <code>my_array: ARRAY&lt;STRUCT&lt;a, b, ...&gt;&gt;</code>   and if <code>a</code> is a unique identifier, then you can add <code>\"my_array!.a\"</code> in the join_cols argument.   (cf. Example 2)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>left_df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>right_df</code> <code>DataFrame</code> <p>Another DataFrame</p> required <code>join_cols</code> <code>Optional[List[str]]</code> <p>Specifies the columns on which the two DataFrames should be joined to compare them</p> <code>None</code> <p>Returns:</p> Type Description <code>DiffResult</code> <p>A DiffResult object</p> <p>Example 1: simple diff</p> <pre><code>&gt;&gt;&gt; from spark_frame.data_diff.compare_dataframes_impl import __get_test_dfs\n&gt;&gt;&gt; from spark_frame.data_diff import compare_dataframes\n&gt;&gt;&gt; df1, df2 = __get_test_dfs()\n</code></pre> <pre><code>&gt;&gt;&gt; df1.show()\n+---+-----------+\n| id|   my_array|\n+---+-----------+\n|  1|[{1, 2, 3}]|\n|  2|[{1, 2, 3}]|\n|  3|[{1, 2, 3}]|\n+---+-----------+\n</code></pre> <pre><code>&gt;&gt;&gt; df2.show()\n+---+--------------+\n| id|      my_array|\n+---+--------------+\n|  1|[{1, 2, 3, 4}]|\n|  2|[{2, 2, 3, 4}]|\n|  4|[{1, 2, 3, 4}]|\n+---+--------------+\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result = compare_dataframes(df1, df2)\n\nAnalyzing differences...\nNo join_cols provided: trying to automatically infer a column that can be used for joining the two DataFrames\nFound the following column: id\nGenerating the diff by joining the DataFrames together using the inferred column: id\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result.display()\nSchema has changed:\n@@ -1,2 +1,2 @@\n\n id INT\n-my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT&gt;&gt;\n+my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT,d:INT&gt;&gt;\nWARNING: columns that do not match both sides will be ignored\n\ndiff NOT ok\n\nRow count ok: 3 rows\n\n0 (0.0%) rows are identical\n2 (50.0%) rows have changed\n1 (25.0%) rows are only in 'left'\n1 (25.0%) rows are only in 'right\n\nFound the following changes:\n+-----------+-------------+---------------------+---------------------------+--------------+\n|column_name|total_nb_diff|left_value           |right_value                |nb_differences|\n+-----------+-------------+---------------------+---------------------------+--------------+\n|my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1             |\n|my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":2,\"b\":2,\"c\":3,\"d\":4}]|1             |\n+-----------+-------------+---------------------+---------------------------+--------------+\n\n1 rows were only found in 'left' :\nMost frequent values in 'left' for each column :\n+-----------+---------------------+---+\n|column_name|value                |nb |\n+-----------+---------------------+---+\n|id         |3                    |1  |\n|my_array   |[{\"a\":1,\"b\":2,\"c\":3}]|1  |\n+-----------+---------------------+---+\n\n1 rows were only found in 'right' :\nMost frequent values in 'right' for each column :\n+-----------+---------------------------+---+\n|column_name|value                      |nb |\n+-----------+---------------------------+---+\n|id         |4                          |1  |\n|my_array   |[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1  |\n+-----------+---------------------------+---+\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_1.html\")\nReport exported as test_working_dir/compare_dataframes_example_1.html\n</code></pre> <p>Check out the exported report here</p> <p>Example 2: diff on complex structures</p> <p>By adding <code>\"my_array!.a\"</code> to the join_cols argument, the array gets unnested for the diff</p> <pre><code>&gt;&gt;&gt; diff_result_unnested = compare_dataframes(df1, df2, join_cols=[\"id\", \"my_array!.a\"])\n\nAnalyzing differences...\nGenerating the diff by joining the DataFrames together using the provided column: id\nGenerating the diff by joining the DataFrames together using the provided columns: ['id', 'my_array!.a']\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result_unnested.display()\nSchema has changed:\n@@ -1,4 +1,5 @@\n\n id INT\n my_array!.a INT\n my_array!.b INT\n my_array!.c INT\n+my_array!.d INT\nWARNING: columns that do not match both sides will be ignored\n\ndiff NOT ok\n\nWARNING: This diff has multiple granularity levels, we will print the results for each granularity level,\n         but we recommend to export the results to html for a much more digest result.\n\n##############################################################\nGranularity : root (4 rows)\n\nRow count ok: 3 rows\n\n2 (50.0%) rows are identical\n0 (0.0%) rows have changed\n1 (25.0%) rows are only in 'left'\n1 (25.0%) rows are only in 'right\n\n1 rows were only found in 'left' :\nMost frequent values in 'left' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |3    |1  |\n|my_array!.a|1    |2  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n+-----------+-----+---+\n\n1 rows were only found in 'right' :\nMost frequent values in 'right' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |4    |1  |\n|my_array!.a|1    |1  |\n|my_array!.a|2    |1  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n|my_array!.d|4    |3  |\n+-----------+-----+---+\n\n##############################################################\nGranularity : my_array (5 rows)\n\nRow count ok: 3 rows\n\n1 (20.0%) rows are identical\n0 (0.0%) rows have changed\n2 (40.0%) rows are only in 'left'\n2 (40.0%) rows are only in 'right\n\n2 rows were only found in 'left' :\nMost frequent values in 'left' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |3    |1  |\n|my_array!.a|1    |2  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n+-----------+-----+---+\n\n2 rows were only found in 'right' :\nMost frequent values in 'right' for each column :\n+-----------+-----+---+\n|column_name|value|nb |\n+-----------+-----+---+\n|id         |4    |1  |\n|my_array!.a|1    |1  |\n|my_array!.a|2    |1  |\n|my_array!.b|2    |2  |\n|my_array!.c|3    |2  |\n|my_array!.d|4    |3  |\n+-----------+-----+---+\n</code></pre> <pre><code>&gt;&gt;&gt; diff_result_unnested.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_2.html\")\nReport exported as test_working_dir/compare_dataframes_example_2.html\n</code></pre> <p>Check out the exported report here</p> Source code in <code>spark_frame/data_diff/compare_dataframes_impl.py</code> <pre><code>def compare_dataframes(\n    left_df: DataFrame,\n    right_df: DataFrame,\n    join_cols: Optional[List[str]] = None,\n) -&gt; DiffResult:\n    \"\"\"Compares two DataFrames and return a [`DiffResult`][spark_frame.data_diff.diff_result.DiffResult] object.\n\n    We first compare the DataFrame schemas. If the schemas are different, we adapt the DataFrames to make them\n    as much comparable as possible:\n    - If the order of the columns changed, we re-order them automatically to perform the diff\n    - If the order of the fields inside a struct changed, we re-order them automatically to perform the diff\n    - If a column type changed, we cast the column to the smallest common type\n    - We don't recognize when a column is renamed, we treat it as if the old column was removed and the new column added\n\n    If `join_cols` is specified, we will use the specified columns to perform the comparison join between the\n    two DataFrames. Ideally, the `join_cols` should respect an unicity constraint.\n\n    If they contain duplicates, a safety check is performed to prevent a potential combinatorial explosion:\n    if the number of rows in the joined DataFrame would be more than twice the size of the original DataFrames,\n    then an Exception is raised and the user will be asked to provide another set of `join_cols`.\n\n    If no `join_cols` is specified, the algorithm will try to automatically find a single column suitable for\n    the join. However, the automatic inference can only find join keys based on a single column.\n    If the DataFrame's unique keys are composite (multiple columns) they must be given explicitly via `join_cols`\n    to perform the diff analysis.\n\n    !!! tip \"Tips\"\n        - If you want to test a column renaming, you can temporarily add renaming steps to the DataFrame\n          you want to test.\n        - If you want to exclude columns from the diff, you can simply drop them from the DataFrames you want to\n          compare.\n        - When comparing arrays, this algorithm ignores their ordering (e.g. `[1, 2, 3] == [3, 2, 1]`).\n        - When dealing with a nested structure, if the struct contains a unique identifier, it can be specified\n          in the join_cols and the structure will be automatically unnested in the diff results.\n          For instance, if we have a structure `my_array: ARRAY&lt;STRUCT&lt;a, b, ...&gt;&gt;`\n          and if `a` is a unique identifier, then you can add `\"my_array!.a\"` in the join_cols argument.\n          (cf. Example 2)\n\n    Args:\n        left_df: A Spark DataFrame\n        right_df: Another DataFrame\n        join_cols: Specifies the columns on which the two DataFrames should be joined to compare them\n\n    Returns:\n        A DiffResult object\n\n    Examples: Example 1: simple diff\n        &gt;&gt;&gt; from spark_frame.data_diff.compare_dataframes_impl import __get_test_dfs\n        &gt;&gt;&gt; from spark_frame.data_diff import compare_dataframes\n        &gt;&gt;&gt; df1, df2 = __get_test_dfs()\n\n        &gt;&gt;&gt; df1.show()\n        +---+-----------+\n        | id|   my_array|\n        +---+-----------+\n        |  1|[{1, 2, 3}]|\n        |  2|[{1, 2, 3}]|\n        |  3|[{1, 2, 3}]|\n        +---+-----------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; df2.show()\n        +---+--------------+\n        | id|      my_array|\n        +---+--------------+\n        |  1|[{1, 2, 3, 4}]|\n        |  2|[{2, 2, 3, 4}]|\n        |  4|[{1, 2, 3, 4}]|\n        +---+--------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; diff_result = compare_dataframes(df1, df2)\n        &lt;BLANKLINE&gt;\n        Analyzing differences...\n        No join_cols provided: trying to automatically infer a column that can be used for joining the two DataFrames\n        Found the following column: id\n        Generating the diff by joining the DataFrames together using the inferred column: id\n\n        &gt;&gt;&gt; diff_result.display()\n        Schema has changed:\n        @@ -1,2 +1,2 @@\n        &lt;BLANKLINE&gt;\n         id INT\n        -my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT&gt;&gt;\n        +my_array ARRAY&lt;STRUCT&lt;a:INT,b:INT,c:INT,d:INT&gt;&gt;\n        WARNING: columns that do not match both sides will be ignored\n        &lt;BLANKLINE&gt;\n        diff NOT ok\n        &lt;BLANKLINE&gt;\n        Row count ok: 3 rows\n        &lt;BLANKLINE&gt;\n        0 (0.0%) rows are identical\n        2 (50.0%) rows have changed\n        1 (25.0%) rows are only in 'left'\n        1 (25.0%) rows are only in 'right\n        &lt;BLANKLINE&gt;\n        Found the following changes:\n        +-----------+-------------+---------------------+---------------------------+--------------+\n        |column_name|total_nb_diff|left_value           |right_value                |nb_differences|\n        +-----------+-------------+---------------------+---------------------------+--------------+\n        |my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1             |\n        |my_array   |2            |[{\"a\":1,\"b\":2,\"c\":3}]|[{\"a\":2,\"b\":2,\"c\":3,\"d\":4}]|1             |\n        +-----------+-------------+---------------------+---------------------------+--------------+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'left' :\n        Most frequent values in 'left' for each column :\n        +-----------+---------------------+---+\n        |column_name|value                |nb |\n        +-----------+---------------------+---+\n        |id         |3                    |1  |\n        |my_array   |[{\"a\":1,\"b\":2,\"c\":3}]|1  |\n        +-----------+---------------------+---+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'right' :\n        Most frequent values in 'right' for each column :\n        +-----------+---------------------------+---+\n        |column_name|value                      |nb |\n        +-----------+---------------------------+---+\n        |id         |4                          |1  |\n        |my_array   |[{\"a\":1,\"b\":2,\"c\":3,\"d\":4}]|1  |\n        +-----------+---------------------------+---+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_1.html\")\n        Report exported as test_working_dir/compare_dataframes_example_1.html\n\n        [Check out the exported report here](../diff_reports/compare_dataframes_example_1.html)\n\n    Examples: Example 2: diff on complex structures\n        By adding `\"my_array!.a\"` to the join_cols argument, the array gets unnested for the diff\n        &gt;&gt;&gt; diff_result_unnested = compare_dataframes(df1, df2, join_cols=[\"id\", \"my_array!.a\"])\n        &lt;BLANKLINE&gt;\n        Analyzing differences...\n        Generating the diff by joining the DataFrames together using the provided column: id\n        Generating the diff by joining the DataFrames together using the provided columns: ['id', 'my_array!.a']\n\n        &gt;&gt;&gt; diff_result_unnested.display()\n        Schema has changed:\n        @@ -1,4 +1,5 @@\n        &lt;BLANKLINE&gt;\n         id INT\n         my_array!.a INT\n         my_array!.b INT\n         my_array!.c INT\n        +my_array!.d INT\n        WARNING: columns that do not match both sides will be ignored\n        &lt;BLANKLINE&gt;\n        diff NOT ok\n        &lt;BLANKLINE&gt;\n        WARNING: This diff has multiple granularity levels, we will print the results for each granularity level,\n                 but we recommend to export the results to html for a much more digest result.\n        &lt;BLANKLINE&gt;\n        ##############################################################\n        Granularity : root (4 rows)\n        &lt;BLANKLINE&gt;\n        Row count ok: 3 rows\n        &lt;BLANKLINE&gt;\n        2 (50.0%) rows are identical\n        0 (0.0%) rows have changed\n        1 (25.0%) rows are only in 'left'\n        1 (25.0%) rows are only in 'right\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'left' :\n        Most frequent values in 'left' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |3    |1  |\n        |my_array!.a|1    |2  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n        1 rows were only found in 'right' :\n        Most frequent values in 'right' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |4    |1  |\n        |my_array!.a|1    |1  |\n        |my_array!.a|2    |1  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        |my_array!.d|4    |3  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n        ##############################################################\n        Granularity : my_array (5 rows)\n        &lt;BLANKLINE&gt;\n        Row count ok: 3 rows\n        &lt;BLANKLINE&gt;\n        1 (20.0%) rows are identical\n        0 (0.0%) rows have changed\n        2 (40.0%) rows are only in 'left'\n        2 (40.0%) rows are only in 'right\n        &lt;BLANKLINE&gt;\n        2 rows were only found in 'left' :\n        Most frequent values in 'left' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |3    |1  |\n        |my_array!.a|1    |2  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n        2 rows were only found in 'right' :\n        Most frequent values in 'right' for each column :\n        +-----------+-----+---+\n        |column_name|value|nb |\n        +-----------+-----+---+\n        |id         |4    |1  |\n        |my_array!.a|1    |1  |\n        |my_array!.a|2    |1  |\n        |my_array!.b|2    |2  |\n        |my_array!.c|3    |2  |\n        |my_array!.d|4    |3  |\n        +-----------+-----+---+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; diff_result_unnested.export_to_html(output_file_path=\"test_working_dir/compare_dataframes_example_2.html\")\n        Report exported as test_working_dir/compare_dataframes_example_2.html\n\n        [Check out the exported report here](../diff_reports/compare_dataframes_example_2.html)\n    \"\"\"\n    print(\"\\nAnalyzing differences...\")\n\n    if join_cols == []:\n        join_cols = None\n    specified_join_cols = join_cols\n    left_df = convert_all_maps_to_arrays(left_df)\n    right_df = convert_all_maps_to_arrays(right_df)\n\n    if join_cols is None:\n        left_flat = flatten(left_df, struct_separator=STRUCT_SEPARATOR_REPLACEMENT)\n        right_flat = flatten(right_df, struct_separator=STRUCT_SEPARATOR_REPLACEMENT)\n        join_cols, _ = _get_join_cols(\n            left_flat,\n            right_flat,\n            join_cols,\n        )\n    else:\n        validate_fields_exist(join_cols, nested.fields(left_df))\n        validate_fields_exist(join_cols, nested.fields(right_df))\n\n    global_schema_diff_result = diff_dataframe_schemas(left_df, right_df, join_cols)\n    left_df, right_df = _harmonize_and_normalize_dataframes(\n        left_df,\n        right_df,\n        skip_make_dataframes_comparable=global_schema_diff_result.same_schema,\n    )\n\n    diff_dataframe_shards = _build_diff_dataframe_shards(\n        left_df,\n        right_df,\n        global_schema_diff_result,\n        join_cols,\n        specified_join_cols,\n    )\n    diff_result = DiffResult(\n        global_schema_diff_result,\n        diff_dataframe_shards,\n        join_cols,\n    )\n\n    return diff_result\n</code></pre> spark_frame.data_diff.DiffResult.export_to_html <p>Generate an HTML report of this diff result.</p> <p>This generates an HTML report file at the specified <code>output_file_path</code> URI location.</p> <p>The report file can be opened directly with a web browser, even without any internet connection.</p> <p>Info</p> <p>This method uses Spark's FileSystem API to write the report. This means that <code>output_file_path</code> behaves the same way as the path argument in <code>df.write.save(path)</code>:</p> <ul> <li>It can be a fully qualified URI pointing to a location on a remote filesystem   (e.g. \"hdfs://...\", \"s3://...\", etc.), provided that Spark is configured to access it</li> <li>If a relative path with no scheme is specified (e.g. <code>output_file_path=\"diff_report.html\"</code>), it will   write on Spark's default's output location. For example:<ul> <li>when running locally, it will be the process current working directory.</li> <li>when running on Hadoop, it will be the user's home directory on HDFS.</li> <li>when running on the cloud (EMR, Dataproc, Azure Synapse, Databricks), it should write on the   default remote storage linked to the cluster.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>Optional[str]</code> <p>The title of the report</p> <code>None</code> <code>encoding</code> <code>str</code> <p>Encoding used when writing the html report</p> <code>'utf8'</code> <code>output_file_path</code> <code>str</code> <p>URI of the file to write to.</p> <code>'diff_report.html'</code> <code>diff_format_options</code> <code>Optional[DiffFormatOptions]</code> <p>Formatting options</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n&gt;&gt;&gt; diff_result = _get_test_diff_result()\n&gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/diff_result_export_to_html_example.html\")\nReport exported as test_working_dir/diff_result_export_to_html_example.html\n</code></pre> <p>Check out the exported report here</p> Source code in <code>spark_frame/data_diff/diff_result.py</code> <pre><code>def export_to_html(\n    self,\n    title: Optional[str] = None,\n    output_file_path: str = \"diff_report.html\",\n    encoding: str = \"utf8\",\n    diff_format_options: Optional[DiffFormatOptions] = None,\n) -&gt; None:\n    \"\"\"Generate an HTML report of this diff result.\n\n    This generates an HTML report file at the specified `output_file_path` URI location.\n\n    The report file can be opened directly with a web browser, even without any internet connection.\n\n    !!! info\n        This method uses Spark's FileSystem API to write the report.\n        This means that `output_file_path` behaves the same way as the path argument in `df.write.save(path)`:\n\n        - It can be a fully qualified URI pointing to a location on a remote filesystem\n          (e.g. \"hdfs://...\", \"s3://...\", etc.), provided that Spark is configured to access it\n        - If a relative path with no scheme is specified (e.g. `output_file_path=\"diff_report.html\"`), it will\n          write on Spark's default's output location. For example:\n            - when running locally, it will be the process current working directory.\n            - when running on Hadoop, it will be the user's home directory on HDFS.\n            - when running on the cloud (EMR, Dataproc, Azure Synapse, Databricks), it should write on the\n              default remote storage linked to the cluster.\n\n    Args:\n        title: The title of the report\n        encoding: Encoding used when writing the html report\n        output_file_path: URI of the file to write to.\n        diff_format_options: Formatting options\n\n    Examples:\n        &gt;&gt;&gt; from spark_frame.data_diff.diff_result import _get_test_diff_result\n        &gt;&gt;&gt; diff_result = _get_test_diff_result()\n        &gt;&gt;&gt; diff_result.export_to_html(output_file_path=\"test_working_dir/diff_result_export_to_html_example.html\")\n        Report exported as test_working_dir/diff_result_export_to_html_example.html\n\n        [Check out the exported report here](../diff_reports/diff_result_export_to_html_example.html)\n    \"\"\"\n    if diff_format_options is None:\n        diff_format_options = DiffFormatOptions()\n    from spark_frame.data_diff.diff_result_analyzer import DiffResultAnalyzer\n\n    analyzer = DiffResultAnalyzer(diff_format_options)\n    diff_result_summary = analyzer.get_diff_result_summary(self)\n    export_html_diff_report(\n        diff_result_summary,\n        title=title,\n        output_file_path=output_file_path,\n        encoding=encoding,\n    )\n</code></pre>"},{"location":"use_cases/comparing_dataframes/#examples","title":"Examples","text":""},{"location":"use_cases/comparing_dataframes/#more-examples-coming-soon","title":"More examples coming soon!","text":""},{"location":"use_cases/comparing_dataframes/#simple-examples","title":"Simple examples","text":"<p>Some simple examples are available in the reference of the method  spark_frame.data_diff.compare_dataframes.</p>"},{"location":"use_cases/comparing_dataframes/#french-gas-price","title":"French gas price","text":"<p>Here is an example of diff made with Open Data: The French government maintains in real-time a dataset giving the price of gas at the pump in every French gas station </p> <p>It's a zip file containing an XML file that can be downloaded at this URL and is refreshed every 10 minutes: https://donnees.roulez-eco.fr/opendata/instantane</p> <p>We used data-diff to display the changes between the 2023-12-28 and the 2023-12-30. It was an interesting use case because the dataset is heavily nested as can be seen by displaying its schema:</p> df_2023_12_28.printSchema()<pre><code>root\n |-- cp: long (nullable = true)\n |-- id: long (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- pop: string (nullable = true)\n |-- adresse: string (nullable = true)\n |-- horaires: struct (nullable = false)\n |    |-- automate-24-24: long (nullable = true)\n |    |-- jour: array (nullable = true)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- value: string (nullable = true)\n |    |    |    |-- ferme: long (nullable = true)\n |    |    |    |-- id: long (nullable = true)\n |    |    |    |-- nom: string (nullable = true)\n |    |    |    |-- horaire: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |    |-- value: string (nullable = true)\n |    |    |    |    |    |-- fermeture: double (nullable = true)\n |    |    |    |    |    |-- ouverture: double (nullable = true)\n |-- prix: array (nullable = true)\n |    |-- element: struct (containsNull = false)\n |    |    |-- value: string (nullable = true)\n |    |    |-- id: long (nullable = true)\n |    |    |-- maj: timestamp (nullable = true)\n |    |    |-- nom: string (nullable = true)\n |    |    |-- valeur: double (nullable = true)\n |-- services: struct (nullable = false)\n |    |-- service: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- ville: string (nullable = true)\n</code></pre> <p>Or, more simply, if we use <code>nested.print_schema</code> instead:</p> nested.print_schema(df_2023_12_28)<pre><code>root\n |-- cp: long (nullable = true)\n |-- id: long (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- pop: string (nullable = true)\n |-- adresse: string (nullable = true)\n |-- horaires.automate-24-24: long (nullable = true)\n |-- horaires.jour!.value: string (nullable = true)\n |-- horaires.jour!.ferme: long (nullable = true)\n |-- horaires.jour!.id: long (nullable = true)\n |-- horaires.jour!.nom: string (nullable = true)\n |-- horaires.jour!.horaire!.value: string (nullable = true)\n |-- horaires.jour!.horaire!.fermeture: double (nullable = true)\n |-- horaires.jour!.horaire!.ouverture: double (nullable = true)\n |-- prix!.value: string (nullable = true)\n |-- prix!.id: long (nullable = true)\n |-- prix!.maj: timestamp (nullable = true)\n |-- prix!.nom: string (nullable = true)\n |-- prix!.valeur: double (nullable = true)\n |-- services.service!: string (nullable = true)\n |-- ville: string (nullable = true)\n</code></pre> <p>We used the following code to generate the diff_result and export it as HTML:</p> <pre><code>from spark_frame.data_diff import compare_dataframes, DiffFormatOptions\n\ndiff_result = compare_dataframes(df_2023_12_28, df_2023_12_30, join_cols=[\"id\", \"horaires.jour!.id\", \"prix!.id\"])\ndiff_format_options = DiffFormatOptions(\n    nb_top_values_kept_per_column=20,\n    left_df_alias=\"2023-12-28\",\n    right_df_alias=\"2023-12-30\",\n)\ndiff_result.export_to_html(\n    title=\"Comparaison des prix du carburant en France entre le 2023-12-28 et le 2023-12-30\",\n    diff_format_options=diff_format_options\n)\n</code></pre> <p>The interesting part is <code>join_cols=[\"id\", \"horaires.jour!.id\", \"prix!.id\"]</code> at line 3. This allows us to automatically explode the arrays <code>horaires.jour</code> and <code>prix</code> to make the diff  much more readable.</p> <p>The HTML report is available here:</p> <p>Comparaison des prix du carburant en France entre le 2023-12-28 et le 2023-12-30</p>"},{"location":"use_cases/comparing_dataframes/#what-are-some-common-use-cases","title":"What are some common use cases ?","text":"<p>From the simplest to the most complex, data-diff is super useful in many cases :</p>"},{"location":"use_cases/comparing_dataframes/#refactoring-a-single-sql-query","title":"Refactoring a single SQL query","text":"<p>Refactoring a single SQL query is something I do very often when I get my hand on legacy code. I do it quite often in order to :</p> <ul> <li>Improve the readability of the code.</li> <li>Get to know the code better (\"why is it done like this and not like that ?\").</li> <li>Improving the performances.</li> </ul> <p>Usually, the results of the refactoring process will be a new query which is much cleaner, more efficient, but produces exactly the same result.</p> <p>That's when data-diff becomes extremely handy, as it can make sure the results are 100% identical.</p> <p>Funny story: more than once, after refactoring a SQL query, I noticed small differences in the results thanks to data-diff.  After further inspection I sometimes realized that my refactoring had introduced a new bug, but other times  I realized that my refactoring actually fixed a bug that was previously existing and went unnoticed until then. </p> <p>But of course, what we said also applies to data transformations written with DataFrames, and also to whole pipelines chaining multiple transformations or queries.</p>"},{"location":"use_cases/comparing_dataframes/#refactoring-a-whole-data-pipeline","title":"Refactoring a whole data pipeline","text":"<p>Refactoring a data pipeline is often one of the most daunting and scary tasks that an Analytics Engineer has to carry. But there are many good reasons to do it, such as:</p> <ul> <li>To reduce code duplication (when you realise that the exact same CTE appears in 3 or 4 different queries) </li> <li>To improve maintainability, readability, stability, documentation, testing: if a query contains 20 CTEs,   it's probably a good idea to split it in smaller parts, thus making the intermediary results easier to document,    test and inspect.</li> <li>For many other cases listed below in dedicated items</li> </ul> <p>Here again, when we do so we want to make sure the results are exactly the same as they were before.</p>"},{"location":"use_cases/comparing_dataframes/#fixing-a-bug-in-a-query","title":"Fixing a bug in a query","text":"<p>Whenever I work on a fix, I use data-diff to make sure that the changes I wrote have the exact effect I was planning to have on the query and nothing more. I also make sure that this change does not impact the tables downstream.</p> Example <p>Here is a very simple example of a fix that could go wrong: you notice one of your type has a column <code>country</code>  that contains the following value counts:</p> direction count FRANCE 124 590 france 4 129 germany 209 643 GERMANY 1 2345 <p>Note</p> <p>As this is a toy example, we won't go far into the details of why such things happened in the first place.  Let's imagine you work for a French company that bought a German company, and you are merging the two information  systems, and that at some point the prod platform was updated to make sure only UPPERCASE was used. Needless to stress the importance of making sure that your input data is of the highest possible quality  using data contracts...</p> <p>You decide to apply a fix by passing everything to uppercase, adding a nice <code>\"SELECT UPPER(direction) as direction\"</code> somewhere in the code, ideally during the cleaning or bronze stage of your data pipeline. (Needless to say, it would be even better if the golden source for your data were fixed and backfilled, or even better if that inconsistency  never happened in the first place, but that kind of perfect solution requires a strong commitment at every level of your  organisation, and the metrics in your CEO's dashboard needs to be fixed today, not in a few months...) </p> <p>So you implement that fix, and run data-diff to make sure the results are good. You get something like this:</p> <pre><code>136935 (39.0%) rows are identical\n213772 (61.0%) rows have changed\n0 (0.0%) rows are only in 'left'\n0 (0.0%) rows are only in 'right\n\nFound the following changes:\n+-----------+-------------+---------------------+---------------------------+--------------+\n|column_name|total_nb_diff|left_value           |right_value                |nb_differences|\n+-----------+-------------+---------------------+---------------------------+--------------+\n|direction  |213772       |germany              |GERMANY                    |209643        |\n|direction  |213772       |france               |FRANCE                     |4129          |\n+-----------+-------------+---------------------+---------------------------+--------------+\n</code></pre> <p>With this, you are now 100% sure that the change you wrote did not impact anything else... at least on this table. Let's say that you now recompute this table and use data-diff on the table downstream, and you notice that one of your tables (the one that generates the CEO's dashboard) has the following change:</p> <pre><code>136935 (39.0%) rows are identical\n213772 (61.0%) rows have changed\n0 (0.0%) rows are only in 'left'\n0 (0.0%) rows are only in 'right\n\nFound the following changes:\n+------------+-------------+---------------------+---------------------------+--------------+\n|column_name |total_nb_diff|left_value           |right_value                |nb_differences|\n+------------+-------------+---------------------+---------------------------+--------------+\n|country_code|213772       |DE                   |NULL                       |209643        |\n|country_code|213772       |NULL                 |FR                         |4129          |\n+------------+-------------+---------------------+---------------------------+--------------+\n</code></pre> <p>Now, that change is unexpected. So you go have a look at the SQL query generating this dashboard, and you notice this:</p> <pre><code>SELECT\n    ...\n    CASE \n        WHEN country = \"germany\" THEN \"DE\"\n        WHEN country = \"FRANCE\" THEN \"FR\n    END as country_code,\n    ...\n</code></pre> <p>Now it all makes sense! Perhaps the query used to be \"correct\" at some point in time when \"germany\" and \"FRANCE\" were the only possible values in that column, but this is not the case anymore. And by fixing one bug upstream,  you had unforeseen impacts on the downstream tables. Thankfully data-diff made it very easy to spot ! </p> <p>Note</p> <p>Of course, upon reading this, any senior analytics engineer is probably thinking that many things went wrong in the company to lead to this result. Indeed. Surely, the engineer who wrote that <code>CASE WHEN</code> statement should have performed some safety data harmonization in the cleaning stage of the pipeline. Surely, the developers of the data source never should have sent inconsistent data like this. But nevertheless that kind of scenario (and  much worse ones) often happens in real life, especially when you arrive in a young company that  \"go fast and break things\" and you inherit the legacy of your predecessors. </p>"},{"location":"use_cases/comparing_dataframes/#implementing-a-new-feature","title":"Implementing a new feature","text":"<p>Sometimes I am tasked with adding a new column, or enriching the content of an existing column.  Once again, data-diff makes it very easy to make sure that I added the new column in the right place and that it  contains the expected values.</p>"},{"location":"use_cases/comparing_dataframes/#reviewing-others-changes","title":"Reviewing other's changes","text":"<p>Code review is one of the most important engineering best practices of this century. I find that some data teams still don't do it as much as they should, but we are getting there. dbt did a lot for the community to bring engineering best-practices to the data teams. What makes code reviews even better and easier, is when a data-diff comes with it. <code>DataDiffResults</code> can be exported as standalone HTML reports, which makes them very easy to share to others, or to post in the comment of a Merge Request. If your DataOps are good enough, they can even automate the whole  process and make your CI pipeline generate and post the data-diff report automatically for you.</p> <p>If you prefer premium tech rather than building things yourself, I strongly suggest you have a look  at DataFold who provides on-the-shelf CI/CD for data teams, including a  nice data-diff feature.</p> <p>Note</p> <p>At this point, you are probably wondering why I went all the way to make my own data-diff tool,  if I recommend trying another paying tool that already does it on the shelf. Here a few elements of response:</p> <ul> <li>Spark-frame is 100% free and open-source.</li> <li>Datafold does have an open-source data-diff version, but it is much more   limited (and does not generate HTML reports). If I ever have the time, I will make a detailed feature comparison    of both data-diffs.</li> <li>I believe spark-frame's data-diff is more powerful than Datafold's premium data-diff version, because it works   well with complex data structures too.</li> <li>I hope people will use spark-frame's (and bigquery-frame's) data-diff    to build up more nice features to easily have a full data-diff integration in their CI/CD pipelines. </li> </ul>"},{"location":"use_cases/comparing_dataframes/#see-what-changed-in-your-data","title":"See what changed in your data","text":"<p>When receiving full updates on a particular dataset.  Data-diff can be simply used to display the changes that occurred between the old and new version of the data.</p> <p>This can be useful to troubleshoot issues, or simply to know what changed.</p> <p>However, this not the primary use-case we had in mind when making data-diff, and some other data-drift monitoring tools might be better suited for this (like displaying the number of rows added per day, etc.).</p> <p>Even though, advanced users might want to take a look at the  <code>DiffResult.diff_df_shards</code> attribute  that provides access to the whole diff DataFrame, which can be used to retrieve the exact set of rows that were updated, for instance.</p>"},{"location":"use_cases/comparing_dataframes/#releasing-your-data-models-to-prod-like-a-pro","title":"Releasing your data-models to prod like a pro","text":"<p>For me, the Graal  of analytics engineering best practices is to achieve full versioning of your data model. Just like well-maintained libraries are fully versioned.</p> <p>That would mean that every table you provide to your users has a version number. Whenever you introduce a breaking change in the table's schema or data, you increase the minor version number (and you keep the major version for full pipeline overhauls). You then maintain your pipelines for multiple versions, and leave the time for your users to catch up with the changes before decommissioning the older versions.</p> <p>Of course, this is quite difficult to achieve in practice because:</p> <ol> <li>It's complicated to put in place:<ul> <li>SQL warehouses were not designed to facilitate this kind of logic<sup>1</sup>.</li> <li>Most SQL development frameworks don't support this either<sup>2</sup>. </li> </ul> </li> <li>It can be very costly:<ul> <li>If your data warehouse already costs you an arm, then maintaining two or more versions of it would cost as many    more limbs. This is clearly not feasible when you are trying to optimize costs.</li> </ul> </li> </ol> <p>One simpler alternative to versioning exists, it is called  Blue-Green deployment. It is a concept used in infrastructure deployment but the idea can be adapted to data pipelines. The main advantage is that it limits the number of versions of your data to 2. But it means that you need  your users to adapt to the new version quickly before being able to continue pushing new breaking changes.</p> <p>Whichever strategy you choose, versioning or blue-green, I believe it would be extremely valuable for end users  to get release notes whenever a model evolves, and if those notes showed you exactly which table changed and how.  Not only for the table's schema but also for the table's data. That would be, for me, the apex of analytics  engineering best practices.</p> <ol> <li> <p>Which is a shame, really. I really hope that one day Spark and BigQuery will natively support version numbers for tables.\u00a0\u21a9</p> </li> <li> <p>Perhaps dbt will one day, or perhaps some dbt plugins will or already do.  sqlmesh does seem to go in that direction, which is nice.\u00a0\u21a9</p> </li> </ol>"},{"location":"use_cases/flatten_unflatten/","title":"Using flatten/unflatten","text":""},{"location":"use_cases/flatten_unflatten/#transforming-nested-fields","title":"Transforming nested fields","text":"<p>Warning</p> <p>The use case presented in this page is deprecated, but is kept to illustrate what flatten/unflatten can do. The spark_frame.nested module is much more powerful for manipulating nested data,  because unlike flatten/unflatten, it does work with arrays. We recommend checking  this use-case to see the spark_frame.nested module in action.</p> <p>This example demonstrates how the <code>spark_frame.transformations.flatten</code> and unflatten <code>spark_frame.transformations.unflatten</code> methods can be used to make data cleaning pipeline easier with PySpark.</p> <p>Let's take a sample DataFrame with our favorite example: Pokemons</p> <pre><code>&gt;&gt;&gt; from spark_frame.examples.flatten_unflatten import _get_sample_pokemon_data\n&gt;&gt;&gt; df = _get_sample_pokemon_data()\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- base_stats: struct (nullable = true)\n |    |-- Attack: long (nullable = true)\n |    |-- Defense: long (nullable = true)\n |    |-- HP: long (nullable = true)\n |    |-- Sp Attack: long (nullable = true)\n |    |-- Sp Defense: long (nullable = true)\n |    |-- Speed: long (nullable = true)\n |-- id: long (nullable = true)\n |-- name: struct (nullable = true)\n |    |-- english: string (nullable = true)\n |    |-- french: string (nullable = true)\n |-- types: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n&gt;&gt;&gt; df.show(vertical=True, truncate=False)\n-RECORD 0------------------------------\n base_stats | {49, 49, 45, 65, 65, 45}\n id         | 1\n name       | {Bulbasaur, Bulbizarre}\n types      | [Grass, Poison]\n</code></pre> <p>Let's say we want to add a new enrich the \"base_stats\" struct with a new field named \"Total\".</p> <p>Methods used in this example</p> transformations.flatten <p>Flatten all the struct columns of a Spark DataFrame. Nested fields names will be joined together using the specified separator</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>struct_separator</code> <code>str</code> <p>A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots</p> <code>'.'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A flattened DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame(\n...         [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})],\n...         \"id INT, s STRUCT&lt;a:INT, b:STRUCT&lt;c:INT, d:INT&gt;&gt;\"\n...      )\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s: struct (nullable = true)\n |    |-- a: integer (nullable = true)\n |    |-- b: struct (nullable = true)\n |    |    |-- c: integer (nullable = true)\n |    |    |-- d: integer (nullable = true)\n\n&gt;&gt;&gt; flatten(df).printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.a: integer (nullable = true)\n |-- s.b.c: integer (nullable = true)\n |-- s.b.d: integer (nullable = true)\n\n&gt;&gt;&gt; df = spark.createDataFrame(\n...         [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})],\n...         \"id INT, `s.s1` STRUCT&lt;`a.a1`:INT, `b.b1`:STRUCT&lt;`c.c1`:INT, `d.d1`:INT&gt;&gt;\"\n... )\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1: struct (nullable = true)\n |    |-- a.a1: integer (nullable = true)\n |    |-- b.b1: struct (nullable = true)\n |    |    |-- c.c1: integer (nullable = true)\n |    |    |-- d.d1: integer (nullable = true)\n\n&gt;&gt;&gt; flatten(df, \"?\").printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1?a.a1: integer (nullable = true)\n |-- s.s1?b.b1?c.c1: integer (nullable = true)\n |-- s.s1?b.b1?d.d1: integer (nullable = true)\n</code></pre> Source code in <code>spark_frame/transformations_impl/flatten.py</code> <pre><code>def flatten(df: DataFrame, struct_separator: str = \".\") -&gt; DataFrame:\n    \"\"\"Flatten all the struct columns of a Spark [DataFrame][pyspark.sql.DataFrame].\n    Nested fields names will be joined together using the specified separator\n\n    Args:\n        df: A Spark DataFrame\n        struct_separator: A string used to separate the structs names from their elements.\n            It might be useful to change the separator when some DataFrame's column names already contain dots\n\n    Returns:\n        A flattened DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...         [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})],\n        ...         \"id INT, s STRUCT&lt;a:INT, b:STRUCT&lt;c:INT, d:INT&gt;&gt;\"\n        ...      )\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s: struct (nullable = true)\n         |    |-- a: integer (nullable = true)\n         |    |-- b: struct (nullable = true)\n         |    |    |-- c: integer (nullable = true)\n         |    |    |-- d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; flatten(df).printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.a: integer (nullable = true)\n         |-- s.b.c: integer (nullable = true)\n         |-- s.b.d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df = spark.createDataFrame(\n        ...         [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})],\n        ...         \"id INT, `s.s1` STRUCT&lt;`a.a1`:INT, `b.b1`:STRUCT&lt;`c.c1`:INT, `d.d1`:INT&gt;&gt;\"\n        ... )\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1: struct (nullable = true)\n         |    |-- a.a1: integer (nullable = true)\n         |    |-- b.b1: struct (nullable = true)\n         |    |    |-- c.c1: integer (nullable = true)\n         |    |    |-- d.d1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; flatten(df, \"?\").printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1?a.a1: integer (nullable = true)\n         |-- s.s1?b.b1?c.c1: integer (nullable = true)\n         |-- s.s1?b.b1?d.d1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    # The idea is to recursively write a \"SELECT s.b.c as `s.b.c`\" for each nested column.\n    cols = []\n\n    def expand_struct(struct: StructType, col_stack: List[str]) -&gt; None:\n        for field in struct:\n            if isinstance(field.dataType, StructType):\n                struct_field = field.dataType\n                expand_struct(struct_field, [*col_stack, field.name])\n            else:\n                column = f.col(\".\".join(quote_columns([*col_stack, field.name])))\n                cols.append(column.alias(struct_separator.join([*col_stack, field.name])))\n\n    expand_struct(df.schema, col_stack=[])\n    return df.select(cols)\n</code></pre> transformations.unflatten <p>Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>separator</code> <code>str</code> <p>A string used to separate the structs names from their elements.        It might be useful to change the separator when some DataFrame's column names already contain dots</p> <code>'.'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A flattened DataFrame</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\")\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.a: integer (nullable = true)\n |-- s.b.c: integer (nullable = true)\n |-- s.b.d: integer (nullable = true)\n\n&gt;&gt;&gt; unflatten(df).printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s: struct (nullable = true)\n |    |-- a: integer (nullable = true)\n |    |-- b: struct (nullable = true)\n |    |    |-- c: integer (nullable = true)\n |    |    |-- d: integer (nullable = true)\n\n&gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\")\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1?a.a1: integer (nullable = true)\n |-- s.s1?b.b1: integer (nullable = true)\n\n&gt;&gt;&gt; unflatten(df, \"?\").printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- s.s1: struct (nullable = true)\n |    |-- a.a1: integer (nullable = true)\n |    |-- b.b1: integer (nullable = true)\n</code></pre> Source code in <code>spark_frame/transformations_impl/unflatten.py</code> <pre><code>def unflatten(df: DataFrame, separator: str = \".\") -&gt; DataFrame:\n    \"\"\"Reverse of the flatten operation\n    Nested fields names will be separated from each other using the specified separator\n\n    Args:\n        df: A Spark DataFrame\n        separator: A string used to separate the structs names from their elements.\n                   It might be useful to change the separator when some DataFrame's column names already contain dots\n\n    Returns:\n        A flattened DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\")\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.a: integer (nullable = true)\n         |-- s.b.c: integer (nullable = true)\n         |-- s.b.d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; unflatten(df).printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s: struct (nullable = true)\n         |    |-- a: integer (nullable = true)\n         |    |-- b: struct (nullable = true)\n         |    |    |-- c: integer (nullable = true)\n         |    |    |-- d: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\")\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1?a.a1: integer (nullable = true)\n         |-- s.s1?b.b1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; unflatten(df, \"?\").printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- s.s1: struct (nullable = true)\n         |    |-- a.a1: integer (nullable = true)\n         |    |-- b.b1: integer (nullable = true)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    # The idea is to recursively write a \"SELECT struct(a, struct(s.b.c, s.b.d)) as s\" for each nested column.\n    # There is a little twist as we don't want to rebuild the struct if all its fields are NULL, so we add a CASE WHEN\n\n    def has_structs(df: DataFrame) -&gt; bool:\n        struct_fields = [field for field in df.schema if is_struct(field)]\n        return len(struct_fields) &gt; 0\n\n    if has_structs(df):\n        df = flatten(df)\n\n    tree = _build_nested_struct_tree(df.columns, separator)\n    cols = _build_struct_from_tree(tree, separator)\n    return df.select(cols)\n</code></pre>"},{"location":"use_cases/flatten_unflatten/#spark_frame.examples.flatten_unflatten.transform_nested_fields--without-spark-frame","title":"Without spark-frame","text":"<p>Of course, we could write something in DataFrame or SQL like this:</p> <pre><code>&gt;&gt;&gt; df.createOrReplaceTempView(\"df\")\n&gt;&gt;&gt; new_df = df.sparkSession.sql('''\n... SELECT\n...   STRUCT(\n...     base_stats.*,\n...     base_stats.Attack + base_stats.Defense + base_stats.HP +\n...     base_stats.`Sp Attack` + base_stats.`Sp Defense` + base_stats.Speed as Total\n...   ) as base_stats,\n...   id,\n...   name,\n...   types\n... FROM df\n... ''').show(vertical=True, truncate=False)\n-RECORD 0-----------------------------------\n base_stats | {49, 49, 45, 65, 65, 45, 318}\n id         | 1\n name       | {Bulbasaur, Bulbizarre}\n types      | [Grass, Poison]\n</code></pre> <p>It works, but it is a little cumbersome. Imagine how ugly the query would look like with a much bigger table, with hundreds of columns with three levels of nesting or more...</p>"},{"location":"use_cases/flatten_unflatten/#spark_frame.examples.flatten_unflatten.transform_nested_fields--with-spark-frame","title":"With spark-frame","text":"<p>Instead, we can use the <code>spark_frame.transformations.flatten</code> and unflatten <code>spark_frame.transformations.unflatten</code> methods to reduce boilerplate significantly.</p> <pre><code>&gt;&gt;&gt; from spark_frame.transformations import flatten, unflatten\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; flat_df = flatten(df)\n&gt;&gt;&gt; flat_df = flat_df.withColumn(\"base_stats.Total\",\n...     f.col(\"`base_stats.Attack`\") + f.col(\"`base_stats.Defense`\") + f.col(\"`base_stats.HP`\") +\n...     f.col(\"`base_stats.Sp Attack`\") + f.col(\"`base_stats.Sp Defense`\") + f.col(\"`base_stats.Speed`\")\n... )\n&gt;&gt;&gt; new_df = unflatten(flat_df)\n&gt;&gt;&gt; new_df.show(vertical=True, truncate=False)\n-RECORD 0-----------------------------------\n base_stats | {49, 49, 45, 65, 65, 45, 318}\n id         | 1\n name       | {Bulbasaur, Bulbizarre}\n types      | [Grass, Poison]\n</code></pre> <p>This yield the same result, and we did not have to mention the names of the columns we did not care about. This makes pipelines much easier to maintain. If a new column is added to your source table, you don't need to update this data enrichment code to propagate it automatically. On the other hand, with the first SQL solution, you would have had to specifically add this new field to the query to propagate it.</p> <p>We can even use DataFrame.transform to inline everything!</p> <pre><code>&gt;&gt;&gt; df.transform(flatten).withColumn(\n...     \"base_stats.Total\",\n...     f.col(\"`base_stats.Attack`\") + f.col(\"`base_stats.Defense`\") + f.col(\"`base_stats.HP`\") +\n...     f.col(\"`base_stats.Sp Attack`\") + f.col(\"`base_stats.Sp Defense`\") + f.col(\"`base_stats.Speed`\")\n...   ).transform(unflatten).show(vertical=True, truncate=False)\n-RECORD 0-----------------------------------\n base_stats | {49, 49, 45, 65, 65, 45, 318}\n id         | 1\n name       | {Bulbasaur, Bulbizarre}\n types      | [Grass, Poison]\n</code></pre> <p>Update: Since version 0.0.4, the same result can be achieved with an even simpler and more powerful transformation</p> <pre><code>&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- base_stats.Attack: long (nullable = true)\n |-- base_stats.Defense: long (nullable = true)\n |-- base_stats.HP: long (nullable = true)\n |-- base_stats.Sp Attack: long (nullable = true)\n |-- base_stats.Sp Defense: long (nullable = true)\n |-- base_stats.Speed: long (nullable = true)\n |-- id: long (nullable = true)\n |-- name.english: string (nullable = true)\n |-- name.french: string (nullable = true)\n |-- types!: string (nullable = true)\n\n&gt;&gt;&gt; df.transform(nested.with_fields, {\n...     \"base_stats.Total\":\n...     f.col(\"base_stats.Attack\") + f.col(\"base_stats.Defense\") + f.col(\"base_stats.HP\") +\n...     f.col(\"base_stats.`Sp Attack`\") + f.col(\"base_stats.`Sp Defense`\") + f.col(\"base_stats.Speed\")\n... }).show(vertical=True, truncate=False)\n-RECORD 0-----------------------------------\n base_stats | {49, 49, 45, 65, 65, 45, 318}\n id         | 1\n name       | {Bulbasaur, Bulbizarre}\n types      | [Grass, Poison]\n</code></pre> <p>Info</p> <p>This example uses data taken from https://raw.githubusercontent.com/fanzeyi/pokemon.json/master/pokedex.json.</p>"},{"location":"use_cases/intro/","title":"Intro","text":"<p>This section present several use cases to give ideas how each function may be used.</p> <p>Each time a method is used in a use cases, it's full documentation will be available in a collapsible section. For instance, if a use-case uses the method <code>spark_frame.functions.nullable</code> you will see this section at the end of the section:</p> nullable <p>You can also find a comprehensive list of all methods in the reference.</p>"},{"location":"use_cases/intro/#spark_frame.functions.nullable","title":"<code>nullable(col: Column) -&gt; Column</code>","text":"<p>Make a <code>pyspark.sql.Column</code> nullable. This is especially useful for literal which are always non-nullable by default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\"))\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- a: integer (nullable = false)\n |-- b: string (nullable = false)\n\n&gt;&gt;&gt; res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b')))\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- a: integer (nullable = true)\n |-- b: string (nullable = true)\n</code></pre> Source code in <code>spark_frame/functions.py</code> <pre><code>def nullable(col: Column) -&gt; Column:\n    \"\"\"Make a `pyspark.sql.Column` nullable.\n    This is especially useful for literal which are always non-nullable by default.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\"))\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- a: integer (nullable = false)\n         |-- b: string (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b')))\n        &gt;&gt;&gt; res.printSchema()\n        root\n         |-- a: integer (nullable = true)\n         |-- b: string (nullable = true)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    return f.when(~col.isNull(), col)\n</code></pre>"},{"location":"use_cases/working_with_json/","title":"Working with json","text":""},{"location":"use_cases/working_with_json/#extracting-json-values","title":"Extracting json values","text":"<p>Sometimes, a column in a data source contains raw json strings, and you want to extract this value before starting to understand it.</p> <p>This already happened to me in several cases, such as: - Some automatic data capture tool that wraps a payload's raw json value into an Avro file. - A microservice event that follows a data contract but one of the column contains the raw json payload that this   microservice exchanged with another external API.</p> <p>Let's take a sample DataFrame with two raw json columns.</p> <pre><code>&gt;&gt;&gt; from spark_frame.examples.working_with_json import _get_sample_data\n&gt;&gt;&gt; df = _get_sample_data()\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- call_id: integer (nullable = true)\n |-- raw_input: string (nullable = true)\n |-- raw_output: string (nullable = true)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+-------+-----------------------------------------------------------------------------+---------------------------------------------------------+\n|call_id|raw_input                                                                    |raw_output                                               |\n+-------+-----------------------------------------------------------------------------+---------------------------------------------------------+\n|1      |{\"model_name\": \"bot_detector\", \"model_version\": 3, \"model_args\": \"some data\"}|{\"model_score\": 0.94654, \"model_parameters\": \"some data\"}|\n|2      |{\"model_name\": \"cat_finder\", \"model_version\": 3, \"model_args\": \"some data\"}  |{\"model_score\": 0.4234, \"model_parameters\": \"some data\"} |\n+-------+-----------------------------------------------------------------------------+---------------------------------------------------------+\n</code></pre> <p>This DataFrame represents the logs of an application calling a machine learning model. Keeping the \"call_id\" is important to be able to link this call to other events that happen in the system, and we would like to analyze these logs with typed data.</p> <p>Methods used in this example</p> schema_utils.schema_from_simple_string <p>Parses the given data type string to a :class:<code>DataType</code>. The data type string format equals pyspark.sql.types.DataType.simpleString, except that the top level struct type can omit the <code>struct&lt;&gt;</code>. This method requires the SparkSession to have already been instantiated.</p> <p>Parameters:</p> Name Type Description Default <code>schema_string</code> <code>str</code> <p>A simpleString representing a DataFrame schema.</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>A DataType object representing the DataFrame schema.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If no SparkContext has been instantiated first.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; schema_from_simple_string(\"int \")\nIntegerType()\n&gt;&gt;&gt; schema_from_simple_string(\"INT \")\nIntegerType()\n&gt;&gt;&gt; schema_from_simple_string(\"a: byte, b: decimal(  16 , 8   ) \")\nStructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])\n&gt;&gt;&gt; schema_from_simple_string(\"a DOUBLE, b STRING\")\nStructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])\n&gt;&gt;&gt; schema_from_simple_string(\"a: array&lt; short&gt;\")\nStructType([StructField('a', ArrayType(ShortType(), True), True)])\n&gt;&gt;&gt; schema_from_simple_string(\" map&lt;string , string &gt; \")\nMapType(StringType(), StringType(), True)\n</code></pre> <p>Error cases:</p> <pre><code>&gt;&gt;&gt; schema_from_simple_string(\"blabla\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n&gt;&gt;&gt; schema_from_simple_string(\"a: int,\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n&gt;&gt;&gt; schema_from_simple_string(\"array&lt;int\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n&gt;&gt;&gt; schema_from_simple_string(\"map&lt;int, boolean&gt;&gt;\")\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.ParseException:...\n</code></pre> Source code in <code>spark_frame/schema_utils.py</code> <pre><code>def schema_from_simple_string(schema_string: str) -&gt; DataType:\n    \"\"\"Parses the given data type string to a :class:`DataType`. The data type string format equals\n    [pyspark.sql.types.DataType.simpleString][], except that the top level struct type can omit\n    the ``struct&lt;&gt;``.\n    This method requires the SparkSession to have already been instantiated.\n\n    Args:\n        schema_string: A simpleString representing a DataFrame schema.\n\n    Returns:\n        A DataType object representing the DataFrame schema.\n\n    Raises:\n        AssertionError: If no SparkContext has been instantiated first.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; schema_from_simple_string(\"int \")\n        IntegerType()\n        &gt;&gt;&gt; schema_from_simple_string(\"INT \")\n        IntegerType()\n        &gt;&gt;&gt; schema_from_simple_string(\"a: byte, b: decimal(  16 , 8   ) \")\n        StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])\n        &gt;&gt;&gt; schema_from_simple_string(\"a DOUBLE, b STRING\")\n        StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])\n        &gt;&gt;&gt; schema_from_simple_string(\"a: array&lt; short&gt;\")\n        StructType([StructField('a', ArrayType(ShortType(), True), True)])\n        &gt;&gt;&gt; schema_from_simple_string(\" map&lt;string , string &gt; \")\n        MapType(StringType(), StringType(), True)\n\n        **Error cases:**\n\n        &gt;&gt;&gt; schema_from_simple_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n        &gt;&gt;&gt; schema_from_simple_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n        &gt;&gt;&gt; schema_from_simple_string(\"array&lt;int\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n        &gt;&gt;&gt; schema_from_simple_string(\"map&lt;int, boolean&gt;&gt;\") # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        pyspark.sql.utils.ParseException:...\n\n    \"\"\"\n    sc = SparkContext._active_spark_context  # noqa: SLF001\n    assert_true(sc is not None, \"No SparkContext has been instantiated yet\")\n    return _parse_datatype_string(schema_string)\n</code></pre> spark_frame.transformations.parse_json_columns <p>Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information.</p> <p>This method is similar to Spark's <code>from_json</code> function, with one main difference: <code>from_json</code> requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column.</p> <p>By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names.</p> <p>Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism.</p> <p>WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \"<code>a.b.c</code>\") (See Example 2).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>columns</code> <code>Union[str, List[str], Dict[str, str]]</code> <p>A column name, list of column names, or dict(column_name, parsed_column_name)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame</p> <p>Example 1</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.createDataFrame([\n...         (1, '[{\"a\": 1}, {\"a\": 2}]'),\n...         (1, '[{\"a\": 2}, {\"a\": 4}]'),\n...         (2, None)\n...     ], \"id INT, json1 STRING\"\n... )\n&gt;&gt;&gt; df.show()\n+---+--------------------+\n| id|               json1|\n+---+--------------------+\n|  1|[{\"a\": 1}, {\"a\": 2}]|\n|  1|[{\"a\": 2}, {\"a\": 4}]|\n|  2|                NULL|\n+---+--------------------+\n\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- json1: string (nullable = true)\n\n&gt;&gt;&gt; parse_json_columns(df, 'json1').printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- json1: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: long (nullable = true)\n</code></pre> <p>Example 2 : different output column name</p> <pre><code>&gt;&gt;&gt; parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- json1: string (nullable = true)\n |-- parsed_json1: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: long (nullable = true)\n</code></pre> <p>Example 3 : json inside a struct</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([\n...         (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}),\n...         (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}),\n...         (2, None)\n...     ], \"id INT, struct STRUCT&lt;json1: STRING&gt;\"\n... )\n&gt;&gt;&gt; df.show(10, False)\n+---+----------------------+\n|id |struct                |\n+---+----------------------+\n|1  |{[{\"a\": 1}, {\"a\": 2}]}|\n|1  |{[{\"a\": 2}, {\"a\": 4}]}|\n|2  |NULL                  |\n+---+----------------------+\n\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- struct: struct (nullable = true)\n |    |-- json1: string (nullable = true)\n\n&gt;&gt;&gt; res = parse_json_columns(df, 'struct.json1')\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- id: integer (nullable = true)\n |-- struct: struct (nullable = true)\n |    |-- json1: string (nullable = true)\n |-- struct.json1: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: long (nullable = true)\n\n&gt;&gt;&gt; res.show(10, False)\n+---+----------------------+------------+\n|id |struct                |struct.json1|\n+---+----------------------+------------+\n|1  |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}]  |\n|1  |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}]  |\n|2  |NULL                  |NULL        |\n+---+----------------------+------------+\n</code></pre> Source code in <code>spark_frame/transformations_impl/parse_json_columns.py</code> <pre><code>def parse_json_columns(df: DataFrame, columns: Union[str, List[str], Dict[str, str]]) -&gt; DataFrame:\n    \"\"\"Transform the specified columns containing json strings in the given DataFrame into structs containing\n    the equivalent parsed information.\n\n    This method is similar to Spark's `from_json` function, with one main difference: `from_json` requires the user\n    to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically\n    the json schema of each column.\n\n    By default, the output columns will have the same name as the input columns, but if you want to keep the input\n    columns you can pass a dict(input_col_name, output_col_name) to specify different output column names.\n\n    Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful\n    for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema\n    of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism.\n\n    WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"),\n    instead of replacing that column it will create a new column outside the struct (e.g. \"`a.b.c`\") (See Example 2).\n\n    Args:\n        df: A Spark DataFrame\n        columns: A column name, list of column names, or dict(column_name, parsed_column_name)\n\n    Returns:\n        A new DataFrame\n\n    Examples: Example 1\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...         (1, '[{\"a\": 1}, {\"a\": 2}]'),\n        ...         (1, '[{\"a\": 2}, {\"a\": 4}]'),\n        ...         (2, None)\n        ...     ], \"id INT, json1 STRING\"\n        ... )\n        &gt;&gt;&gt; df.show()\n        +---+--------------------+\n        | id|               json1|\n        +---+--------------------+\n        |  1|[{\"a\": 1}, {\"a\": 2}]|\n        |  1|[{\"a\": 2}, {\"a\": 4}]|\n        |  2|                NULL|\n        +---+--------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- json1: string (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; parse_json_columns(df, 'json1').printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- json1: array (nullable = true)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- a: long (nullable = true)\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2 : different output column name\n        &gt;&gt;&gt; parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- json1: string (nullable = true)\n         |-- parsed_json1: array (nullable = true)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- a: long (nullable = true)\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 3 : json inside a struct\n        &gt;&gt;&gt; df = spark.createDataFrame([\n        ...         (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}),\n        ...         (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}),\n        ...         (2, None)\n        ...     ], \"id INT, struct STRUCT&lt;json1: STRING&gt;\"\n        ... )\n        &gt;&gt;&gt; df.show(10, False)\n        +---+----------------------+\n        |id |struct                |\n        +---+----------------------+\n        |1  |{[{\"a\": 1}, {\"a\": 2}]}|\n        |1  |{[{\"a\": 2}, {\"a\": 4}]}|\n        |2  |NULL                  |\n        +---+----------------------+\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- struct: struct (nullable = true)\n         |    |-- json1: string (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res = parse_json_columns(df, 'struct.json1')\n        &gt;&gt;&gt; res.printSchema()\n        root\n         |-- id: integer (nullable = true)\n         |-- struct: struct (nullable = true)\n         |    |-- json1: string (nullable = true)\n         |-- struct.json1: array (nullable = true)\n         |    |-- element: struct (containsNull = true)\n         |    |    |-- a: long (nullable = true)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; res.show(10, False)\n        +---+----------------------+------------+\n        |id |struct                |struct.json1|\n        +---+----------------------+------------+\n        |1  |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}]  |\n        |1  |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}]  |\n        |2  |NULL                  |NULL        |\n        +---+----------------------+------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    if isinstance(columns, str):\n        columns = [columns]\n    if isinstance(columns, list):\n        columns = {col: col for col in columns}\n\n    wrapped_df = __wrap_json_columns(df, columns)\n    schema_per_col = __infer_schema_per_column(wrapped_df, list(columns.values()))\n    res = __parse_json_columns(wrapped_df, schema_per_col)\n    return res\n</code></pre>"},{"location":"use_cases/working_with_json/#spark_frame.examples.working_with_json.extracting_json_values--without-spark-frame","title":"Without spark-frame","text":"<p>Spark does provide a <code>from_json</code> function that can parse a raw json column and convert it into a struct, but it does require the user to provide the schema of the json column in advance, like this:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; raw_input_schema = '{\"fields\":[{\"name\":\"model_name\",\"nullable\":true,\"type\":\"string\"},{\"name\":\"model_version\",\"nullable\":true,\"type\":\"integer\"},{\"name\":\"model_args\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'\n&gt;&gt;&gt; raw_output_schema = '{\"fields\":[{\"name\":\"model_score\",\"nullable\":true,\"type\":\"double\"},{\"name\":\"model_parameters\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}'\n&gt;&gt;&gt; df.withColumn(\n...     \"raw_input\", f.from_json(\"raw_input\", raw_input_schema)\n... ).withColumn(\n...     \"raw_output\", f.from_json(\"raw_output\", raw_output_schema)\n... ).show(truncate=False)\n+-------+----------------------------+--------------------+\n|call_id|raw_input                   |raw_output          |\n+-------+----------------------------+--------------------+\n|1      |{bot_detector, 3, some data}|{0.94654, some data}|\n|2      |{cat_finder, 3, some data}  |{0.4234, some data} |\n+-------+----------------------------+--------------------+\n</code></pre>"},{"location":"use_cases/working_with_json/#spark_frame.examples.working_with_json.extracting_json_values--with-spark-frame","title":"With spark-frame","text":"<p>While it does works, as you can see writing the schema can be quite heavy. Also, for some reason, <code>from_json</code> does not accept the \"simpleString\" format, unlike the <code>SparkSession.createDataFrame</code> method. The first thing we can do to make things simpler is by using the method [<code>spark_frame.schema_utils.schema_from_simple_string</code>][] like this :</p> <pre><code>&gt;&gt;&gt; from spark_frame.schema_utils import schema_from_simple_string\n&gt;&gt;&gt; raw_input_schema = schema_from_simple_string(\"model_name: STRING, model_version: INT, model_args: STRING\")\n&gt;&gt;&gt; raw_output_schema = schema_from_simple_string(\"model_score: DOUBLE, model_parameters: STRING\")\n&gt;&gt;&gt; df.withColumn(\n...     \"raw_input\", f.from_json(\"raw_input\", raw_input_schema)\n... ).withColumn(\n...     \"raw_output\", f.from_json(\"raw_output\", raw_output_schema)\n... ).show(truncate=False)\n+-------+----------------------------+--------------------+\n|call_id|raw_input                   |raw_output          |\n+-------+----------------------------+--------------------+\n|1      |{bot_detector, 3, some data}|{0.94654, some data}|\n|2      |{cat_finder, 3, some data}  |{0.4234, some data} |\n+-------+----------------------------+--------------------+\n</code></pre> <p>But if we don't know the schema or if we know that the schema may evolve and we want to add (or at least, detect) the new fields automatically, we can leverage Spark's automatic json schema inference by using the method [<code>spark_frame.transformations.parse_json_columns</code>] [<code>spark_frame.transformations_impl.parse_json_columns.parse_json_columns</code>] to infer automatically the schema of these json columns.</p> <pre><code>&gt;&gt;&gt; from spark_frame.transformations import parse_json_columns\n&gt;&gt;&gt; res = parse_json_columns(df, [\"raw_input\", \"raw_output\"])\n&gt;&gt;&gt; res.show(truncate=False)\n+-------+----------------------------+--------------------+\n|call_id|raw_input                   |raw_output          |\n+-------+----------------------------+--------------------+\n|1      |{some data, bot_detector, 3}|{some data, 0.94654}|\n|2      |{some data, cat_finder, 3}  |{some data, 0.4234} |\n+-------+----------------------------+--------------------+\n\n&gt;&gt;&gt; res.printSchema()\nroot\n |-- call_id: integer (nullable = true)\n |-- raw_input: struct (nullable = true)\n |    |-- model_args: string (nullable = true)\n |    |-- model_name: string (nullable = true)\n |    |-- model_version: long (nullable = true)\n |-- raw_output: struct (nullable = true)\n |    |-- model_parameters: string (nullable = true)\n |    |-- model_score: double (nullable = true)\n</code></pre> <p>As we can see, the order of the field is different, this is because Spark's automatic inference will always sort the json field by names.</p>"},{"location":"use_cases/working_with_nested_data/","title":"Working with nested data","text":""},{"location":"use_cases/working_with_nested_data/#transforming-nested-fields","title":"Transforming nested fields","text":"<p>Let's take a sample DataFrame with a deeply nested schema</p> <pre><code>&gt;&gt;&gt; from spark_frame.examples.working_with_nested_data import _get_sample_employee_data\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; df = _get_sample_employee_data()\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- level: string (nullable = true)\n |-- projects: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- client: string (nullable = true)\n |    |    |-- tasks: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- status: string (nullable = true)\n |    |    |    |    |-- estimate: long (nullable = true)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|employee_id|name      |age|skills                                       |projects                                                                                                                                                                                                                          |\n+-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|1          |John Smith|30 |[{Java, expert}, {Python, intermediate}]     |[{Project A, Acme Inc, [{Task 1, Implement feature X, completed, 8}, {Task 2, Fix bug Y, in progress, 5}]}, {Project B, Beta Corp, [{Task 3, Implement feature Z, pending, 13}, {Task 4, Improve performance, in progress, 3}]}]  |\n|2          |Jane Doe  |25 |[{JavaScript, advanced}, {PHP, intermediate}]|[{Project C, Gamma Inc, [{Task 5, Implement feature W, completed, 20}, {Task 6, Fix bug V, in progress, 13}]}, {Project D, Delta Ltd, [{Task 7, Implement feature U, pending, 8}, {Task 8, Improve performance, in progress, 5}]}]|\n+-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>As we can see, the schema has two top-level columns of type ARRAY (<code>skills</code> and <code>projects</code>), and the <code>projects</code> array contains a second level of repetition <code>projects.tasks</code>.</p> <p>Manipulating this DataFrame with Spark can quickly become really painful, and it is still quite simple compare to what engineers may encounter while working with entreprise-grade datasets.</p> <p>Methods used in this example</p> nested.print_schema <p>Print the DataFrame's flattened schema to the standard output.</p> <ul> <li>Structs are flattened with a <code>.</code> after their name.</li> <li>Arrays are flattened with a <code>!</code> character after their name.</li> <li>Maps are flattened with a <code>%key</code> and '%value' after their name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     1 as id,\n...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n...     STRUCT(7 as f) as s2,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4,\n...     MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1\n... ''')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s1: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- a: integer (nullable = false)\n |    |    |-- b: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- c: integer (nullable = false)\n |    |    |    |    |-- d: integer (nullable = false)\n |    |    |-- e: array (nullable = false)\n |    |    |    |-- element: integer (containsNull = false)\n |-- s2: struct (nullable = false)\n |    |-- f: integer (nullable = false)\n |-- s3: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: integer (containsNull = false)\n |-- s4: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- e: integer (nullable = false)\n |    |    |    |-- f: integer (nullable = false)\n |-- m1: map (nullable = false)\n |    |-- key: struct\n |    |    |-- a: integer (nullable = false)\n |    |-- value: struct (valueContainsNull = false)\n |    |    |-- b: integer (nullable = false)\n\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s1!.b!.c: integer (nullable = false)\n |-- s1!.b!.d: integer (nullable = false)\n |-- s1!.e!: integer (nullable = false)\n |-- s2.f: integer (nullable = false)\n |-- s3!!: integer (nullable = false)\n |-- s4!!.e: integer (nullable = false)\n |-- s4!!.f: integer (nullable = false)\n |-- m1%key.a: integer (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n</code></pre> Source code in <code>spark_frame/nested_impl/print_schema.py</code> <pre><code>def print_schema(df: DataFrame) -&gt; None:\n    \"\"\"Print the DataFrame's flattened schema to the standard output.\n\n    - Structs are flattened with a `.` after their name.\n    - Arrays are flattened with a `!` character after their name.\n    - Maps are flattened with a `%key` and '%value' after their name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     1 as id,\n        ...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n        ...     STRUCT(7 as f) as s2,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n        ...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4,\n        ...     MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1\n        ... ''')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s1: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- a: integer (nullable = false)\n         |    |    |-- b: array (nullable = false)\n         |    |    |    |-- element: struct (containsNull = false)\n         |    |    |    |    |-- c: integer (nullable = false)\n         |    |    |    |    |-- d: integer (nullable = false)\n         |    |    |-- e: array (nullable = false)\n         |    |    |    |-- element: integer (containsNull = false)\n         |-- s2: struct (nullable = false)\n         |    |-- f: integer (nullable = false)\n         |-- s3: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: integer (containsNull = false)\n         |-- s4: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: struct (containsNull = false)\n         |    |    |    |-- e: integer (nullable = false)\n         |    |    |    |-- f: integer (nullable = false)\n         |-- m1: map (nullable = false)\n         |    |-- key: struct\n         |    |    |-- a: integer (nullable = false)\n         |    |-- value: struct (valueContainsNull = false)\n         |    |    |-- b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s1!.b!.c: integer (nullable = false)\n         |-- s1!.b!.d: integer (nullable = false)\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2.f: integer (nullable = false)\n         |-- s3!!: integer (nullable = false)\n         |-- s4!!.e: integer (nullable = false)\n         |-- s4!!.f: integer (nullable = false)\n         |-- m1%key.a: integer (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    print(schema_string(df))\n</code></pre> nested.with_fields <p>Return a new DataFrame by adding or replacing (when they already exist) columns.</p> <p>This method is similar to the DataFrame.withColumn method, with the extra capability of working on nested and repeated fields (structs and arrays).</p> <p>The syntax for field names works as follows:</p> <ul> <li>\".\" is the separator for struct elements</li> <li>\"!\" must be appended at the end of fields that are repeated (arrays)</li> <li>Map keys are appended with <code>%key</code></li> <li>Map values are appended with <code>%value</code></li> </ul> <p>The following types of transformation are allowed:</p> <ul> <li>String and column expressions can be used on any non-repeated field, even nested ones.</li> <li>When working on repeated fields, transformations must be expressed as higher order functions   (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,   but their value will be repeated multiple times.</li> <li>When working on multiple levels of nested arrays, higher order functions may take multiple arguments,   corresponding to each level of repetition (See Example 5.).</li> <li><code>None</code> can also be used to represent the identity transformation, this is useful to select a field without    changing and without having to repeat its name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>fields</code> <code>Mapping[str, AnyKindOfTransformation]</code> <p>A Dict(field_name, transformation_to_apply)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been</p> <code>DataFrame</code> <p>applied to the corresponding fields. If a field name did not exist in the input DataFrame,</p> <code>DataFrame</code> <p>it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one.</p> <p>Example 1: non-repeated fields</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s.a: integer (nullable = false)\n |-- s.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+------+\n| id|     s|\n+---+------+\n|  1|{2, 3}|\n+---+------+\n</code></pre> <p>Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression.</p> <pre><code>&gt;&gt;&gt; new_df = nested.with_fields(df, {\n...     \"s.id\": \"id\",                                 # column name (string)\n...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")            # Column expression\n... })\n&gt;&gt;&gt; new_df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s: struct (nullable = false)\n |    |-- a: integer (nullable = false)\n |    |-- b: integer (nullable = false)\n |    |-- id: integer (nullable = false)\n |    |-- c: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---+------------+\n| id|           s|\n+---+------------+\n|  1|{2, 3, 1, 5}|\n+---+------------+\n</code></pre> <p>Example 2: repeated fields</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.a: integer (nullable = false)\n |-- s!.b.c: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+--------------------+\n| id|                   s|\n+---+--------------------+\n|  1|[{1, {2}}, {3, {4}}]|\n+---+--------------------+\n</code></pre> <p>Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...     \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]}\n... )\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.a: integer (nullable = false)\n |-- s!.b.c: integer (nullable = false)\n |-- s!.b.d: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+--------------------------+\n|id |s                         |\n+---+--------------------------+\n|1  |[{1, {2, 3}}, {3, {4, 7}}]|\n+---+--------------------------+\n</code></pre> <p>String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.with_fields, {\n...     \"id\": None,\n...     \"s!.a\": \"id\",\n...     \"s!.b.c\": f.lit(2)\n... }).show(truncate=False)\n+---+--------------------+\n|id |s                   |\n+---+--------------------+\n|1  |[{1, {2}}, {1, {2}}]|\n+---+--------------------+\n</code></pre> <p>Example 3: field repeated twice</p> <pre><code>&gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.e!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+\n| id|            s|\n+---+-------------+\n|  1|[{[1, 2, 3]}]|\n+---+-------------+\n</code></pre> <p>Here, the lambda expression will be applied to the last repeated element <code>e</code>.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show()\n+---+-------------------+\n| id|                  s|\n+---+-------------------+\n|  1|[{[1.0, 2.0, 3.0]}]|\n+---+-------------------+\n</code></pre> <p>Example 4: Dataframe with maps</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: integer (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+\n| id|           m1|\n+---+-------------+\n|  1|{a -&gt; {2, 3}}|\n+---+-------------+\n</code></pre> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...  \"m1%key\": lambda key : f.upper(key),\n...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: double (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---+---------------+\n| id|             m1|\n+---+---------------+\n|  1|{A -&gt; {2.0, 3}}|\n+---+---------------+\n</code></pre> <p>Example 5: Accessing multiple repetition levels</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(\n...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n...         ) as s1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.average: integer (nullable = false)\n |-- s1!.values!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+---+--------------------------------------+\n|id |s1                                    |\n+---+--------------------------------------+\n|1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n+---+--------------------------------------+\n</code></pre> <p>Here, the transformation applied to \"s1!.values!\" takes two arguments.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+-----------------------------------------+\n|id |s1                                       |\n+---+-----------------------------------------+\n|1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n+---+-----------------------------------------+\n</code></pre> <p>Extra arguments can be added to the left for each repetition level, up to the root level.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+---------------------------------------+\n|id |s1                                     |\n+---+---------------------------------------+\n|1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n+---+---------------------------------------+\n</code></pre> Source code in <code>spark_frame/nested_impl/with_fields.py</code> <pre><code>def with_fields(df: DataFrame, fields: Mapping[str, AnyKindOfTransformation]) -&gt; DataFrame:\n    \"\"\"Return a new [DataFrame][pyspark.sql.DataFrame] by adding or replacing (when they already exist) columns.\n\n    This method is similar to the [DataFrame.withColumn][pyspark.sql.DataFrame.withColumn] method, with the extra\n    capability of working on nested and repeated fields (structs and arrays).\n\n    The syntax for field names works as follows:\n\n    - \".\" is the separator for struct elements\n    - \"!\" must be appended at the end of fields that are repeated (arrays)\n    - Map keys are appended with `%key`\n    - Map values are appended with `%value`\n\n    The following types of transformation are allowed:\n\n    - String and column expressions can be used on any non-repeated field, even nested ones.\n    - When working on repeated fields, transformations must be expressed as higher order functions\n      (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,\n      but their value will be repeated multiple times.\n    - When working on multiple levels of nested arrays, higher order functions may take multiple arguments,\n      corresponding to each level of repetition (See Example 5.).\n    - `None` can also be used to represent the identity transformation, this is useful to select a field without\n       changing and without having to repeat its name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n        fields: A Dict(field_name, transformation_to_apply)\n\n    Returns:\n        A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been\n        applied to the corresponding fields. If a field name did not exist in the input DataFrame,\n        it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one.\n\n    Examples: Example 1: non-repeated fields\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s.a: integer (nullable = false)\n         |-- s.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+------+\n        | id|     s|\n        +---+------+\n        |  1|{2, 3}|\n        +---+------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on non-repeated fields may be expressed as a string representing a column name\n        or a Column expression.\n        &gt;&gt;&gt; new_df = nested.with_fields(df, {\n        ...     \"s.id\": \"id\",                                 # column name (string)\n        ...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")            # Column expression\n        ... })\n        &gt;&gt;&gt; new_df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s: struct (nullable = false)\n         |    |-- a: integer (nullable = false)\n         |    |-- b: integer (nullable = false)\n         |    |-- id: integer (nullable = false)\n         |    |-- c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---+------------+\n        | id|           s|\n        +---+------------+\n        |  1|{2, 3, 1, 5}|\n        +---+------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2: repeated fields\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.a: integer (nullable = false)\n         |-- s!.b.c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+--------------------+\n        | id|                   s|\n        +---+--------------------+\n        |  1|[{1, {2}}, {3, {4}}]|\n        +---+--------------------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on repeated fields must be expressed as\n        higher-order functions (lambda expressions or named functions).\n        The value passed to this function will correspond to the last repeated element.\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...     \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]}\n        ... )\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.a: integer (nullable = false)\n         |-- s!.b.c: integer (nullable = false)\n         |-- s!.b.d: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+--------------------------+\n        |id |s                         |\n        +---+--------------------------+\n        |1  |[{1, {2, 3}}, {3, {4, 7}}]|\n        +---+--------------------------+\n        &lt;BLANKLINE&gt;\n\n        String and column expressions can be used on repeated fields as well,\n        but their value will be repeated multiple times.\n        &gt;&gt;&gt; df.transform(nested.with_fields, {\n        ...     \"id\": None,\n        ...     \"s!.a\": \"id\",\n        ...     \"s!.b.c\": f.lit(2)\n        ... }).show(truncate=False)\n        +---+--------------------+\n        |id |s                   |\n        +---+--------------------+\n        |1  |[{1, {2}}, {1, {2}}]|\n        +---+--------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 3: field repeated twice\n        &gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.e!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+\n        | id|            s|\n        +---+-------------+\n        |  1|[{[1, 2, 3]}]|\n        +---+-------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the lambda expression will be applied to the last repeated element `e`.\n        &gt;&gt;&gt; df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show()\n        +---+-------------------+\n        | id|                  s|\n        +---+-------------------+\n        |  1|[{[1.0, 2.0, 3.0]}]|\n        +---+-------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 4: Dataframe with maps\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: integer (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+\n        | id|           m1|\n        +---+-------------+\n        |  1|{a -&gt; {2, 3}}|\n        +---+-------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...  \"m1%key\": lambda key : f.upper(key),\n        ...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n        ... })\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: double (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---+---------------+\n        | id|             m1|\n        +---+---------------+\n        |  1|{A -&gt; {2.0, 3}}|\n        +---+---------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 5: Accessing multiple repetition levels\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(\n        ...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n        ...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n        ...         ) as s1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.average: integer (nullable = false)\n         |-- s1!.values!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+--------------------------------------+\n        |id |s1                                    |\n        +---+--------------------------------------+\n        |1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n        +---+--------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the transformation applied to \"s1!.values!\" takes two arguments.\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+-----------------------------------------+\n        |id |s1                                       |\n        +---+-----------------------------------------+\n        |1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n        +---+-----------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Extra arguments can be added to the left for each repetition level, up to the root level.\n        &gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n        ...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+---------------------------------------+\n        |id |s1                                     |\n        +---+---------------------------------------+\n        |1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n        +---+---------------------------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    default_columns = {field: None for field in nested.fields(df)}\n    fields = {**default_columns, **fields}\n    return df.select(*resolve_nested_fields(fields, starting_level=df))\n</code></pre>"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.transform_nested_fields--the-task","title":"The task","text":"<p>Let's say we want to enrich this DataFrame by performing the following changes:</p> <ul> <li>Change the <code>skills.level</code> to uppercase</li> <li>Cast the <code>projects.tasks.estimate</code> to double</li> </ul>"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.transform_nested_fields--without-spark_framenested","title":"Without spark_frame.nested","text":"<p>Prior to Spark 3.0, we would have had only two choices:</p> <ol> <li>Flatten <code>skills</code> and <code>projects.tasks</code> into two separate DataFrames, perform the transformation then join the    two DataFrames back together.</li> <li>Write a custom Python UDF to perform the changes.</li> </ol> <p>Option 1. is not a good solution, as it would be quite costly, and require several shuffle operations.</p> <p>Option 2. is not great either, as the Python UDF would be slow and not very reusable. Had we been using Java or Scala, this might have been a better option already, as we would not incure the performance costs associated with Python UDFs, but this would still have required a lot of work to code the whole Employee data structure in Java/Scala before being able to manipulate it.</p> <p>Since Spark 3.1.0, a third option is available, which consists in using pyspark.sql.functions.transform and pyspark.sql.Column.withField to achieve our goal.</p> <p>However, the code that we need to write is quite complex:</p> <pre><code>&gt;&gt;&gt; new_df = df.withColumn(\n...     \"skills\",\n...     f.transform(f.col(\"skills\"), lambda skill: skill.withField(\"level\", f.upper(skill[\"level\"])))\n... ).withColumn(\n...     \"projects\",\n...     f.transform(\n...         f.col(\"projects\"),\n...         lambda project: project.withField(\n...             \"tasks\",\n...             f.transform(\n...                 project[\"tasks\"],\n...                 lambda task: task.withField(\"estimate\", task[\"estimate\"].cast(\"DOUBLE\"))),\n...         ),\n...     ),\n... )\n&gt;&gt;&gt; new_df.printSchema()\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- level: string (nullable = true)\n |-- projects: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- client: string (nullable = true)\n |    |    |-- tasks: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- name: string (nullable = true)\n |    |    |    |    |-- description: string (nullable = true)\n |    |    |    |    |-- status: string (nullable = true)\n |    |    |    |    |-- estimate: double (nullable = true)\n\n&gt;&gt;&gt; new_df.select(\"employee_id\", \"name\", \"age\", \"skills\").show(truncate=False)\n+-----------+----------+---+---------------------------------------------+\n|employee_id|name      |age|skills                                       |\n+-----------+----------+---+---------------------------------------------+\n|1          |John Smith|30 |[{Java, EXPERT}, {Python, INTERMEDIATE}]     |\n|2          |Jane Doe  |25 |[{JavaScript, ADVANCED}, {PHP, INTERMEDIATE}]|\n+-----------+----------+---+---------------------------------------------+\n</code></pre> <p>As we can see, the transformation worked: the schema is the same except <code>projects.tasks.estimate</code> which is now a <code>double</code>, and <code>skills.name</code> is now in uppercase. But hopefully we can agree that the code to achieve this looks quite complex, and that it's complexity would grow even more if we tried to perform more transformations at the same time.</p>"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.transform_nested_fields--with-spark_framenested","title":"With spark_frame.nested","text":"<p>The module <code>spark_frame.nested</code> proposes several methods to help us deal with nested data structure more easily. First, let's use <code>spark_frame.nested.print_schema</code> to get a flat version of the DataFrame's schema:</p> <pre><code>&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- skills!.name: string (nullable = true)\n |-- skills!.level: string (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.client: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n |-- projects!.tasks!.description: string (nullable = true)\n |-- projects!.tasks!.status: string (nullable = true)\n |-- projects!.tasks!.estimate: long (nullable = true)\n</code></pre> <p>As we can see, this is the same schema as before, but instead of being displayed as a tree, it is displayed as a flat list where each field is represented with its full name. We can also see that fields of type ARRAY can be easily identified thanks to the exclamation marks (<code>!</code>) added after their names. Once you get used to it, this flat representation is more compact and easier to read than the tree representation, while conveying the same amount of information.</p> <p>This notation will also help us performing the target transformations more easily. As a reminder, we want to:</p> <ul> <li>Change the <code>skills.level</code> to uppercase</li> <li>Cast the <code>projects.tasks.estimate</code> to double</li> </ul> <p>Using the <code>spark_frame.nested.with_fields</code> method, this can be done like this:</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.with_fields, {\n...     \"skills!.level\": lambda skill: f.upper(skill[\"level\"]),\n...     \"projects!.tasks!.estimate\": lambda task: task[\"estimate\"].cast(\"DOUBLE\")\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- skills!.name: string (nullable = true)\n |-- skills!.level: string (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.client: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n |-- projects!.tasks!.description: string (nullable = true)\n |-- projects!.tasks!.status: string (nullable = true)\n |-- projects!.tasks!.estimate: double (nullable = true)\n\n&gt;&gt;&gt; new_df.select(\"employee_id\", \"name\", \"age\", \"skills\").show(truncate=False)\n+-----------+----------+---+---------------------------------------------+\n|employee_id|name      |age|skills                                       |\n+-----------+----------+---+---------------------------------------------+\n|1          |John Smith|30 |[{Java, EXPERT}, {Python, INTERMEDIATE}]     |\n|2          |Jane Doe  |25 |[{JavaScript, ADVANCED}, {PHP, INTERMEDIATE}]|\n+-----------+----------+---+---------------------------------------------+\n</code></pre> <p>As we can see, we obtained the same result with a much simpler and cleaner code. Now let's explain what this code did:</p> <p>The <code>spark_frame.nested.with_fields</code> method is similar to the [<code>pyspark.sql.DataFrame.withColumns</code>][] method, except that it works on nested fields inside structs and arrays. We pass it a <code>Dict(field_name, transformation)</code> indicating the expression we want to apply for each field. The transformation must be a higher order function: a lambda expression or named function that takes a Column as argument and returns a Column. The column passed to that function will represent the struct parent of the target field. For instance, when we write <code>\"skills!.level\": lambda skill: f.upper(skill[\"level\"])</code>, the lambda function will be applied to each struct element of the array <code>skills</code>.</p> <p>Info</p> <p>The data for this example was generated by ChatGPT :-)</p>"},{"location":"use_cases/working_with_nested_data/#selecting-nested-fields","title":"Selecting nested fields","text":"<p>In this example, we will see how to select and rename specific elements in a nested data structure</p> <pre><code>&gt;&gt;&gt; from spark_frame.examples.working_with_nested_data import _get_sample_employee_data\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; df = _get_sample_employee_data()\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- skills!.name: string (nullable = true)\n |-- skills!.level: string (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.client: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n |-- projects!.tasks!.description: string (nullable = true)\n |-- projects!.tasks!.status: string (nullable = true)\n |-- projects!.tasks!.estimate: long (nullable = true)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|employee_id|name      |age|skills                                       |projects                                                                                                                                                                                                                          |\n+-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|1          |John Smith|30 |[{Java, expert}, {Python, intermediate}]     |[{Project A, Acme Inc, [{Task 1, Implement feature X, completed, 8}, {Task 2, Fix bug Y, in progress, 5}]}, {Project B, Beta Corp, [{Task 3, Implement feature Z, pending, 13}, {Task 4, Improve performance, in progress, 3}]}]  |\n|2          |Jane Doe  |25 |[{JavaScript, advanced}, {PHP, intermediate}]|[{Project C, Gamma Inc, [{Task 5, Implement feature W, completed, 20}, {Task 6, Fix bug V, in progress, 13}]}, {Project D, Delta Ltd, [{Task 7, Implement feature U, pending, 8}, {Task 8, Improve performance, in progress, 5}]}]|\n+-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> nested.print_schema <p>Print the DataFrame's flattened schema to the standard output.</p> <ul> <li>Structs are flattened with a <code>.</code> after their name.</li> <li>Arrays are flattened with a <code>!</code> character after their name.</li> <li>Maps are flattened with a <code>%key</code> and '%value' after their name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT\n...     1 as id,\n...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n...     STRUCT(7 as f) as s2,\n...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4,\n...     MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1\n... ''')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s1: array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- a: integer (nullable = false)\n |    |    |-- b: array (nullable = false)\n |    |    |    |-- element: struct (containsNull = false)\n |    |    |    |    |-- c: integer (nullable = false)\n |    |    |    |    |-- d: integer (nullable = false)\n |    |    |-- e: array (nullable = false)\n |    |    |    |-- element: integer (containsNull = false)\n |-- s2: struct (nullable = false)\n |    |-- f: integer (nullable = false)\n |-- s3: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: integer (containsNull = false)\n |-- s4: array (nullable = false)\n |    |-- element: array (containsNull = false)\n |    |    |-- element: struct (containsNull = false)\n |    |    |    |-- e: integer (nullable = false)\n |    |    |    |-- f: integer (nullable = false)\n |-- m1: map (nullable = false)\n |    |-- key: struct\n |    |    |-- a: integer (nullable = false)\n |    |-- value: struct (valueContainsNull = false)\n |    |    |-- b: integer (nullable = false)\n\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.a: integer (nullable = false)\n |-- s1!.b!.c: integer (nullable = false)\n |-- s1!.b!.d: integer (nullable = false)\n |-- s1!.e!: integer (nullable = false)\n |-- s2.f: integer (nullable = false)\n |-- s3!!: integer (nullable = false)\n |-- s4!!.e: integer (nullable = false)\n |-- s4!!.f: integer (nullable = false)\n |-- m1%key.a: integer (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n</code></pre> Source code in <code>spark_frame/nested_impl/print_schema.py</code> <pre><code>def print_schema(df: DataFrame) -&gt; None:\n    \"\"\"Print the DataFrame's flattened schema to the standard output.\n\n    - Structs are flattened with a `.` after their name.\n    - Arrays are flattened with a `!` character after their name.\n    - Maps are flattened with a `%key` and '%value' after their name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT\n        ...     1 as id,\n        ...     ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1,\n        ...     STRUCT(7 as f) as s2,\n        ...     ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3,\n        ...     ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4,\n        ...     MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1\n        ... ''')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s1: array (nullable = false)\n         |    |-- element: struct (containsNull = false)\n         |    |    |-- a: integer (nullable = false)\n         |    |    |-- b: array (nullable = false)\n         |    |    |    |-- element: struct (containsNull = false)\n         |    |    |    |    |-- c: integer (nullable = false)\n         |    |    |    |    |-- d: integer (nullable = false)\n         |    |    |-- e: array (nullable = false)\n         |    |    |    |-- element: integer (containsNull = false)\n         |-- s2: struct (nullable = false)\n         |    |-- f: integer (nullable = false)\n         |-- s3: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: integer (containsNull = false)\n         |-- s4: array (nullable = false)\n         |    |-- element: array (containsNull = false)\n         |    |    |-- element: struct (containsNull = false)\n         |    |    |    |-- e: integer (nullable = false)\n         |    |    |    |-- f: integer (nullable = false)\n         |-- m1: map (nullable = false)\n         |    |-- key: struct\n         |    |    |-- a: integer (nullable = false)\n         |    |-- value: struct (valueContainsNull = false)\n         |    |    |-- b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.a: integer (nullable = false)\n         |-- s1!.b!.c: integer (nullable = false)\n         |-- s1!.b!.d: integer (nullable = false)\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2.f: integer (nullable = false)\n         |-- s3!!: integer (nullable = false)\n         |-- s4!!.e: integer (nullable = false)\n         |-- s4!!.f: integer (nullable = false)\n         |-- m1%key.a: integer (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n    \"\"\"\n    print(schema_string(df))\n</code></pre> nested.select <p>Project a set of expressions and returns a new DataFrame.</p> <p>This method is similar to the DataFrame.select method, with the extra capability of working on nested and repeated fields (structs and arrays).</p> <p>The syntax for field names works as follows:</p> <ul> <li>\".\" is the separator for struct elements</li> <li>\"!\" must be appended at the end of fields that are repeated (arrays)</li> <li>Map keys are appended with <code>%key</code></li> <li>Map values are appended with <code>%value</code></li> </ul> <p>The following types of transformation are allowed:</p> <ul> <li>String and column expressions can be used on any non-repeated field, even nested ones.</li> <li>When working on repeated fields, transformations must be expressed as higher order functions   (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,   but their value will be repeated multiple times.</li> <li>When working on multiple levels of nested arrays, higher order functions may take multiple arguments,   corresponding to each level of repetition (See Example 5.).</li> <li><code>None</code> can also be used to represent the identity transformation, this is useful to select a field without    changing and without having to repeat its name.</li> </ul> <p>Limitation: Dots, percents, and exclamation marks are not supported in field names</p> <p>Given the syntax used, every method defined in the <code>spark_frame.nested</code> module assumes that all field names in DataFrames do not contain any dot <code>.</code>, percent <code>%</code> or exclamation mark <code>!</code>. This can be worked around using the transformation <code>spark_frame.transformations.transform_all_field_names</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A Spark DataFrame</p> required <code>fields</code> <code>Mapping[str, ColumnTransformation]</code> <p>A Dict(field_name, transformation_to_apply)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame where only the specified field have been selected and the corresponding</p> <code>DataFrame</code> <p>transformations were applied to each of them.</p> <p>Example 1: non-repeated fields</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; from pyspark.sql import functions as f\n&gt;&gt;&gt; from spark_frame import nested\n&gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n&gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n&gt;&gt;&gt; df.printSchema()\nroot\n |-- id: integer (nullable = false)\n |-- s: struct (nullable = false)\n |    |-- a: integer (nullable = false)\n |    |-- b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+------+\n| id|     s|\n+---+------+\n|  1|{2, 3}|\n+---+------+\n</code></pre> <p>Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected)</p> <pre><code>&gt;&gt;&gt; new_df = nested.select(df, {\n...     \"s.a\": \"s.a\",                        # Column name (string)\n...     \"s.b\": None,                         # None: use to keep a column without having to repeat its name\n...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")   # Column expression\n... })\n&gt;&gt;&gt; new_df.printSchema()\nroot\n |-- s: struct (nullable = false)\n |    |-- a: integer (nullable = false)\n |    |-- b: integer (nullable = false)\n |    |-- c: integer (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---------+\n|        s|\n+---------+\n|{2, 3, 5}|\n+---------+\n</code></pre> <p>Example 2: repeated fields</p> <pre><code>&gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s!.a: integer (nullable = false)\n |-- s!.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+----------------+\n| id|               s|\n+---+----------------+\n|  1|[{1, 2}, {3, 4}]|\n+---+----------------+\n</code></pre> <p>Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.select, {\n...     \"s!.a\": lambda s: s[\"a\"],\n...     \"s!.b\": None,\n...     \"s!.c\": lambda s: s[\"a\"] + s[\"b\"]\n... }).show(truncate=False)\n+----------------------+\n|s                     |\n+----------------------+\n|[{1, 2, 3}, {3, 4, 7}]|\n+----------------------+\n</code></pre> <p>String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times.</p> <pre><code>&gt;&gt;&gt; df.transform(nested.select, {\n...     \"id\": None,\n...     \"s!.a\": \"id\",\n...     \"s!.b\": f.lit(2)\n... }).show(truncate=False)\n+---+----------------+\n|id |s               |\n+---+----------------+\n|1  |[{1, 2}, {1, 2}]|\n+---+----------------+\n</code></pre> <p>Example 3: field repeated twice</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1,\n...         ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.e!: integer (nullable = false)\n |-- s2!.e!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+-------------+\n| id|           s1|           s2|\n+---+-------------+-------------+\n|  1|[{[1, 2, 3]}]|[{[4, 5, 6]}]|\n+---+-------------+-------------+\n</code></pre> <p>Here, the lambda expression will be applied to the last repeated element <code>e</code>.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"s1!.e!\": None,\n...  \"s2!.e!\": lambda e : e.cast(\"DOUBLE\")\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- s1!.e!: integer (nullable = false)\n |-- s2!.e!: double (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+-------------+-------------------+\n|           s1|                 s2|\n+-------------+-------------------+\n|[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]|\n+-------------+-------------------+\n</code></pre> <p>Example 4: Dataframe with maps</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: integer (nullable = false)\n |-- m1%value.b: integer (nullable = false)\n\n&gt;&gt;&gt; df.show()\n+---+-------------+\n| id|           m1|\n+---+-------------+\n|  1|{a -&gt; {2, 3}}|\n+---+-------------+\n</code></pre> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"id\": None,\n...  \"m1%key\": lambda key : f.upper(key),\n...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- id: integer (nullable = false)\n |-- m1%key: string (nullable = false)\n |-- m1%value.a: double (nullable = false)\n\n&gt;&gt;&gt; new_df.show()\n+---+------------+\n| id|          m1|\n+---+------------+\n|  1|{A -&gt; {2.0}}|\n+---+------------+\n</code></pre> <p>Example 5: Accessing multiple repetition levels</p> <pre><code>&gt;&gt;&gt; df = spark.sql('''\n...     SELECT\n...         1 as id,\n...         ARRAY(\n...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n...         ) as s1\n... ''')\n&gt;&gt;&gt; nested.print_schema(df)\nroot\n |-- id: integer (nullable = false)\n |-- s1!.average: integer (nullable = false)\n |-- s1!.values!: integer (nullable = false)\n\n&gt;&gt;&gt; df.show(truncate=False)\n+---+--------------------------------------+\n|id |s1                                    |\n+---+--------------------------------------+\n|1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n+---+--------------------------------------+\n</code></pre> <p>Here, the transformation applied to \"s1!.values!\" takes two arguments.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"id\": None,\n...  \"s1!.average\": None,\n...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+-----------------------------------------+\n|id |s1                                       |\n+---+-----------------------------------------+\n|1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n+---+-----------------------------------------+\n</code></pre> <p>Extra arguments can be added to the left for each repetition level, up to the root level.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...  \"id\": None,\n...  \"s1!.average\": None,\n...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n... })\n&gt;&gt;&gt; new_df.show(truncate=False)\n+---+---------------------------------------+\n|id |s1                                     |\n+---+---------------------------------------+\n|1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n+---+---------------------------------------+\n</code></pre> Source code in <code>spark_frame/nested_impl/select_impl.py</code> <pre><code>def select(df: DataFrame, fields: Mapping[str, ColumnTransformation]) -&gt; DataFrame:\n    \"\"\"Project a set of expressions and returns a new [DataFrame][pyspark.sql.DataFrame].\n\n    This method is similar to the [DataFrame.select][pyspark.sql.DataFrame.select] method, with the extra\n    capability of working on nested and repeated fields (structs and arrays).\n\n    The syntax for field names works as follows:\n\n    - \".\" is the separator for struct elements\n    - \"!\" must be appended at the end of fields that are repeated (arrays)\n    - Map keys are appended with `%key`\n    - Map values are appended with `%value`\n\n    The following types of transformation are allowed:\n\n    - String and column expressions can be used on any non-repeated field, even nested ones.\n    - When working on repeated fields, transformations must be expressed as higher order functions\n      (e.g. lambda expressions). String and column expressions can be used on repeated fields as well,\n      but their value will be repeated multiple times.\n    - When working on multiple levels of nested arrays, higher order functions may take multiple arguments,\n      corresponding to each level of repetition (See Example 5.).\n    - `None` can also be used to represent the identity transformation, this is useful to select a field without\n       changing and without having to repeat its name.\n\n    !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\"\n        Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field\n        names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`.\n        This can be worked around using the transformation\n        [`spark_frame.transformations.transform_all_field_names`]\n        [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names].\n\n    Args:\n        df: A Spark DataFrame\n        fields: A Dict(field_name, transformation_to_apply)\n\n    Returns:\n        A new DataFrame where only the specified field have been selected and the corresponding\n        transformations were applied to each of them.\n\n    Examples: Example 1: non-repeated fields\n        &gt;&gt;&gt; from pyspark.sql import SparkSession\n        &gt;&gt;&gt; from pyspark.sql import functions as f\n        &gt;&gt;&gt; from spark_frame import nested\n        &gt;&gt;&gt; spark = SparkSession.builder.appName(\"doctest\").getOrCreate()\n        &gt;&gt;&gt; df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''')\n        &gt;&gt;&gt; df.printSchema()\n        root\n         |-- id: integer (nullable = false)\n         |-- s: struct (nullable = false)\n         |    |-- a: integer (nullable = false)\n         |    |-- b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+------+\n        | id|     s|\n        +---+------+\n        |  1|{2, 3}|\n        +---+------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on non-repeated fields may be expressed as a string representing a column name,\n        a Column expression or None.\n        (In this example the column \"id\" will be dropped because it was not selected)\n        &gt;&gt;&gt; new_df = nested.select(df, {\n        ...     \"s.a\": \"s.a\",                        # Column name (string)\n        ...     \"s.b\": None,                         # None: use to keep a column without having to repeat its name\n        ...     \"s.c\": f.col(\"s.a\") + f.col(\"s.b\")   # Column expression\n        ... })\n        &gt;&gt;&gt; new_df.printSchema()\n        root\n         |-- s: struct (nullable = false)\n         |    |-- a: integer (nullable = false)\n         |    |-- b: integer (nullable = false)\n         |    |-- c: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---------+\n        |        s|\n        +---------+\n        |{2, 3, 5}|\n        +---------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 2: repeated fields\n        &gt;&gt;&gt; df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s!.a: integer (nullable = false)\n         |-- s!.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+----------------+\n        | id|               s|\n        +---+----------------+\n        |  1|[{1, 2}, {3, 4}]|\n        +---+----------------+\n        &lt;BLANKLINE&gt;\n\n        Transformations on repeated fields must be expressed as higher-order\n        functions (lambda expressions or named functions).\n        The value passed to this function will correspond to the last repeated element.\n        &gt;&gt;&gt; df.transform(nested.select, {\n        ...     \"s!.a\": lambda s: s[\"a\"],\n        ...     \"s!.b\": None,\n        ...     \"s!.c\": lambda s: s[\"a\"] + s[\"b\"]\n        ... }).show(truncate=False)\n        +----------------------+\n        |s                     |\n        +----------------------+\n        |[{1, 2, 3}, {3, 4, 7}]|\n        +----------------------+\n        &lt;BLANKLINE&gt;\n\n        String and column expressions can be used on repeated fields as well,\n        but their value will be repeated multiple times.\n        &gt;&gt;&gt; df.transform(nested.select, {\n        ...     \"id\": None,\n        ...     \"s!.a\": \"id\",\n        ...     \"s!.b\": f.lit(2)\n        ... }).show(truncate=False)\n        +---+----------------+\n        |id |s               |\n        +---+----------------+\n        |1  |[{1, 2}, {1, 2}]|\n        +---+----------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 3: field repeated twice\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1,\n        ...         ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2!.e!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+-------------+\n        | id|           s1|           s2|\n        +---+-------------+-------------+\n        |  1|[{[1, 2, 3]}]|[{[4, 5, 6]}]|\n        +---+-------------+-------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the lambda expression will be applied to the last repeated element `e`.\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"s1!.e!\": None,\n        ...  \"s2!.e!\": lambda e : e.cast(\"DOUBLE\")\n        ... })\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- s1!.e!: integer (nullable = false)\n         |-- s2!.e!: double (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +-------------+-------------------+\n        |           s1|                 s2|\n        +-------------+-------------------+\n        |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]|\n        +-------------+-------------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 4: Dataframe with maps\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: integer (nullable = false)\n         |-- m1%value.b: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show()\n        +---+-------------+\n        | id|           m1|\n        +---+-------------+\n        |  1|{a -&gt; {2, 3}}|\n        +---+-------------+\n        &lt;BLANKLINE&gt;\n\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"id\": None,\n        ...  \"m1%key\": lambda key : f.upper(key),\n        ...  \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\")\n        ... })\n        &gt;&gt;&gt; nested.print_schema(new_df)\n        root\n         |-- id: integer (nullable = false)\n         |-- m1%key: string (nullable = false)\n         |-- m1%value.a: double (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; new_df.show()\n        +---+------------+\n        | id|          m1|\n        +---+------------+\n        |  1|{A -&gt; {2.0}}|\n        +---+------------+\n        &lt;BLANKLINE&gt;\n\n    Examples: Example 5: Accessing multiple repetition levels\n        &gt;&gt;&gt; df = spark.sql('''\n        ...     SELECT\n        ...         1 as id,\n        ...         ARRAY(\n        ...             STRUCT(2 as average, ARRAY(1, 2, 3) as values),\n        ...             STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values)\n        ...         ) as s1\n        ... ''')\n        &gt;&gt;&gt; nested.print_schema(df)\n        root\n         |-- id: integer (nullable = false)\n         |-- s1!.average: integer (nullable = false)\n         |-- s1!.values!: integer (nullable = false)\n        &lt;BLANKLINE&gt;\n        &gt;&gt;&gt; df.show(truncate=False)\n        +---+--------------------------------------+\n        |id |s1                                    |\n        +---+--------------------------------------+\n        |1  |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]|\n        +---+--------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Here, the transformation applied to \"s1!.values!\" takes two arguments.\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"id\": None,\n        ...  \"s1!.average\": None,\n        ...  \"s1!.values!\": lambda s1, value : value - s1[\"average\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+-----------------------------------------+\n        |id |s1                                       |\n        +---+-----------------------------------------+\n        |1  |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]|\n        +---+-----------------------------------------+\n        &lt;BLANKLINE&gt;\n\n        Extra arguments can be added to the left for each repetition level, up to the root level.\n        &gt;&gt;&gt; new_df = df.transform(nested.select, {\n        ...  \"id\": None,\n        ...  \"s1!.average\": None,\n        ...  \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"]\n        ... })\n        &gt;&gt;&gt; new_df.show(truncate=False)\n        +---+---------------------------------------+\n        |id |s1                                     |\n        +---+---------------------------------------+\n        |1  |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]|\n        +---+---------------------------------------+\n        &lt;BLANKLINE&gt;\n\n    \"\"\"\n    return df.select(*resolve_nested_fields(fields, starting_level=df))\n</code></pre>"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.select_nested_fields--the-task","title":"The task","text":"<p>Let's say we want to select only the following fields, while keeping the same overall structure: - employee_id - projects.name - projects.tasks.name</p>"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.select_nested_fields--without-spark_framenested","title":"Without spark_frame.nested","text":"<p>This forces us to do something quite complicated, using pyspark.sql.functions.transform</p> <pre><code>&gt;&gt;&gt; new_df = df.select(\n...     \"employee_id\",\n...     f.transform(\"projects\", lambda project:\n...         f.struct(project[\"name\"].alias(\"name\"), f.transform(project[\"tasks\"], lambda task:\n...             f.struct(task[\"name\"].alias(\"name\"))\n...         ).alias(\"tasks\"))\n...     ).alias(\"projects\")\n... )\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n\n&gt;&gt;&gt; new_df.show(truncate=False)\n+-----------+----------------------------------------------------------------------+\n|employee_id|projects                                                              |\n+-----------+----------------------------------------------------------------------+\n|1          |[{Project A, [{Task 1}, {Task 2}]}, {Project B, [{Task 3}, {Task 4}]}]|\n|2          |[{Project C, [{Task 5}, {Task 6}]}, {Project D, [{Task 7}, {Task 8}]}]|\n+-----------+----------------------------------------------------------------------+\n</code></pre>"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.select_nested_fields--with-spark_framenested","title":"With spark_frame.nested","text":"<p>Using <code>spark_frame.nested.select</code>, we can easily obtain the exact same result.</p> <pre><code>&gt;&gt;&gt; new_df = df.transform(nested.select, {\n...     \"employee_id\": None,\n...     \"projects!.name\": None,\n...     \"projects!.tasks!.name\": None\n... })\n&gt;&gt;&gt; nested.print_schema(new_df)\nroot\n |-- employee_id: integer (nullable = true)\n |-- projects!.name: string (nullable = true)\n |-- projects!.tasks!.name: string (nullable = true)\n\n&gt;&gt;&gt; new_df.show(truncate=False)\n+-----------+----------------------------------------------------------------------+\n|employee_id|projects                                                              |\n+-----------+----------------------------------------------------------------------+\n|1          |[{Project A, [{Task 1}, {Task 2}]}, {Project B, [{Task 3}, {Task 4}]}]|\n|2          |[{Project C, [{Task 5}, {Task 6}]}, {Project D, [{Task 7}, {Task 8}]}]|\n+-----------+----------------------------------------------------------------------+\n</code></pre> <p>Here, <code>None</code> is used to indicate that we don't want to perform any transformation on the column, be we could also replace them with functions to perform transformations at the same time. For instance, we could pass all the names to uppercase like this:</p> <pre><code>&gt;&gt;&gt; df.transform(nested.select, {\n...     \"employee_id\": None,\n...     \"projects!.name\": lambda project: f.upper(project[\"name\"]),\n...     \"projects!.tasks!.name\": lambda task: f.upper(task[\"name\"])\n... }).show(truncate=False)\n+-----------+----------------------------------------------------------------------+\n|employee_id|projects                                                              |\n+-----------+----------------------------------------------------------------------+\n|1          |[{PROJECT A, [{TASK 1}, {TASK 2}]}, {PROJECT B, [{TASK 3}, {TASK 4}]}]|\n|2          |[{PROJECT C, [{TASK 5}, {TASK 6}]}, {PROJECT D, [{TASK 7}, {TASK 8}]}]|\n+-----------+----------------------------------------------------------------------+\n</code></pre>"},{"location":"use_cases/working_with_nested_data/#aggregating-nested-fields-todo","title":"Aggregating nested fields [TODO]","text":""},{"location":"use_cases/working_with_nested_data/#advanced-transformations-with-nested-fields-todo","title":"Advanced transformations with nested fields [TODO]","text":""}]}