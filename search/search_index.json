{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spark-frame What is it ? Spark-frame is a library that super-charges your Spark DataFrames! It brings several utility methods and transformation functions for PySpark DataFrames. These methods were initially part of the karadoc project used at Younited , but they were fully independent from karadoc, so it made more sense to keep them as a standalone library. Several of these methods were my initial inspiration to make the cousin project bigquery-frame , which was first made to illustrate this blog article . This is why you will find similar methods in both spark_frame and bigquery_frame , except the former runs on PySpark while the latter runs on BigQuery (obviously). I try to keep both projects consistent together, and new eventually port new developments made on one project to the other one. Getting Started Visit the official Spark-frame website documentation for use cases examples and reference . Installation spark-frame is available on PyPi . pip install spark-frame Compatibilities and requirements This library does not depend on any other library. Pyspark must be installed separately to use it. It is compatible with the following versions: Python: requires 3.8.1 or higher (tested against Python 3.9, 3.10 and 3.11) pyspark: requires 3.3.0 or higher This library is tested against Mac and Linux. Release notes v0.1.1 Added a new transformation spark_frame.transformations.flatten_all_arrays . Added support for multi-arg transformation to nested.select and nested.with_fields With this feature, we can now access parent fields from higher levels when applying a transformation. Example: >>> nested.print_schema(df) \"\"\" root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) \"\"\" >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ >>> new_df = df.transform(nested.with_fields, { >>> \"s1!.values!\": lambda s1, value: value - s1[\"average\"] # This transformation takes 2 arguments >>> }) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ v0.1.0 Added a new amazing module called spark_frame.nested , which makes manipulation of nested data structure much easier! Make sure to check out the reference and the use-cases . Also added a new module called spark_frame.nested_functions , which contains aggregation methods for nested data structures ( See Reference ). New transformations : spark_frame.transformations.transform_all_field_names spark_frame.transformations.transform_all_fields spark_frame.transformations.unnest_field spark_frame.transformations.unnest_all_fields spark_frame.transformations.union_dataframes v0.0.3 New transformation: spark_frame.transformations.convert_all_maps_to_arrays . New transformation: spark_frame.transformations.sort_all_arrays . New transformation: spark_frame.transformations.harmonize_dataframes . Usage Spark-frame contains several utility methods, all documented in the reference . There are grouped into several modules: functions : Extra functions similar to pyspark.sql.functions . graph : Implementations of graph algorithms. transformations : Generic transformations taking one or more input DataFrames as argument and returning a new DataFrame. schema_utils : Methods useful for manipulating DataFrame schemas.","title":"What is spark-frame ?"},{"location":"#spark-frame","text":"","title":"Spark-frame"},{"location":"#what-is-it","text":"Spark-frame is a library that super-charges your Spark DataFrames! It brings several utility methods and transformation functions for PySpark DataFrames. These methods were initially part of the karadoc project used at Younited , but they were fully independent from karadoc, so it made more sense to keep them as a standalone library. Several of these methods were my initial inspiration to make the cousin project bigquery-frame , which was first made to illustrate this blog article . This is why you will find similar methods in both spark_frame and bigquery_frame , except the former runs on PySpark while the latter runs on BigQuery (obviously). I try to keep both projects consistent together, and new eventually port new developments made on one project to the other one.","title":"What is it ?"},{"location":"#getting-started","text":"Visit the official Spark-frame website documentation for use cases examples and reference .","title":"Getting Started"},{"location":"#installation","text":"spark-frame is available on PyPi . pip install spark-frame","title":"Installation"},{"location":"#compatibilities-and-requirements","text":"This library does not depend on any other library. Pyspark must be installed separately to use it. It is compatible with the following versions: Python: requires 3.8.1 or higher (tested against Python 3.9, 3.10 and 3.11) pyspark: requires 3.3.0 or higher This library is tested against Mac and Linux.","title":"Compatibilities and requirements"},{"location":"#release-notes","text":"","title":"Release notes"},{"location":"#v011","text":"Added a new transformation spark_frame.transformations.flatten_all_arrays . Added support for multi-arg transformation to nested.select and nested.with_fields With this feature, we can now access parent fields from higher levels when applying a transformation. Example: >>> nested.print_schema(df) \"\"\" root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) \"\"\" >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ >>> new_df = df.transform(nested.with_fields, { >>> \"s1!.values!\": lambda s1, value: value - s1[\"average\"] # This transformation takes 2 arguments >>> }) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+","title":"v0.1.1"},{"location":"#v010","text":"Added a new amazing module called spark_frame.nested , which makes manipulation of nested data structure much easier! Make sure to check out the reference and the use-cases . Also added a new module called spark_frame.nested_functions , which contains aggregation methods for nested data structures ( See Reference ). New transformations : spark_frame.transformations.transform_all_field_names spark_frame.transformations.transform_all_fields spark_frame.transformations.unnest_field spark_frame.transformations.unnest_all_fields spark_frame.transformations.union_dataframes","title":"v0.1.0"},{"location":"#v003","text":"New transformation: spark_frame.transformations.convert_all_maps_to_arrays . New transformation: spark_frame.transformations.sort_all_arrays . New transformation: spark_frame.transformations.harmonize_dataframes .","title":"v0.0.3"},{"location":"#usage","text":"Spark-frame contains several utility methods, all documented in the reference . There are grouped into several modules: functions : Extra functions similar to pyspark.sql.functions . graph : Implementations of graph algorithms. transformations : Generic transformations taking one or more input DataFrames as argument and returning a new DataFrame. schema_utils : Methods useful for manipulating DataFrame schemas.","title":"Usage"},{"location":"reference/functions/","text":"Like with pyspark.sql.functions , the methods in this module all return Column expressions and can be used to build operations on Spark DataFrames using select , withColumn , etc. empty_array ( element_type : Union [ DataType , str ]) -> Column Create an empty Spark array column of the specified type. This is a workaround to the Spark method typedLit not being available in PySpark Parameters: Name Type Description Default element_type Union [ DataType , str ] The type of the array's element required Returns: Type Description Column A Spark Column representing an empty array. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> from pyspark.sql import functions as f >>> df = spark . sql ( '''SELECT 1 as a''' ) >>> res = df . withColumn ( 'empty_array' , empty_array ( \"STRUCT<b: int, c: array<string>>\" )) >>> res . printSchema () root |-- a: integer (nullable = false) |-- empty_array: array (nullable = false) | |-- element: struct (containsNull = true) | | |-- b: integer (nullable = true) | | |-- c: array (nullable = true) | | | |-- element: string (containsNull = true) >>> res . show () +---+-----------+ | a|empty_array| +---+-----------+ | 1| []| +---+-----------+ Source code in spark_frame/functions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def empty_array ( element_type : Union [ DataType , str ]) -> Column : \"\"\"Create an empty Spark array column of the specified type. This is a workaround to the Spark method `typedLit` not being available in PySpark Args: element_type: The type of the array's element Returns: A Spark Column representing an empty array. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> from pyspark.sql import functions as f >>> df = spark.sql('''SELECT 1 as a''') >>> res = df.withColumn('empty_array', empty_array(\"STRUCT<b: int, c: array<string>>\")) >>> res.printSchema() root |-- a: integer (nullable = false) |-- empty_array: array (nullable = false) | |-- element: struct (containsNull = true) | | |-- b: integer (nullable = true) | | |-- c: array (nullable = true) | | | |-- element: string (containsNull = true) <BLANKLINE> >>> res.show() +---+-----------+ | a|empty_array| +---+-----------+ | 1| []| +---+-----------+ <BLANKLINE> \"\"\" return f . array_except ( f . array ( f . lit ( None ) . cast ( element_type )), f . array ( f . lit ( None ))) generic_struct ( * columns : str , col_name_alias : str = 'name' , col_value_alias : str = 'value' ) -> Column Transform a set of columns into a generic array of struct of type ARRAY (column_name -> column_value) Parameters: Name Type Description Default *columns str One or multiple column names to add to the generic struct () col_name_alias str Alias of the field containing the column names in the returned struct 'name' col_value_alias str Alias of the field containing the column values in the returned struct 'value' Returns: Type Description Column A Spark Column Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( ''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.name`, ... col3 as `pokemon.types` ... FROM VALUES ... (4, 'Charmander', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (5, 'Charmeleon', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (6, 'Charizard', ARRAY(NAMED_STRUCT('type', 'Fire'), NAMED_STRUCT('type', 'Flying'))) ... ''' ) >>> df . show () +----------+------------+------------------+ |pokemon.id|pokemon.name| pokemon.types| +----------+------------+------------------+ | 4| Charmander| [{Fire}]| | 5| Charmeleon| [{Fire}]| | 6| Charizard|[{Fire}, {Flying}]| +----------+------------+------------------+ >>> res = df . select ( generic_struct ( \"pokemon.id\" , \"pokemon.name\" , \"pokemon.types\" ) . alias ( 'values' )) >>> res . printSchema () root |-- values: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- name: string (nullable = false) | | |-- value: string (nullable = false) >>> res . show ( 10 , False ) +---------------------------------------------------------------------------------+ |values | +---------------------------------------------------------------------------------+ |[{pokemon.id, 4}, {pokemon.name, Charmander}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 5}, {pokemon.name, Charmeleon}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 6}, {pokemon.name, Charizard}, {pokemon.types, [{Fire}, {Flying}]}]| +---------------------------------------------------------------------------------+ Source code in spark_frame/functions.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def generic_struct ( * columns : str , col_name_alias : str = \"name\" , col_value_alias : str = \"value\" ) -> Column : \"\"\"Transform a set of columns into a generic array of struct of type ARRAY<STRUCT<name: STRING, value: STRING> (column_name -> column_value) Args: *columns: One or multiple column names to add to the generic struct col_name_alias: Alias of the field containing the column names in the returned struct col_value_alias: Alias of the field containing the column values in the returned struct Returns: A Spark Column Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql(''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.name`, ... col3 as `pokemon.types` ... FROM VALUES ... (4, 'Charmander', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (5, 'Charmeleon', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (6, 'Charizard', ARRAY(NAMED_STRUCT('type', 'Fire'), NAMED_STRUCT('type', 'Flying'))) ... ''') >>> df.show() +----------+------------+------------------+ |pokemon.id|pokemon.name| pokemon.types| +----------+------------+------------------+ | 4| Charmander| [{Fire}]| | 5| Charmeleon| [{Fire}]| | 6| Charizard|[{Fire}, {Flying}]| +----------+------------+------------------+ <BLANKLINE> >>> res = df.select(generic_struct(\"pokemon.id\", \"pokemon.name\", \"pokemon.types\").alias('values')) >>> res.printSchema() root |-- values: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- name: string (nullable = false) | | |-- value: string (nullable = false) <BLANKLINE> >>> res.show(10, False) +---------------------------------------------------------------------------------+ |values | +---------------------------------------------------------------------------------+ |[{pokemon.id, 4}, {pokemon.name, Charmander}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 5}, {pokemon.name, Charmeleon}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 6}, {pokemon.name, Charizard}, {pokemon.types, [{Fire}, {Flying}]}]| +---------------------------------------------------------------------------------+ <BLANKLINE> \"\"\" return f . array ( * [ f . struct ( f . lit ( c ) . alias ( col_name_alias ), f . col ( quote ( c )) . astype ( StringType ()) . alias ( col_value_alias )) for c in columns ] ) nullable ( col : Column ) -> Column Make a pyspark.sql.Column nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> from pyspark.sql import functions as f >>> df = spark . sql ( '''SELECT 1 as a''' ) . withColumn ( \"b\" , f . lit ( \"2\" )) >>> df . printSchema () root |-- a: integer (nullable = false) |-- b: string (nullable = false) >>> res = df . withColumn ( 'a' , nullable ( f . col ( 'a' ))) . withColumn ( 'b' , nullable ( f . col ( 'b' ))) >>> res . printSchema () root |-- a: integer (nullable = true) |-- b: string (nullable = true) Source code in spark_frame/functions.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def nullable ( col : Column ) -> Column : \"\"\"Make a `pyspark.sql.Column` nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> from pyspark.sql import functions as f >>> df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\")) >>> df.printSchema() root |-- a: integer (nullable = false) |-- b: string (nullable = false) <BLANKLINE> >>> res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b'))) >>> res.printSchema() root |-- a: integer (nullable = true) |-- b: string (nullable = true) <BLANKLINE> \"\"\" return f . when ( ~ col . isNull (), col )","title":"spark_frame.functions"},{"location":"reference/functions/#spark_frame.functions.empty_array","text":"Create an empty Spark array column of the specified type. This is a workaround to the Spark method typedLit not being available in PySpark Parameters: Name Type Description Default element_type Union [ DataType , str ] The type of the array's element required Returns: Type Description Column A Spark Column representing an empty array. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> from pyspark.sql import functions as f >>> df = spark . sql ( '''SELECT 1 as a''' ) >>> res = df . withColumn ( 'empty_array' , empty_array ( \"STRUCT<b: int, c: array<string>>\" )) >>> res . printSchema () root |-- a: integer (nullable = false) |-- empty_array: array (nullable = false) | |-- element: struct (containsNull = true) | | |-- b: integer (nullable = true) | | |-- c: array (nullable = true) | | | |-- element: string (containsNull = true) >>> res . show () +---+-----------+ | a|empty_array| +---+-----------+ | 1| []| +---+-----------+ Source code in spark_frame/functions.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def empty_array ( element_type : Union [ DataType , str ]) -> Column : \"\"\"Create an empty Spark array column of the specified type. This is a workaround to the Spark method `typedLit` not being available in PySpark Args: element_type: The type of the array's element Returns: A Spark Column representing an empty array. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> from pyspark.sql import functions as f >>> df = spark.sql('''SELECT 1 as a''') >>> res = df.withColumn('empty_array', empty_array(\"STRUCT<b: int, c: array<string>>\")) >>> res.printSchema() root |-- a: integer (nullable = false) |-- empty_array: array (nullable = false) | |-- element: struct (containsNull = true) | | |-- b: integer (nullable = true) | | |-- c: array (nullable = true) | | | |-- element: string (containsNull = true) <BLANKLINE> >>> res.show() +---+-----------+ | a|empty_array| +---+-----------+ | 1| []| +---+-----------+ <BLANKLINE> \"\"\" return f . array_except ( f . array ( f . lit ( None ) . cast ( element_type )), f . array ( f . lit ( None )))","title":"empty_array()"},{"location":"reference/functions/#spark_frame.functions.generic_struct","text":"Transform a set of columns into a generic array of struct of type ARRAY (column_name -> column_value) Parameters: Name Type Description Default *columns str One or multiple column names to add to the generic struct () col_name_alias str Alias of the field containing the column names in the returned struct 'name' col_value_alias str Alias of the field containing the column values in the returned struct 'value' Returns: Type Description Column A Spark Column Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( ''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.name`, ... col3 as `pokemon.types` ... FROM VALUES ... (4, 'Charmander', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (5, 'Charmeleon', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (6, 'Charizard', ARRAY(NAMED_STRUCT('type', 'Fire'), NAMED_STRUCT('type', 'Flying'))) ... ''' ) >>> df . show () +----------+------------+------------------+ |pokemon.id|pokemon.name| pokemon.types| +----------+------------+------------------+ | 4| Charmander| [{Fire}]| | 5| Charmeleon| [{Fire}]| | 6| Charizard|[{Fire}, {Flying}]| +----------+------------+------------------+ >>> res = df . select ( generic_struct ( \"pokemon.id\" , \"pokemon.name\" , \"pokemon.types\" ) . alias ( 'values' )) >>> res . printSchema () root |-- values: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- name: string (nullable = false) | | |-- value: string (nullable = false) >>> res . show ( 10 , False ) +---------------------------------------------------------------------------------+ |values | +---------------------------------------------------------------------------------+ |[{pokemon.id, 4}, {pokemon.name, Charmander}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 5}, {pokemon.name, Charmeleon}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 6}, {pokemon.name, Charizard}, {pokemon.types, [{Fire}, {Flying}]}]| +---------------------------------------------------------------------------------+ Source code in spark_frame/functions.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def generic_struct ( * columns : str , col_name_alias : str = \"name\" , col_value_alias : str = \"value\" ) -> Column : \"\"\"Transform a set of columns into a generic array of struct of type ARRAY<STRUCT<name: STRING, value: STRING> (column_name -> column_value) Args: *columns: One or multiple column names to add to the generic struct col_name_alias: Alias of the field containing the column names in the returned struct col_value_alias: Alias of the field containing the column values in the returned struct Returns: A Spark Column Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql(''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.name`, ... col3 as `pokemon.types` ... FROM VALUES ... (4, 'Charmander', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (5, 'Charmeleon', ARRAY(NAMED_STRUCT('type', 'Fire'))), ... (6, 'Charizard', ARRAY(NAMED_STRUCT('type', 'Fire'), NAMED_STRUCT('type', 'Flying'))) ... ''') >>> df.show() +----------+------------+------------------+ |pokemon.id|pokemon.name| pokemon.types| +----------+------------+------------------+ | 4| Charmander| [{Fire}]| | 5| Charmeleon| [{Fire}]| | 6| Charizard|[{Fire}, {Flying}]| +----------+------------+------------------+ <BLANKLINE> >>> res = df.select(generic_struct(\"pokemon.id\", \"pokemon.name\", \"pokemon.types\").alias('values')) >>> res.printSchema() root |-- values: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- name: string (nullable = false) | | |-- value: string (nullable = false) <BLANKLINE> >>> res.show(10, False) +---------------------------------------------------------------------------------+ |values | +---------------------------------------------------------------------------------+ |[{pokemon.id, 4}, {pokemon.name, Charmander}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 5}, {pokemon.name, Charmeleon}, {pokemon.types, [{Fire}]}] | |[{pokemon.id, 6}, {pokemon.name, Charizard}, {pokemon.types, [{Fire}, {Flying}]}]| +---------------------------------------------------------------------------------+ <BLANKLINE> \"\"\" return f . array ( * [ f . struct ( f . lit ( c ) . alias ( col_name_alias ), f . col ( quote ( c )) . astype ( StringType ()) . alias ( col_value_alias )) for c in columns ] )","title":"generic_struct()"},{"location":"reference/functions/#spark_frame.functions.nullable","text":"Make a pyspark.sql.Column nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> from pyspark.sql import functions as f >>> df = spark . sql ( '''SELECT 1 as a''' ) . withColumn ( \"b\" , f . lit ( \"2\" )) >>> df . printSchema () root |-- a: integer (nullable = false) |-- b: string (nullable = false) >>> res = df . withColumn ( 'a' , nullable ( f . col ( 'a' ))) . withColumn ( 'b' , nullable ( f . col ( 'b' ))) >>> res . printSchema () root |-- a: integer (nullable = true) |-- b: string (nullable = true) Source code in spark_frame/functions.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def nullable ( col : Column ) -> Column : \"\"\"Make a `pyspark.sql.Column` nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> from pyspark.sql import functions as f >>> df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\")) >>> df.printSchema() root |-- a: integer (nullable = false) |-- b: string (nullable = false) <BLANKLINE> >>> res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b'))) >>> res.printSchema() root |-- a: integer (nullable = true) |-- b: string (nullable = true) <BLANKLINE> \"\"\" return f . when ( ~ col . isNull (), col )","title":"nullable()"},{"location":"reference/graph/","text":"This module contains implementations of graph algorithms and related methods. ascending_forest_traversal ( input_df : DataFrame , node_id : str , parent_id : str , keep_labels : bool = False ) -> DataFrame Given a DataFrame representing a labeled forest with columns id , parent_id and other label columns, performs a graph traversal that will return a DataFrame with the same schema that gives for each node the labels of it's furthest ancestor. In the input DataFrame, a node is considered to have no parent if its parent_id is null or equal to its node_id. In the output DataFrame, a node that has no parent will have its parent_id equal to its node_id. Cycle protection: If the graph contains any cycle, the nodes in that cycle will have a NULL parent_id. It has a security against dependency cycles, but no security preventing a combinatorial explosion if some nodes have more than one parent. Parameters: Name Type Description Default input_df DataFrame A Spark DataFrame required node_id str Name of the column that represent the node's ids required parent_id str Name of the column that represent the parent node's ids required keep_labels bool If set to true, add two structs column called \"node\" and \"furthest_ancestor\" containing the content of the row from the input DataFrame for the corresponding nodes and their furthest ancestor False Returns: Type Description DataFrame A DataFrame with two columns named according to node_id and parent_id that gives for each node DataFrame the id of it's furthest ancestor (in the parent_id column). DataFrame If the option keep_labels is used, two extra columns of type STRUCT are a added to the output DataFrame, DataFrame they represent the content of the rows in the input DataFrame corresponding to the node and its furthest DataFrame ancestor, respectively. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () Given a DataFrame with pokemon attributes and evolution links >>> input_df = spark . sql ( ''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.evolve_to_id`, ... col3 as `pokemon.name`, ... col4 as `pokemon.types` ... FROM VALUES ... (4, 5, 'Charmander', ARRAY('Fire')), ... (5, 6, 'Charmeleon', ARRAY('Fire')), ... (6, NULL, 'Charizard', ARRAY('Fire', 'Flying')) ... ''' ) >>> input_df . show () +----------+--------------------+------------+--------------+ |pokemon.id|pokemon.evolve_to_id|pokemon.name| pokemon.types| +----------+--------------------+------------+--------------+ | 4| 5| Charmander| [Fire]| | 5| 6| Charmeleon| [Fire]| | 6| null| Charizard|[Fire, Flying]| +----------+--------------------+------------+--------------+ We compute a DataFrame that for each pokemon.id gives the attributes of its highest level of evolution >>> ascending_forest_traversal ( input_df , \"pokemon.id\" , \"pokemon.evolve_to_id\" ) . orderBy ( \"`pokemon.id`\" ) . show () +----------+--------------------+ |pokemon.id|pokemon.evolve_to_id| +----------+--------------------+ | 4| 6| | 5| 6| | 6| 6| +----------+--------------------+ With the keep_label option extra joins are performed at the end of the algorithm to add two struct columns containing the corresponding row for the original node and the furthest ancestor. >>> ascending_forest_traversal ( input_df , \"pokemon.id\" , \"pokemon.evolve_to_id\" , keep_labels = True ... ) . orderBy ( \"`pokemon.id`\" ) . show ( 10 , False ) +----------+--------------------+------------------------------------+------------------------------------+ |pokemon.id|pokemon.evolve_to_id|node |furthest_ancestor | +----------+--------------------+------------------------------------+------------------------------------+ |4 |6 |{4, 5, Charmander, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |5 |6 |{5, 6, Charmeleon, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |6 |6 |{6, null, Charizard, [Fire, Flying]}|{6, null, Charizard, [Fire, Flying]}| +----------+--------------------+------------------------------------+------------------------------------+ Cycle Protection: to prevent the algorithm from looping indefinitely, cycles are detected, and the nodes that are part of cycles will end up with a NULL value as their furthest ancestor >>> input_df = spark . sql ( ''' ... SELECT ... col1 as `node_id`, ... col2 as `parent_id` ... FROM VALUES (1, 2), (2, 3), (3, 1) ... ''' ) >>> input_df . show () +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| 2| | 2| 3| | 3| 1| +-------+---------+ >>> ascending_forest_traversal ( input_df , \"node_id\" , \"parent_id\" ) . orderBy ( \"node_id\" ) . show () +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| null| | 2| null| | 3| null| +-------+---------+ Source code in spark_frame/graph_impl/ascending_forest_traversal.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 def ascending_forest_traversal ( input_df : DataFrame , node_id : str , parent_id : str , keep_labels : bool = False ) -> DataFrame : \"\"\"Given a DataFrame representing a labeled forest with columns `id`, `parent_id` and other label columns, performs a graph traversal that will return a DataFrame with the same schema that gives for each node the labels of it's furthest ancestor. In the input DataFrame, a node is considered to have no parent if its parent_id is null or equal to its node_id. In the output DataFrame, a node that has no parent will have its parent_id equal to its node_id. Cycle protection: If the graph contains any cycle, the nodes in that cycle will have a NULL parent_id. It has a security against dependency cycles, but no security preventing a combinatorial explosion if some nodes have more than one parent. Args: input_df: A Spark DataFrame node_id: Name of the column that represent the node's ids parent_id: Name of the column that represent the parent node's ids keep_labels: If set to true, add two structs column called \"node\" and \"furthest_ancestor\" containing the content of the row from the input DataFrame for the corresponding nodes and their furthest ancestor Returns: A DataFrame with two columns named according to `node_id` and `parent_id` that gives for each node the id of it's furthest ancestor (in the `parent_id` column). If the option `keep_labels` is used, two extra columns of type STRUCT are a added to the output DataFrame, they represent the content of the rows in the input DataFrame corresponding to the node and its furthest ancestor, respectively. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() Given a DataFrame with pokemon attributes and evolution links >>> input_df = spark.sql(''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.evolve_to_id`, ... col3 as `pokemon.name`, ... col4 as `pokemon.types` ... FROM VALUES ... (4, 5, 'Charmander', ARRAY('Fire')), ... (5, 6, 'Charmeleon', ARRAY('Fire')), ... (6, NULL, 'Charizard', ARRAY('Fire', 'Flying')) ... ''') >>> input_df.show() +----------+--------------------+------------+--------------+ |pokemon.id|pokemon.evolve_to_id|pokemon.name| pokemon.types| +----------+--------------------+------------+--------------+ | 4| 5| Charmander| [Fire]| | 5| 6| Charmeleon| [Fire]| | 6| null| Charizard|[Fire, Flying]| +----------+--------------------+------------+--------------+ <BLANKLINE> We compute a DataFrame that for each pokemon.id gives the attributes of its highest level of evolution >>> ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\").orderBy(\"`pokemon.id`\").show() +----------+--------------------+ |pokemon.id|pokemon.evolve_to_id| +----------+--------------------+ | 4| 6| | 5| 6| | 6| 6| +----------+--------------------+ <BLANKLINE> With the `keep_label` option extra joins are performed at the end of the algorithm to add two struct columns containing the corresponding row for the original node and the furthest ancestor. >>> ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\", keep_labels=True ... ).orderBy(\"`pokemon.id`\").show(10, False) +----------+--------------------+------------------------------------+------------------------------------+ |pokemon.id|pokemon.evolve_to_id|node |furthest_ancestor | +----------+--------------------+------------------------------------+------------------------------------+ |4 |6 |{4, 5, Charmander, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |5 |6 |{5, 6, Charmeleon, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |6 |6 |{6, null, Charizard, [Fire, Flying]}|{6, null, Charizard, [Fire, Flying]}| +----------+--------------------+------------------------------------+------------------------------------+ <BLANKLINE> *Cycle Protection:* to prevent the algorithm from looping indefinitely, cycles are detected, and the nodes that are part of cycles will end up with a NULL value as their furthest ancestor >>> input_df = spark.sql(''' ... SELECT ... col1 as `node_id`, ... col2 as `parent_id` ... FROM VALUES (1, 2), (2, 3), (3, 1) ... ''') >>> input_df.show() +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| 2| | 2| 3| | 3| 1| +-------+---------+ <BLANKLINE> >>> ascending_forest_traversal(input_df, \"node_id\", \"parent_id\").orderBy(\"node_id\").show() +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| null| | 2| null| | 3| null| +-------+---------+ <BLANKLINE> \"\"\" assert_true ( node_id in input_df . columns , \"Could not find column %s in Dataframe's columns: %s \" % ( node_id , input_df . columns ) ) assert_true ( parent_id in input_df . columns , \"Could not find column %s in Dataframe's columns: %s \" % ( parent_id , input_df . columns ), ) node_id_col_name = \"node_id\" parent_id_col_name = \"parent_id\" status_col_name = \"status\" df = input_df . select ( f . col ( quote ( node_id )) . alias ( node_id_col_name ), f . col ( quote ( parent_id )) . alias ( parent_id_col_name ) ) res_df = _ascending_forest_traversal ( df , node_id_col_name = node_id_col_name , parent_id_col_name = parent_id_col_name , status_col_name = status_col_name ) res_df = res_df . select ( f . col ( node_id_col_name ) . alias ( node_id ), f . col ( parent_id_col_name ) . alias ( parent_id ), ) if keep_labels : res_df = res_df . join ( input_df , node_id ) . select ( res_df [ \"*\" ], f . struct ( * [ input_df [ quote ( col )] for col in input_df . columns ]) . alias ( \"node\" ), ) res_df = res_df . join ( input_df , res_df [ quote ( parent_id )] == input_df [ quote ( node_id )]) . select ( res_df [ quote ( node_id )], res_df [ quote ( parent_id )], res_df [ \"node\" ], f . struct ( * [ input_df [ quote ( col )] for col in input_df . columns ]) . alias ( \"furthest_ancestor\" ), ) return res_df","title":"spark_frame.graph"},{"location":"reference/graph/#spark_frame.graph_impl.ascending_forest_traversal.ascending_forest_traversal","text":"Given a DataFrame representing a labeled forest with columns id , parent_id and other label columns, performs a graph traversal that will return a DataFrame with the same schema that gives for each node the labels of it's furthest ancestor. In the input DataFrame, a node is considered to have no parent if its parent_id is null or equal to its node_id. In the output DataFrame, a node that has no parent will have its parent_id equal to its node_id. Cycle protection: If the graph contains any cycle, the nodes in that cycle will have a NULL parent_id. It has a security against dependency cycles, but no security preventing a combinatorial explosion if some nodes have more than one parent. Parameters: Name Type Description Default input_df DataFrame A Spark DataFrame required node_id str Name of the column that represent the node's ids required parent_id str Name of the column that represent the parent node's ids required keep_labels bool If set to true, add two structs column called \"node\" and \"furthest_ancestor\" containing the content of the row from the input DataFrame for the corresponding nodes and their furthest ancestor False Returns: Type Description DataFrame A DataFrame with two columns named according to node_id and parent_id that gives for each node DataFrame the id of it's furthest ancestor (in the parent_id column). DataFrame If the option keep_labels is used, two extra columns of type STRUCT are a added to the output DataFrame, DataFrame they represent the content of the rows in the input DataFrame corresponding to the node and its furthest DataFrame ancestor, respectively. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () Given a DataFrame with pokemon attributes and evolution links >>> input_df = spark . sql ( ''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.evolve_to_id`, ... col3 as `pokemon.name`, ... col4 as `pokemon.types` ... FROM VALUES ... (4, 5, 'Charmander', ARRAY('Fire')), ... (5, 6, 'Charmeleon', ARRAY('Fire')), ... (6, NULL, 'Charizard', ARRAY('Fire', 'Flying')) ... ''' ) >>> input_df . show () +----------+--------------------+------------+--------------+ |pokemon.id|pokemon.evolve_to_id|pokemon.name| pokemon.types| +----------+--------------------+------------+--------------+ | 4| 5| Charmander| [Fire]| | 5| 6| Charmeleon| [Fire]| | 6| null| Charizard|[Fire, Flying]| +----------+--------------------+------------+--------------+ We compute a DataFrame that for each pokemon.id gives the attributes of its highest level of evolution >>> ascending_forest_traversal ( input_df , \"pokemon.id\" , \"pokemon.evolve_to_id\" ) . orderBy ( \"`pokemon.id`\" ) . show () +----------+--------------------+ |pokemon.id|pokemon.evolve_to_id| +----------+--------------------+ | 4| 6| | 5| 6| | 6| 6| +----------+--------------------+ With the keep_label option extra joins are performed at the end of the algorithm to add two struct columns containing the corresponding row for the original node and the furthest ancestor. >>> ascending_forest_traversal ( input_df , \"pokemon.id\" , \"pokemon.evolve_to_id\" , keep_labels = True ... ) . orderBy ( \"`pokemon.id`\" ) . show ( 10 , False ) +----------+--------------------+------------------------------------+------------------------------------+ |pokemon.id|pokemon.evolve_to_id|node |furthest_ancestor | +----------+--------------------+------------------------------------+------------------------------------+ |4 |6 |{4, 5, Charmander, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |5 |6 |{5, 6, Charmeleon, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |6 |6 |{6, null, Charizard, [Fire, Flying]}|{6, null, Charizard, [Fire, Flying]}| +----------+--------------------+------------------------------------+------------------------------------+ Cycle Protection: to prevent the algorithm from looping indefinitely, cycles are detected, and the nodes that are part of cycles will end up with a NULL value as their furthest ancestor >>> input_df = spark . sql ( ''' ... SELECT ... col1 as `node_id`, ... col2 as `parent_id` ... FROM VALUES (1, 2), (2, 3), (3, 1) ... ''' ) >>> input_df . show () +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| 2| | 2| 3| | 3| 1| +-------+---------+ >>> ascending_forest_traversal ( input_df , \"node_id\" , \"parent_id\" ) . orderBy ( \"node_id\" ) . show () +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| null| | 2| null| | 3| null| +-------+---------+ Source code in spark_frame/graph_impl/ascending_forest_traversal.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 def ascending_forest_traversal ( input_df : DataFrame , node_id : str , parent_id : str , keep_labels : bool = False ) -> DataFrame : \"\"\"Given a DataFrame representing a labeled forest with columns `id`, `parent_id` and other label columns, performs a graph traversal that will return a DataFrame with the same schema that gives for each node the labels of it's furthest ancestor. In the input DataFrame, a node is considered to have no parent if its parent_id is null or equal to its node_id. In the output DataFrame, a node that has no parent will have its parent_id equal to its node_id. Cycle protection: If the graph contains any cycle, the nodes in that cycle will have a NULL parent_id. It has a security against dependency cycles, but no security preventing a combinatorial explosion if some nodes have more than one parent. Args: input_df: A Spark DataFrame node_id: Name of the column that represent the node's ids parent_id: Name of the column that represent the parent node's ids keep_labels: If set to true, add two structs column called \"node\" and \"furthest_ancestor\" containing the content of the row from the input DataFrame for the corresponding nodes and their furthest ancestor Returns: A DataFrame with two columns named according to `node_id` and `parent_id` that gives for each node the id of it's furthest ancestor (in the `parent_id` column). If the option `keep_labels` is used, two extra columns of type STRUCT are a added to the output DataFrame, they represent the content of the rows in the input DataFrame corresponding to the node and its furthest ancestor, respectively. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() Given a DataFrame with pokemon attributes and evolution links >>> input_df = spark.sql(''' ... SELECT ... col1 as `pokemon.id`, ... col2 as `pokemon.evolve_to_id`, ... col3 as `pokemon.name`, ... col4 as `pokemon.types` ... FROM VALUES ... (4, 5, 'Charmander', ARRAY('Fire')), ... (5, 6, 'Charmeleon', ARRAY('Fire')), ... (6, NULL, 'Charizard', ARRAY('Fire', 'Flying')) ... ''') >>> input_df.show() +----------+--------------------+------------+--------------+ |pokemon.id|pokemon.evolve_to_id|pokemon.name| pokemon.types| +----------+--------------------+------------+--------------+ | 4| 5| Charmander| [Fire]| | 5| 6| Charmeleon| [Fire]| | 6| null| Charizard|[Fire, Flying]| +----------+--------------------+------------+--------------+ <BLANKLINE> We compute a DataFrame that for each pokemon.id gives the attributes of its highest level of evolution >>> ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\").orderBy(\"`pokemon.id`\").show() +----------+--------------------+ |pokemon.id|pokemon.evolve_to_id| +----------+--------------------+ | 4| 6| | 5| 6| | 6| 6| +----------+--------------------+ <BLANKLINE> With the `keep_label` option extra joins are performed at the end of the algorithm to add two struct columns containing the corresponding row for the original node and the furthest ancestor. >>> ascending_forest_traversal(input_df, \"pokemon.id\", \"pokemon.evolve_to_id\", keep_labels=True ... ).orderBy(\"`pokemon.id`\").show(10, False) +----------+--------------------+------------------------------------+------------------------------------+ |pokemon.id|pokemon.evolve_to_id|node |furthest_ancestor | +----------+--------------------+------------------------------------+------------------------------------+ |4 |6 |{4, 5, Charmander, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |5 |6 |{5, 6, Charmeleon, [Fire]} |{6, null, Charizard, [Fire, Flying]}| |6 |6 |{6, null, Charizard, [Fire, Flying]}|{6, null, Charizard, [Fire, Flying]}| +----------+--------------------+------------------------------------+------------------------------------+ <BLANKLINE> *Cycle Protection:* to prevent the algorithm from looping indefinitely, cycles are detected, and the nodes that are part of cycles will end up with a NULL value as their furthest ancestor >>> input_df = spark.sql(''' ... SELECT ... col1 as `node_id`, ... col2 as `parent_id` ... FROM VALUES (1, 2), (2, 3), (3, 1) ... ''') >>> input_df.show() +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| 2| | 2| 3| | 3| 1| +-------+---------+ <BLANKLINE> >>> ascending_forest_traversal(input_df, \"node_id\", \"parent_id\").orderBy(\"node_id\").show() +-------+---------+ |node_id|parent_id| +-------+---------+ | 1| null| | 2| null| | 3| null| +-------+---------+ <BLANKLINE> \"\"\" assert_true ( node_id in input_df . columns , \"Could not find column %s in Dataframe's columns: %s \" % ( node_id , input_df . columns ) ) assert_true ( parent_id in input_df . columns , \"Could not find column %s in Dataframe's columns: %s \" % ( parent_id , input_df . columns ), ) node_id_col_name = \"node_id\" parent_id_col_name = \"parent_id\" status_col_name = \"status\" df = input_df . select ( f . col ( quote ( node_id )) . alias ( node_id_col_name ), f . col ( quote ( parent_id )) . alias ( parent_id_col_name ) ) res_df = _ascending_forest_traversal ( df , node_id_col_name = node_id_col_name , parent_id_col_name = parent_id_col_name , status_col_name = status_col_name ) res_df = res_df . select ( f . col ( node_id_col_name ) . alias ( node_id ), f . col ( parent_id_col_name ) . alias ( parent_id ), ) if keep_labels : res_df = res_df . join ( input_df , node_id ) . select ( res_df [ \"*\" ], f . struct ( * [ input_df [ quote ( col )] for col in input_df . columns ]) . alias ( \"node\" ), ) res_df = res_df . join ( input_df , res_df [ quote ( parent_id )] == input_df [ quote ( node_id )]) . select ( res_df [ quote ( node_id )], res_df [ quote ( parent_id )], res_df [ \"node\" ], f . struct ( * [ input_df [ quote ( col )] for col in input_df . columns ]) . alias ( \"furthest_ancestor\" ), ) return res_df","title":"ascending_forest_traversal()"},{"location":"reference/nested/","text":"Please read this before using the spark_frame.nested module The spark_frame.nested module contains several methods that make the manipulation of deeply nested data structures much easier. Before diving into it, it is important to explicit the concept of Field in the context of this library. First, let's distinguish the notion of Column and Field . Both terms are already used in Spark, but we chose here to make the following distinction: A Column is a root-level column of a DataFrame. A Field is any column or sub-column inside a struct of the DataFrame. Example: let's consider the following DataFrame >>> from spark_frame.examples.reference_nested import _get_sample_data >>> df = _get_sample_data () >>> df . show ( truncate = False ) # noqa: E501 +---+-----------------------+---------------+ |id |name |types | +---+-----------------------+---------------+ |1 |{Bulbasaur, Bulbizarre}|[Grass, Poison]| +---+-----------------------+---------------+ >>> df . printSchema () root |-- id: integer (nullable = false) |-- name: struct (nullable = false) | |-- english: string (nullable = false) | |-- french: string (nullable = false) |-- types: array (nullable = false) | |-- element: string (containsNull = false) This DataFrame has 3 columns: id name types But it has 4 fields: id name.english name.french types! This can be seen by using the method spark_frame.nested.print_schema >>> from spark_frame import nested >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- name.english: string (nullable = false) |-- name.french: string (nullable = false) |-- types!: string (nullable = false) As we can see, some field names contain dots . or exclamation marks ! , they convey the following meaning: A dot . represents a struct. An exclamation mark ! represents an array. While the dot syntax for structs should feel familiar to users, the exclamation mark ! should feel new. It is used as a repetition marker indicating that this field is repeated. Tip It is important to not forget to use exclamation marks ! when mentionning a field. For instance: types designates the root-level field which is of type ARRAY<STRING> types! designates the elements inside this array In particular, if a field \"my_field\" is of type ARRAY<ARRAY<STRING>> , the innermost elements of the arrays will be designated as \"my_field!!\" with two exclamation marks. Limitation: Do not use dots, exclamation marks or percents in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , exclamation mark ! or percents % . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . print_schema ( df : DataFrame ) -> None Print the DataFrame's flattened schema to the standard output. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) Source code in spark_frame/nested_impl/print_schema.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def print_schema ( df : DataFrame ) -> None : \"\"\"Print the DataFrame's flattened schema to the standard output. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) <BLANKLINE> >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> \"\"\" print ( schema_string ( df )) select ( df : DataFrame , fields : Mapping [ str , ColumnTransformation ]) -> DataFrame Project a set of expressions and returns a new DataFrame . This method is similar to the DataFrame.select method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , ColumnTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame where only the specified field have been selected and the corresponding DataFrame transformations were applied to each of them. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested . select ( df , { ... \"s.a\" : \"s.a\" , # Column name (string) ... \"s.b\" : None , # None: use to keep a column without having to repeat its name ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---------+ | s| +---------+ |{2, 3, 5}| +---------+ Example 2: repeated fields >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) >>> df . show () +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df . transform ( nested . select , { ... \"s!.a\" : lambda s : s [ \"a\" ], ... \"s!.b\" : None , ... \"s!.c\" : lambda s : s [ \"a\" ] + s [ \"b\" ] ... }) . show ( truncate = False ) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . select , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ Example 3: field repeated twice >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) >>> df . show () +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> new_df = df . transform ( nested . select , { ... \"s1!.e!\" : None , ... \"s2!.e!\" : lambda e : e . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) >>> new_df . show () +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) >>> new_df . show () +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/select_impl.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def select ( df : DataFrame , fields : Mapping [ str , ColumnTransformation ]) -> DataFrame : \"\"\"Project a set of expressions and returns a new [DataFrame][pyspark.sql.DataFrame]. This method is similar to the [DataFrame.select][pyspark.sql.DataFrame.select] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame where only the specified field have been selected and the corresponding transformations were applied to each of them. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested.select(df, { ... \"s.a\": \"s.a\", # Column name (string) ... \"s.b\": None, # None: use to keep a column without having to repeat its name ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---------+ | s| +---------+ |{2, 3, 5}| +---------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df.transform(nested.select, { ... \"s!.a\": lambda s: s[\"a\"], ... \"s!.b\": None, ... \"s!.c\": lambda s: s[\"a\"] + s[\"b\"] ... }).show(truncate=False) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.select, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b\": f.lit(2) ... }).show(truncate=False) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> new_df = df.transform(nested.select, { ... \"s1!.e!\": None, ... \"s2!.e!\": lambda e : e.cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) <BLANKLINE> >>> new_df.show() +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" return df . select ( * resolve_nested_fields ( fields , starting_level = df )) schema_string ( df : DataFrame ) -> str Write the DataFrame's flattened schema to a string. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description str a string representing the flattened schema Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) >>> print ( nested . schema_string ( df )) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) Source code in spark_frame/nested_impl/schema_string.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def schema_string ( df : DataFrame ) -> str : \"\"\"Write the DataFrame's flattened schema to a string. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Returns: a string representing the flattened schema Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) <BLANKLINE> >>> print(nested.schema_string(df)) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) <BLANKLINE> \"\"\" flat_schema = flatten_schema ( df . schema , explode = True ) return _flat_schema_to_tree_string ( flat_schema . fields ) unnest_all_fields ( df : DataFrame , keep_columns : Optional [ List [ str ]] = None ) -> List [ DataFrame ] Given a DataFrame, return a list of DataFrames where all arrays have been recursively unnested (a.k.a. exploded). This produce one DataFrame for each possible granularity. For instance, given a DataFrame with the following flattened schema: id s1.a s2!.b s2!.c s2!.s3!.d s4!.e s4!.f This will return four DataFrames containing the following unnested columns id, s1.a s2!.b, s2!.c s2!.s3!.d s4!.e, s4!.f Limitation: Maps are not unnested Fields of type Maps are not unnested by this method. A possible workaround is to first use the transformation spark_frame.transformations.convert_all_maps_to_arrays Parameters: Name Type Description Default df DataFrame A Spark DataFrame required keep_columns Optional [ List [ str ]] Names of columns that should be kept while unnesting None Returns: Type Description List [ DataFrame ] A list of DataFrames Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... STRUCT(2 as a) as s1, ... ARRAY(STRUCT(3 as b, 4 as c, ARRAY(STRUCT(5 as d), STRUCT(6 as d)) as s3)) as s2, ... ARRAY(STRUCT(7 as e, 8 as f), STRUCT(9 as e, 10 as f)) as s4 ... ''' ) >>> df . show ( truncate = False ) +---+---+--------------------+-----------------+ |id |s1 |s2 |s4 | +---+---+--------------------+-----------------+ |1 |{2}|[{3, 4, [{5}, {6}]}]|[{7, 8}, {9, 10}]| +---+---+--------------------+-----------------+ >>> nested . fields ( df ) ['id', 's1.a', 's2!.b', 's2!.c', 's2!.s3!.d', 's4!.e', 's4!.f'] >>> result_df_list = nested . unnest_all_fields ( df , keep_columns = [ \"id\" ]) >>> for result_df in result_df_list : result_df . show () +---+----+ | id|s1.a| +---+----+ | 1| 2| +---+----+ +---+-----+-----+ | id|s2!.b|s2!.c| +---+-----+-----+ | 1| 3| 4| +---+-----+-----+ +---+---------+ | id|s2!.s3!.d| +---+---------+ | 1| 5| | 1| 6| +---+---------+ +---+-----+-----+ | id|s4!.e|s4!.f| +---+-----+-----+ | 1| 7| 8| | 1| 9| 10| +---+-----+-----+ Source code in spark_frame/nested_impl/unnest_all_fields.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def unnest_all_fields ( df : DataFrame , keep_columns : Optional [ List [ str ]] = None ) -> List [ DataFrame ]: \"\"\"Given a DataFrame, return a list of DataFrames where all arrays have been recursively unnested (a.k.a. exploded). This produce one DataFrame for each possible granularity. For instance, given a DataFrame with the following flattened schema: id s1.a s2!.b s2!.c s2!.s3!.d s4!.e s4!.f This will return four DataFrames containing the following unnested columns: - id, s1.a - s2!.b, s2!.c - s2!.s3!.d - s4!.e, s4!.f !!! warning \"Limitation: Maps are not unnested\" - Fields of type Maps are not unnested by this method. - A possible workaround is to first use the transformation [`spark_frame.transformations.convert_all_maps_to_arrays`] [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays] Args: df: A Spark DataFrame keep_columns: Names of columns that should be kept while unnesting Returns: A list of DataFrames Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql(''' ... SELECT ... 1 as id, ... STRUCT(2 as a) as s1, ... ARRAY(STRUCT(3 as b, 4 as c, ARRAY(STRUCT(5 as d), STRUCT(6 as d)) as s3)) as s2, ... ARRAY(STRUCT(7 as e, 8 as f), STRUCT(9 as e, 10 as f)) as s4 ... ''') >>> df.show(truncate=False) +---+---+--------------------+-----------------+ |id |s1 |s2 |s4 | +---+---+--------------------+-----------------+ |1 |{2}|[{3, 4, [{5}, {6}]}]|[{7, 8}, {9, 10}]| +---+---+--------------------+-----------------+ <BLANKLINE> >>> nested.fields(df) ['id', 's1.a', 's2!.b', 's2!.c', 's2!.s3!.d', 's4!.e', 's4!.f'] >>> result_df_list = nested.unnest_all_fields(df, keep_columns=[\"id\"]) >>> for result_df in result_df_list: result_df.show() +---+----+ | id|s1.a| +---+----+ | 1| 2| +---+----+ <BLANKLINE> +---+-----+-----+ | id|s2!.b|s2!.c| +---+-----+-----+ | 1| 3| 4| +---+-----+-----+ <BLANKLINE> +---+---------+ | id|s2!.s3!.d| +---+---------+ | 1| 5| | 1| 6| +---+---------+ <BLANKLINE> +---+-----+-----+ | id|s4!.e|s4!.f| +---+-----+-----+ | 1| 7| 8| | 1| 9| 10| +---+-----+-----+ <BLANKLINE> \"\"\" if keep_columns is None : keep_columns = [] fields_to_unnest = [ field for field in nested . fields ( df ) if not is_sub_field_of_any ( field , keep_columns )] return unnest_fields ( df , fields_to_unnest , keep_columns = keep_columns ) unnest_field ( df : DataFrame , field_name : str , keep_columns : Optional [ List [ str ]] = None ) -> DataFrame Given a DataFrame, return a new DataFrame where the specified column has been recursively unnested (a.k.a. exploded). Limitation: Maps are not unnested Fields of type Maps are not unnested by this method. A possible workaround is to first use the transformation spark_frame.transformations.convert_all_maps_to_arrays Parameters: Name Type Description Default df DataFrame A Spark DataFrame required field_name str The name of a nested column to unnest required keep_columns Optional [ List [ str ]] List of column names to keep while unnesting None Returns: Type Description DataFrame A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as arr ... ''' ) >>> df . show ( truncate = False ) +---+----------------+ |id |arr | +---+----------------+ |1 |[[1, 2], [3, 4]]| +---+----------------+ >>> nested . fields ( df ) ['id', 'arr!!'] >>> nested . unnest_field ( df , 'arr!' ) . show ( truncate = False ) +------+ |arr! | +------+ |[1, 2]| |[3, 4]| +------+ >>> nested . unnest_field ( df , 'arr!!' ) . show ( truncate = False ) +-----+ |arr!!| +-----+ |1 | |2 | |3 | |4 | +-----+ >>> nested . unnest_field ( df , 'arr!!' , keep_columns = [ \"id\" ]) . show ( truncate = False ) +---+-----+ |id |arr!!| +---+-----+ |1 |1 | |1 |2 | |1 |3 | |1 |4 | +---+-----+ >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(ARRAY(STRUCT(\"a1\" as a, \"b1\" as b), STRUCT(\"a2\" as a, \"b1\" as b)) as s2), ... STRUCT(ARRAY(STRUCT(\"a3\" as a, \"b3\" as b)) as s2) ... ) as s1 ... ''' ) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{[{a1, b1}, {a2, b1}]}, {[{a3, b3}]}]| +---+--------------------------------------+ >>> nested . fields ( df ) ['id', 's1!.s2!.a', 's1!.s2!.b'] >>> nested . unnest_field ( df , 's1!.s2!' ) . show ( truncate = False ) +--------+ |s1!.s2! | +--------+ |{a1, b1}| |{a2, b1}| |{a3, b3}| +--------+ Source code in spark_frame/nested_impl/unnest_field.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def unnest_field ( df : DataFrame , field_name : str , keep_columns : Optional [ List [ str ]] = None ) -> DataFrame : \"\"\"Given a DataFrame, return a new DataFrame where the specified column has been recursively unnested (a.k.a. exploded). !!! warning \"Limitation: Maps are not unnested\" - Fields of type Maps are not unnested by this method. - A possible workaround is to first use the transformation [`spark_frame.transformations.convert_all_maps_to_arrays`] [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays] Args: df: A Spark DataFrame field_name: The name of a nested column to unnest keep_columns: List of column names to keep while unnesting Returns: A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as arr ... ''') >>> df.show(truncate=False) +---+----------------+ |id |arr | +---+----------------+ |1 |[[1, 2], [3, 4]]| +---+----------------+ <BLANKLINE> >>> nested.fields(df) ['id', 'arr!!'] >>> nested.unnest_field(df, 'arr!').show(truncate=False) +------+ |arr! | +------+ |[1, 2]| |[3, 4]| +------+ <BLANKLINE> >>> nested.unnest_field(df, 'arr!!').show(truncate=False) +-----+ |arr!!| +-----+ |1 | |2 | |3 | |4 | +-----+ <BLANKLINE> >>> nested.unnest_field(df, 'arr!!', keep_columns=[\"id\"]).show(truncate=False) +---+-----+ |id |arr!!| +---+-----+ |1 |1 | |1 |2 | |1 |3 | |1 |4 | +---+-----+ <BLANKLINE> >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(ARRAY(STRUCT(\"a1\" as a, \"b1\" as b), STRUCT(\"a2\" as a, \"b1\" as b)) as s2), ... STRUCT(ARRAY(STRUCT(\"a3\" as a, \"b3\" as b)) as s2) ... ) as s1 ... ''') >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{[{a1, b1}, {a2, b1}]}, {[{a3, b3}]}]| +---+--------------------------------------+ <BLANKLINE> >>> nested.fields(df) ['id', 's1!.s2!.a', 's1!.s2!.b'] >>> nested.unnest_field(df, 's1!.s2!').show(truncate=False) +--------+ |s1!.s2! | +--------+ |{a1, b1}| |{a2, b1}| |{a3, b3}| +--------+ <BLANKLINE> \"\"\" if keep_columns is None : keep_columns = [] return unnest_fields ( df , field_name , keep_columns = keep_columns )[ 0 ] with_fields ( df : DataFrame , fields : Mapping [ str , AnyKindOfTransformation ]) -> DataFrame Return a new DataFrame by adding or replacing (when they already exist) columns. This method is similar to the DataFrame.withColumn method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , AnyKindOfTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been DataFrame applied to the corresponding fields. If a field name did not exist in the input DataFrame, DataFrame it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested . with_fields ( df , { ... \"s.id\" : \"id\" , # column name (string) ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ Example 2: repeated fields >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) >>> df . show () +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df . transform ( nested . with_fields , { ... \"s!.b.d\" : lambda s : s [ \"a\" ] + s [ \"b\" ][ \"c\" ]} ... ) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) >>> new_df . show ( truncate = False ) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b.c\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ Example 3: field repeated twice >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) >>> df . show () +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> df . transform ( nested . with_fields , { \"s!.e!\" : lambda e : e . cast ( \"DOUBLE\" )}) . show () +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . with_fields , { ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) >>> new_df . show () +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/with_fields.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def with_fields ( df : DataFrame , fields : Mapping [ str , AnyKindOfTransformation ]) -> DataFrame : \"\"\"Return a new [DataFrame][pyspark.sql.DataFrame] by adding or replacing (when they already exist) columns. This method is similar to the [DataFrame.withColumn][pyspark.sql.DataFrame.withColumn] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been applied to the corresponding fields. If a field name did not exist in the input DataFrame, it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested.with_fields(df, { ... \"s.id\": \"id\", # column name (string) ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show() +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df.transform(nested.with_fields, { ... \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]} ... ) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) <BLANKLINE> >>> new_df.show(truncate=False) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.with_fields, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b.c\": f.lit(2) ... }).show(truncate=False) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show() +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.with_fields, { ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" default_columns = { field : None for field in nested . fields ( df )} fields = { ** default_columns , ** fields } return df . select ( * resolve_nested_fields ( fields , starting_level = df ))","title":"spark_frame.nested"},{"location":"reference/nested/#please-read-this-before-using-the-spark_framenested-module","text":"The spark_frame.nested module contains several methods that make the manipulation of deeply nested data structures much easier. Before diving into it, it is important to explicit the concept of Field in the context of this library. First, let's distinguish the notion of Column and Field . Both terms are already used in Spark, but we chose here to make the following distinction: A Column is a root-level column of a DataFrame. A Field is any column or sub-column inside a struct of the DataFrame. Example: let's consider the following DataFrame >>> from spark_frame.examples.reference_nested import _get_sample_data >>> df = _get_sample_data () >>> df . show ( truncate = False ) # noqa: E501 +---+-----------------------+---------------+ |id |name |types | +---+-----------------------+---------------+ |1 |{Bulbasaur, Bulbizarre}|[Grass, Poison]| +---+-----------------------+---------------+ >>> df . printSchema () root |-- id: integer (nullable = false) |-- name: struct (nullable = false) | |-- english: string (nullable = false) | |-- french: string (nullable = false) |-- types: array (nullable = false) | |-- element: string (containsNull = false) This DataFrame has 3 columns: id name types But it has 4 fields: id name.english name.french types! This can be seen by using the method spark_frame.nested.print_schema >>> from spark_frame import nested >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- name.english: string (nullable = false) |-- name.french: string (nullable = false) |-- types!: string (nullable = false) As we can see, some field names contain dots . or exclamation marks ! , they convey the following meaning: A dot . represents a struct. An exclamation mark ! represents an array. While the dot syntax for structs should feel familiar to users, the exclamation mark ! should feel new. It is used as a repetition marker indicating that this field is repeated. Tip It is important to not forget to use exclamation marks ! when mentionning a field. For instance: types designates the root-level field which is of type ARRAY<STRING> types! designates the elements inside this array In particular, if a field \"my_field\" is of type ARRAY<ARRAY<STRING>> , the innermost elements of the arrays will be designated as \"my_field!!\" with two exclamation marks. Limitation: Do not use dots, exclamation marks or percents in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , exclamation mark ! or percents % . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names .","title":"Please read this before using the spark_frame.nested module"},{"location":"reference/nested/#spark_frame.nested_impl.print_schema.print_schema","text":"Print the DataFrame's flattened schema to the standard output. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) Source code in spark_frame/nested_impl/print_schema.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def print_schema ( df : DataFrame ) -> None : \"\"\"Print the DataFrame's flattened schema to the standard output. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) <BLANKLINE> >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> \"\"\" print ( schema_string ( df ))","title":"print_schema()"},{"location":"reference/nested/#spark_frame.nested_impl.select_impl.select","text":"Project a set of expressions and returns a new DataFrame . This method is similar to the DataFrame.select method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , ColumnTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame where only the specified field have been selected and the corresponding DataFrame transformations were applied to each of them. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested . select ( df , { ... \"s.a\" : \"s.a\" , # Column name (string) ... \"s.b\" : None , # None: use to keep a column without having to repeat its name ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---------+ | s| +---------+ |{2, 3, 5}| +---------+ Example 2: repeated fields >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) >>> df . show () +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df . transform ( nested . select , { ... \"s!.a\" : lambda s : s [ \"a\" ], ... \"s!.b\" : None , ... \"s!.c\" : lambda s : s [ \"a\" ] + s [ \"b\" ] ... }) . show ( truncate = False ) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . select , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ Example 3: field repeated twice >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) >>> df . show () +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> new_df = df . transform ( nested . select , { ... \"s1!.e!\" : None , ... \"s2!.e!\" : lambda e : e . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) >>> new_df . show () +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) >>> new_df . show () +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/select_impl.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def select ( df : DataFrame , fields : Mapping [ str , ColumnTransformation ]) -> DataFrame : \"\"\"Project a set of expressions and returns a new [DataFrame][pyspark.sql.DataFrame]. This method is similar to the [DataFrame.select][pyspark.sql.DataFrame.select] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame where only the specified field have been selected and the corresponding transformations were applied to each of them. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested.select(df, { ... \"s.a\": \"s.a\", # Column name (string) ... \"s.b\": None, # None: use to keep a column without having to repeat its name ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---------+ | s| +---------+ |{2, 3, 5}| +---------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df.transform(nested.select, { ... \"s!.a\": lambda s: s[\"a\"], ... \"s!.b\": None, ... \"s!.c\": lambda s: s[\"a\"] + s[\"b\"] ... }).show(truncate=False) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.select, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b\": f.lit(2) ... }).show(truncate=False) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> new_df = df.transform(nested.select, { ... \"s1!.e!\": None, ... \"s2!.e!\": lambda e : e.cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) <BLANKLINE> >>> new_df.show() +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" return df . select ( * resolve_nested_fields ( fields , starting_level = df ))","title":"select()"},{"location":"reference/nested/#spark_frame.nested_impl.schema_string.schema_string","text":"Write the DataFrame's flattened schema to a string. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description str a string representing the flattened schema Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) >>> print ( nested . schema_string ( df )) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) Source code in spark_frame/nested_impl/schema_string.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def schema_string ( df : DataFrame ) -> str : \"\"\"Write the DataFrame's flattened schema to a string. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Returns: a string representing the flattened schema Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) <BLANKLINE> >>> print(nested.schema_string(df)) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) <BLANKLINE> \"\"\" flat_schema = flatten_schema ( df . schema , explode = True ) return _flat_schema_to_tree_string ( flat_schema . fields )","title":"schema_string()"},{"location":"reference/nested/#spark_frame.nested_impl.unnest_all_fields.unnest_all_fields","text":"Given a DataFrame, return a list of DataFrames where all arrays have been recursively unnested (a.k.a. exploded). This produce one DataFrame for each possible granularity. For instance, given a DataFrame with the following flattened schema: id s1.a s2!.b s2!.c s2!.s3!.d s4!.e s4!.f This will return four DataFrames containing the following unnested columns id, s1.a s2!.b, s2!.c s2!.s3!.d s4!.e, s4!.f Limitation: Maps are not unnested Fields of type Maps are not unnested by this method. A possible workaround is to first use the transformation spark_frame.transformations.convert_all_maps_to_arrays Parameters: Name Type Description Default df DataFrame A Spark DataFrame required keep_columns Optional [ List [ str ]] Names of columns that should be kept while unnesting None Returns: Type Description List [ DataFrame ] A list of DataFrames Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... STRUCT(2 as a) as s1, ... ARRAY(STRUCT(3 as b, 4 as c, ARRAY(STRUCT(5 as d), STRUCT(6 as d)) as s3)) as s2, ... ARRAY(STRUCT(7 as e, 8 as f), STRUCT(9 as e, 10 as f)) as s4 ... ''' ) >>> df . show ( truncate = False ) +---+---+--------------------+-----------------+ |id |s1 |s2 |s4 | +---+---+--------------------+-----------------+ |1 |{2}|[{3, 4, [{5}, {6}]}]|[{7, 8}, {9, 10}]| +---+---+--------------------+-----------------+ >>> nested . fields ( df ) ['id', 's1.a', 's2!.b', 's2!.c', 's2!.s3!.d', 's4!.e', 's4!.f'] >>> result_df_list = nested . unnest_all_fields ( df , keep_columns = [ \"id\" ]) >>> for result_df in result_df_list : result_df . show () +---+----+ | id|s1.a| +---+----+ | 1| 2| +---+----+ +---+-----+-----+ | id|s2!.b|s2!.c| +---+-----+-----+ | 1| 3| 4| +---+-----+-----+ +---+---------+ | id|s2!.s3!.d| +---+---------+ | 1| 5| | 1| 6| +---+---------+ +---+-----+-----+ | id|s4!.e|s4!.f| +---+-----+-----+ | 1| 7| 8| | 1| 9| 10| +---+-----+-----+ Source code in spark_frame/nested_impl/unnest_all_fields.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def unnest_all_fields ( df : DataFrame , keep_columns : Optional [ List [ str ]] = None ) -> List [ DataFrame ]: \"\"\"Given a DataFrame, return a list of DataFrames where all arrays have been recursively unnested (a.k.a. exploded). This produce one DataFrame for each possible granularity. For instance, given a DataFrame with the following flattened schema: id s1.a s2!.b s2!.c s2!.s3!.d s4!.e s4!.f This will return four DataFrames containing the following unnested columns: - id, s1.a - s2!.b, s2!.c - s2!.s3!.d - s4!.e, s4!.f !!! warning \"Limitation: Maps are not unnested\" - Fields of type Maps are not unnested by this method. - A possible workaround is to first use the transformation [`spark_frame.transformations.convert_all_maps_to_arrays`] [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays] Args: df: A Spark DataFrame keep_columns: Names of columns that should be kept while unnesting Returns: A list of DataFrames Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql(''' ... SELECT ... 1 as id, ... STRUCT(2 as a) as s1, ... ARRAY(STRUCT(3 as b, 4 as c, ARRAY(STRUCT(5 as d), STRUCT(6 as d)) as s3)) as s2, ... ARRAY(STRUCT(7 as e, 8 as f), STRUCT(9 as e, 10 as f)) as s4 ... ''') >>> df.show(truncate=False) +---+---+--------------------+-----------------+ |id |s1 |s2 |s4 | +---+---+--------------------+-----------------+ |1 |{2}|[{3, 4, [{5}, {6}]}]|[{7, 8}, {9, 10}]| +---+---+--------------------+-----------------+ <BLANKLINE> >>> nested.fields(df) ['id', 's1.a', 's2!.b', 's2!.c', 's2!.s3!.d', 's4!.e', 's4!.f'] >>> result_df_list = nested.unnest_all_fields(df, keep_columns=[\"id\"]) >>> for result_df in result_df_list: result_df.show() +---+----+ | id|s1.a| +---+----+ | 1| 2| +---+----+ <BLANKLINE> +---+-----+-----+ | id|s2!.b|s2!.c| +---+-----+-----+ | 1| 3| 4| +---+-----+-----+ <BLANKLINE> +---+---------+ | id|s2!.s3!.d| +---+---------+ | 1| 5| | 1| 6| +---+---------+ <BLANKLINE> +---+-----+-----+ | id|s4!.e|s4!.f| +---+-----+-----+ | 1| 7| 8| | 1| 9| 10| +---+-----+-----+ <BLANKLINE> \"\"\" if keep_columns is None : keep_columns = [] fields_to_unnest = [ field for field in nested . fields ( df ) if not is_sub_field_of_any ( field , keep_columns )] return unnest_fields ( df , fields_to_unnest , keep_columns = keep_columns )","title":"unnest_all_fields()"},{"location":"reference/nested/#spark_frame.nested_impl.unnest_field.unnest_field","text":"Given a DataFrame, return a new DataFrame where the specified column has been recursively unnested (a.k.a. exploded). Limitation: Maps are not unnested Fields of type Maps are not unnested by this method. A possible workaround is to first use the transformation spark_frame.transformations.convert_all_maps_to_arrays Parameters: Name Type Description Default df DataFrame A Spark DataFrame required field_name str The name of a nested column to unnest required keep_columns Optional [ List [ str ]] List of column names to keep while unnesting None Returns: Type Description DataFrame A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as arr ... ''' ) >>> df . show ( truncate = False ) +---+----------------+ |id |arr | +---+----------------+ |1 |[[1, 2], [3, 4]]| +---+----------------+ >>> nested . fields ( df ) ['id', 'arr!!'] >>> nested . unnest_field ( df , 'arr!' ) . show ( truncate = False ) +------+ |arr! | +------+ |[1, 2]| |[3, 4]| +------+ >>> nested . unnest_field ( df , 'arr!!' ) . show ( truncate = False ) +-----+ |arr!!| +-----+ |1 | |2 | |3 | |4 | +-----+ >>> nested . unnest_field ( df , 'arr!!' , keep_columns = [ \"id\" ]) . show ( truncate = False ) +---+-----+ |id |arr!!| +---+-----+ |1 |1 | |1 |2 | |1 |3 | |1 |4 | +---+-----+ >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(ARRAY(STRUCT(\"a1\" as a, \"b1\" as b), STRUCT(\"a2\" as a, \"b1\" as b)) as s2), ... STRUCT(ARRAY(STRUCT(\"a3\" as a, \"b3\" as b)) as s2) ... ) as s1 ... ''' ) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{[{a1, b1}, {a2, b1}]}, {[{a3, b3}]}]| +---+--------------------------------------+ >>> nested . fields ( df ) ['id', 's1!.s2!.a', 's1!.s2!.b'] >>> nested . unnest_field ( df , 's1!.s2!' ) . show ( truncate = False ) +--------+ |s1!.s2! | +--------+ |{a1, b1}| |{a2, b1}| |{a3, b3}| +--------+ Source code in spark_frame/nested_impl/unnest_field.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def unnest_field ( df : DataFrame , field_name : str , keep_columns : Optional [ List [ str ]] = None ) -> DataFrame : \"\"\"Given a DataFrame, return a new DataFrame where the specified column has been recursively unnested (a.k.a. exploded). !!! warning \"Limitation: Maps are not unnested\" - Fields of type Maps are not unnested by this method. - A possible workaround is to first use the transformation [`spark_frame.transformations.convert_all_maps_to_arrays`] [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays] Args: df: A Spark DataFrame field_name: The name of a nested column to unnest keep_columns: List of column names to keep while unnesting Returns: A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as arr ... ''') >>> df.show(truncate=False) +---+----------------+ |id |arr | +---+----------------+ |1 |[[1, 2], [3, 4]]| +---+----------------+ <BLANKLINE> >>> nested.fields(df) ['id', 'arr!!'] >>> nested.unnest_field(df, 'arr!').show(truncate=False) +------+ |arr! | +------+ |[1, 2]| |[3, 4]| +------+ <BLANKLINE> >>> nested.unnest_field(df, 'arr!!').show(truncate=False) +-----+ |arr!!| +-----+ |1 | |2 | |3 | |4 | +-----+ <BLANKLINE> >>> nested.unnest_field(df, 'arr!!', keep_columns=[\"id\"]).show(truncate=False) +---+-----+ |id |arr!!| +---+-----+ |1 |1 | |1 |2 | |1 |3 | |1 |4 | +---+-----+ <BLANKLINE> >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(ARRAY(STRUCT(\"a1\" as a, \"b1\" as b), STRUCT(\"a2\" as a, \"b1\" as b)) as s2), ... STRUCT(ARRAY(STRUCT(\"a3\" as a, \"b3\" as b)) as s2) ... ) as s1 ... ''') >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{[{a1, b1}, {a2, b1}]}, {[{a3, b3}]}]| +---+--------------------------------------+ <BLANKLINE> >>> nested.fields(df) ['id', 's1!.s2!.a', 's1!.s2!.b'] >>> nested.unnest_field(df, 's1!.s2!').show(truncate=False) +--------+ |s1!.s2! | +--------+ |{a1, b1}| |{a2, b1}| |{a3, b3}| +--------+ <BLANKLINE> \"\"\" if keep_columns is None : keep_columns = [] return unnest_fields ( df , field_name , keep_columns = keep_columns )[ 0 ]","title":"unnest_field()"},{"location":"reference/nested/#spark_frame.nested_impl.with_fields.with_fields","text":"Return a new DataFrame by adding or replacing (when they already exist) columns. This method is similar to the DataFrame.withColumn method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , AnyKindOfTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been DataFrame applied to the corresponding fields. If a field name did not exist in the input DataFrame, DataFrame it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested . with_fields ( df , { ... \"s.id\" : \"id\" , # column name (string) ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ Example 2: repeated fields >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) >>> df . show () +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df . transform ( nested . with_fields , { ... \"s!.b.d\" : lambda s : s [ \"a\" ] + s [ \"b\" ][ \"c\" ]} ... ) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) >>> new_df . show ( truncate = False ) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b.c\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ Example 3: field repeated twice >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) >>> df . show () +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> df . transform ( nested . with_fields , { \"s!.e!\" : lambda e : e . cast ( \"DOUBLE\" )}) . show () +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . with_fields , { ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) >>> new_df . show () +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/with_fields.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def with_fields ( df : DataFrame , fields : Mapping [ str , AnyKindOfTransformation ]) -> DataFrame : \"\"\"Return a new [DataFrame][pyspark.sql.DataFrame] by adding or replacing (when they already exist) columns. This method is similar to the [DataFrame.withColumn][pyspark.sql.DataFrame.withColumn] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been applied to the corresponding fields. If a field name did not exist in the input DataFrame, it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested.with_fields(df, { ... \"s.id\": \"id\", # column name (string) ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show() +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df.transform(nested.with_fields, { ... \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]} ... ) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) <BLANKLINE> >>> new_df.show(truncate=False) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.with_fields, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b.c\": f.lit(2) ... }).show(truncate=False) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show() +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.with_fields, { ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" default_columns = { field : None for field in nested . fields ( df )} fields = { ** default_columns , ** fields } return df . select ( * resolve_nested_fields ( fields , starting_level = df ))","title":"with_fields()"},{"location":"reference/nested_functions/","text":"Like with pyspark.sql.functions , the methods in this module all return Column expressions and can be used to build operations on Spark DataFrames using select , withColumn , etc. aggregate ( field_name : str , initial_value : StringOrColumn , merge : Callable [[ Column , Column ], Column ], start : Optional [ Callable [[ Column ], Column ]] = None , finish : Optional [ Callable [[ Column ], Column ]] = None , starting_level : Union [ Column , DataFrame , None ] = None ) -> Column Recursively compute an aggregation of all elements in the given repeated field. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default field_name str Name of the repeated field to sum. It may be repeated multiple times. required initial_value StringOrColumn Name of column or Column expression. required merge Callable [[ Column , Column ], Column ] A binary function (acc: Column, x: Column[) -> Column returning an expression of the same type as initial_value . required start Optional [ Callable [[ Column ], Column ]] An optional unary function (x: Column) -> Column that transforms the values to aggregate into the same type as initial_value . None finish Optional [ Callable [[ Column ], Column ]] An optional unary function (x: Column) -> Column used to convert accumulated value into the final result. None starting_level Union [ Column , DataFrame , None] Nesting level from which the aggregation is started None Returns: Type Description Column A Column expression Examples: Example 1 >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data () >>> nested . print_schema ( employee_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> employee_df . withColumn ( \"projects\" , f . to_json ( \"projects.tasks\" )) . show ( truncate = False ) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"projects!.tasks!.estimate\" : None ... }) . show ( truncate = False ) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ >>> employee_df . transform ( ... nested . select , ... { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"total_task_estimate\" : nf . aggregate ( ... field_name = \"projects!.tasks!.estimate\" , ... initial_value = f . lit ( 0 ) . cast ( \"BIGINT\" ), ... merge = lambda acc , x : acc + x ... ), ... \"projects!.task_estimate_per_project\" : lambda project : nf . aggregate ( ... field_name = \"tasks!.estimate\" , ... initial_value = f . lit ( 0 ) . cast ( \"BIGINT\" ), ... merge = lambda acc , x : acc + x , ... starting_level = project , ... ), ... }, ... ) . show ( truncate = False ) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ Source code in spark_frame/nested_functions_impl/aggregate.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def aggregate ( field_name : str , initial_value : StringOrColumn , merge : Callable [[ Column , Column ], Column ], start : Optional [ Callable [[ Column ], Column ]] = None , finish : Optional [ Callable [[ Column ], Column ]] = None , starting_level : Union [ Column , DataFrame , None ] = None , ) -> Column : \"\"\"Recursively compute an aggregation of all elements in the given repeated field. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: field_name: Name of the repeated field to sum. It may be repeated multiple times. initial_value: Name of column or Column expression. merge: A binary function `(acc: Column, x: Column[) -> Column` returning an expression of the same type as `initial_value`. start: An optional unary function `(x: Column) -> Column` that transforms the values to aggregate into the same type as `initial_value`. finish: An optional unary function `(x: Column) -> Column` used to convert accumulated value into the final result. starting_level: Nesting level from which the aggregation is started Returns: A Column expression Examples: *Example 1* >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data() >>> nested.print_schema(employee_df) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) <BLANKLINE> >>> employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"projects!.tasks!.estimate\": None ... }).show(truncate=False) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ <BLANKLINE> >>> employee_df.transform( ... nested.select, ... { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"total_task_estimate\": nf.aggregate( ... field_name=\"projects!.tasks!.estimate\", ... initial_value=f.lit(0).cast(\"BIGINT\"), ... merge=lambda acc, x: acc + x ... ), ... \"projects!.task_estimate_per_project\": lambda project: nf.aggregate( ... field_name=\"tasks!.estimate\", ... initial_value=f.lit(0).cast(\"BIGINT\"), ... merge=lambda acc, x: acc + x, ... starting_level=project, ... ), ... }, ... ).show(truncate=False) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ <BLANKLINE> \"\"\" validate_nested_field_names ( field_name , allow_maps = False ) agg_merge = PrintableFunction ( lambda a : f . aggregate ( a , initial_value , merge ), lambda s : f \"f.aggregate( { s } , initial_value, merge)\" , ) if finish is not None : agg_finish = PrintableFunction ( lambda a : f . aggregate ( f . array ( a ), initial_value , merge , finish ), lambda s : f \"f.aggregate(f.array( { s } ), initial_value, merge, finish))\" , ) else : agg_finish = higher_order . identity if start is not None : agg_start = PrintableFunction ( start , lambda s : f \"start( { s } )\" ) else : agg_start = higher_order . identity field_parts = _split_field_name ( field_name ) def recurse_item ( parts : List [ str ], prefix = \"\" ): key = parts [ 0 ] is_struct = key == STRUCT_SEPARATOR is_repeated = key == REPETITION_MARKER has_children = len ( parts ) > 1 if has_children : child_transformation = recurse_item ( parts [ 1 :], prefix + key ) else : child_transformation = agg_start if is_struct : assert_true ( has_children , \"Error, this should not happen: struct without children\" ) return child_transformation elif is_repeated : return fp . compose ( agg_merge , higher_order . transform ( child_transformation )) else : return fp . compose ( child_transformation , higher_order . struct_get ( key )) root_transformation = recurse_item ( field_parts ) root_transformation = fp . compose ( agg_finish , root_transformation ) return root_transformation ( starting_level ) average ( field_name : str , starting_level : Union [ Column , DataFrame , None ] = None ) -> Column Recursively compute the average of all elements in the given repeated field. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default field_name str Name of the repeated field to sum. It may be repeated multiple times. required starting_level Union [ Column , DataFrame , None] Nesting level from which the aggregation is started. None Returns: Type Description Column A Column expression Examples: Example 1 >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data () >>> nested . print_schema ( employee_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> employee_df . withColumn ( \"projects\" , f . to_json ( \"projects.tasks\" )) . show ( truncate = False ) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"projects!.tasks!.estimate\" : None ... }) . show ( truncate = False ) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"average_task_estimate\" : nf . average ( \"projects!.tasks!.estimate\" ), ... \"projects!.average_task_estimate_per_project\" : ... lambda project : nf . average ( \"tasks!.estimate\" , starting_level = project ), ... }) . show ( truncate = False ) +-----------+----------+---+---------------------+---------------+ |employee_id|name |age|average_task_estimate|projects | +-----------+----------+---+---------------------+---------------+ |1 |John Smith|30 |7.25 |[{6.5}, {8.0}] | |1 |Jane Doe |25 |11.5 |[{16.5}, {6.5}]| +-----------+----------+---+---------------------+---------------+ Example 2 : with all kind of nested structures >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> df . show ( truncate = False ) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ >>> df . select ( nf . average ( \"s1!.a\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 1.5| +-------+ >>> df . select ( nf . average ( \"s2!!\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 2.5| +-------+ >>> df . select ( nf . average ( \"s3!!.a\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 1.5| +-------+ >>> df . select ( nf . average ( \"s4!.a!\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 2.5| +-------+ >>> df . select ( nf . average ( \"s5!.a!.b.c\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 2.5| +-------+ Source code in spark_frame/nested_functions_impl/average.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def average ( field_name : str , starting_level : Union [ Column , DataFrame , None ] = None ) -> Column : \"\"\"Recursively compute the average of all elements in the given repeated field. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: field_name: Name of the repeated field to sum. It may be repeated multiple times. starting_level: Nesting level from which the aggregation is started. Returns: A Column expression Examples: *Example 1* >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data() >>> nested.print_schema(employee_df) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) <BLANKLINE> >>> employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"projects!.tasks!.estimate\": None ... }).show(truncate=False) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"average_task_estimate\": nf.average(\"projects!.tasks!.estimate\"), ... \"projects!.average_task_estimate_per_project\": ... lambda project: nf.average(\"tasks!.estimate\", starting_level=project), ... }).show(truncate=False) +-----------+----------+---+---------------------+---------------+ |employee_id|name |age|average_task_estimate|projects | +-----------+----------+---+---------------------+---------------+ |1 |John Smith|30 |7.25 |[{6.5}, {8.0}] | |1 |Jane Doe |25 |11.5 |[{16.5}, {6.5}]| +-----------+----------+---+---------------------+---------------+ <BLANKLINE> *Example 2 : with all kind of nested structures* >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ <BLANKLINE> >>> df.select(nf.average(\"s1!.a\").alias(\"average\")).show() +-------+ |average| +-------+ | 1.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s2!!\").alias(\"average\")).show() +-------+ |average| +-------+ | 2.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s3!!.a\").alias(\"average\")).show() +-------+ |average| +-------+ | 1.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s4!.a!\").alias(\"average\")).show() +-------+ |average| +-------+ | 2.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s5!.a!.b.c\").alias(\"average\")).show() +-------+ |average| +-------+ | 2.5| +-------+ <BLANKLINE> \"\"\" initial_value = f . struct ( f . lit ( 0 ) . cast ( \"BIGINT\" ) . alias ( \"sum\" ), f . lit ( 0 ) . cast ( \"BIGINT\" ) . alias ( \"count\" )) def start ( x : Column ) -> Column : return f . struct ( x . alias ( \"sum\" ), f . lit ( 1 ) . alias ( \"count\" )) def merge ( acc : Column , x : Column ) -> Column : return f . struct (( acc [ \"sum\" ] + x [ \"sum\" ]) . alias ( \"sum\" ), ( acc [ \"count\" ] + x [ \"count\" ]) . alias ( \"count\" )) def finish ( acc : Column ) -> Column : return f . when ( acc [ \"count\" ] > 0 , acc [ \"sum\" ] / acc [ \"count\" ]) return aggregate ( field_name , initial_value = initial_value , merge = merge , start = start , finish = finish , starting_level = starting_level ) sum ( field_name : str , starting_level : Union [ Column , DataFrame , None ] = None ) -> Column Recursively compute the sum of all elements in the given repeated field. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default field_name str Name of the repeated field to sum. It may be repeated multiple times. required starting_level Union [ Column , DataFrame , None] Nesting level from which the aggregation is started. None Returns: Type Description Column A Column expression Examples: Example 1 >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data () >>> nested . print_schema ( employee_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> employee_df . withColumn ( \"projects\" , f . to_json ( \"projects.tasks\" )) . show ( truncate = False ) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"projects!.tasks!.estimate\" : None ... }) . show ( truncate = False ) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"total_task_estimate\" : nf . sum ( \"projects!.tasks!.estimate\" ), ... \"projects!.task_estimate_per_project\" : lambda project : nf . sum ( \"tasks!.estimate\" , starting_level = project ), ... }) . show ( truncate = False ) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ Example 2 : with all kind of nested structures >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> df . show ( truncate = False ) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ >>> df . select ( nf . sum ( \"s1!.a\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 3| +---+ >>> df . select ( nf . sum ( \"s2!!\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 10| +---+ >>> df . select ( nf . sum ( \"s3!!.a\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 3| +---+ >>> df . select ( nf . sum ( \"s4!.a!\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 10| +---+ >>> df . select ( nf . sum ( \"s5!.a!.b.c\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 10| +---+ Source code in spark_frame/nested_functions_impl/sum.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def sum ( field_name : str , starting_level : Union [ Column , DataFrame , None ] = None ) -> Column : \"\"\"Recursively compute the sum of all elements in the given repeated field. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: field_name: Name of the repeated field to sum. It may be repeated multiple times. starting_level: Nesting level from which the aggregation is started. Returns: A Column expression Examples: *Example 1* >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data() >>> nested.print_schema(employee_df) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) <BLANKLINE> >>> employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"projects!.tasks!.estimate\": None ... }).show(truncate=False) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"total_task_estimate\": nf.sum(\"projects!.tasks!.estimate\"), ... \"projects!.task_estimate_per_project\": lambda project: nf.sum(\"tasks!.estimate\", starting_level=project), ... }).show(truncate=False) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ <BLANKLINE> *Example 2 : with all kind of nested structures* >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ <BLANKLINE> >>> df.select(nf.sum(\"s1!.a\").alias(\"sum\")).show() +---+ |sum| +---+ | 3| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s2!!\").alias(\"sum\")).show() +---+ |sum| +---+ | 10| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s3!!.a\").alias(\"sum\")).show() +---+ |sum| +---+ | 3| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s4!.a!\").alias(\"sum\")).show() +---+ |sum| +---+ | 10| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s5!.a!.b.c\").alias(\"sum\")).show() +---+ |sum| +---+ | 10| +---+ <BLANKLINE> \"\"\" initial_value = f . lit ( 0 ) . cast ( \"BIGINT\" ) def merge ( acc : Column , x : Column ) -> Column : return acc + x return aggregate ( field_name , initial_value = initial_value , merge = merge , starting_level = starting_level )","title":"spark_frame.nested_functions"},{"location":"reference/nested_functions/#spark_frame.nested_functions_impl.aggregate.aggregate","text":"Recursively compute an aggregation of all elements in the given repeated field. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default field_name str Name of the repeated field to sum. It may be repeated multiple times. required initial_value StringOrColumn Name of column or Column expression. required merge Callable [[ Column , Column ], Column ] A binary function (acc: Column, x: Column[) -> Column returning an expression of the same type as initial_value . required start Optional [ Callable [[ Column ], Column ]] An optional unary function (x: Column) -> Column that transforms the values to aggregate into the same type as initial_value . None finish Optional [ Callable [[ Column ], Column ]] An optional unary function (x: Column) -> Column used to convert accumulated value into the final result. None starting_level Union [ Column , DataFrame , None] Nesting level from which the aggregation is started None Returns: Type Description Column A Column expression Examples: Example 1 >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data () >>> nested . print_schema ( employee_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> employee_df . withColumn ( \"projects\" , f . to_json ( \"projects.tasks\" )) . show ( truncate = False ) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"projects!.tasks!.estimate\" : None ... }) . show ( truncate = False ) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ >>> employee_df . transform ( ... nested . select , ... { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"total_task_estimate\" : nf . aggregate ( ... field_name = \"projects!.tasks!.estimate\" , ... initial_value = f . lit ( 0 ) . cast ( \"BIGINT\" ), ... merge = lambda acc , x : acc + x ... ), ... \"projects!.task_estimate_per_project\" : lambda project : nf . aggregate ( ... field_name = \"tasks!.estimate\" , ... initial_value = f . lit ( 0 ) . cast ( \"BIGINT\" ), ... merge = lambda acc , x : acc + x , ... starting_level = project , ... ), ... }, ... ) . show ( truncate = False ) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ Source code in spark_frame/nested_functions_impl/aggregate.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def aggregate ( field_name : str , initial_value : StringOrColumn , merge : Callable [[ Column , Column ], Column ], start : Optional [ Callable [[ Column ], Column ]] = None , finish : Optional [ Callable [[ Column ], Column ]] = None , starting_level : Union [ Column , DataFrame , None ] = None , ) -> Column : \"\"\"Recursively compute an aggregation of all elements in the given repeated field. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: field_name: Name of the repeated field to sum. It may be repeated multiple times. initial_value: Name of column or Column expression. merge: A binary function `(acc: Column, x: Column[) -> Column` returning an expression of the same type as `initial_value`. start: An optional unary function `(x: Column) -> Column` that transforms the values to aggregate into the same type as `initial_value`. finish: An optional unary function `(x: Column) -> Column` used to convert accumulated value into the final result. starting_level: Nesting level from which the aggregation is started Returns: A Column expression Examples: *Example 1* >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data() >>> nested.print_schema(employee_df) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) <BLANKLINE> >>> employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"projects!.tasks!.estimate\": None ... }).show(truncate=False) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ <BLANKLINE> >>> employee_df.transform( ... nested.select, ... { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"total_task_estimate\": nf.aggregate( ... field_name=\"projects!.tasks!.estimate\", ... initial_value=f.lit(0).cast(\"BIGINT\"), ... merge=lambda acc, x: acc + x ... ), ... \"projects!.task_estimate_per_project\": lambda project: nf.aggregate( ... field_name=\"tasks!.estimate\", ... initial_value=f.lit(0).cast(\"BIGINT\"), ... merge=lambda acc, x: acc + x, ... starting_level=project, ... ), ... }, ... ).show(truncate=False) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ <BLANKLINE> \"\"\" validate_nested_field_names ( field_name , allow_maps = False ) agg_merge = PrintableFunction ( lambda a : f . aggregate ( a , initial_value , merge ), lambda s : f \"f.aggregate( { s } , initial_value, merge)\" , ) if finish is not None : agg_finish = PrintableFunction ( lambda a : f . aggregate ( f . array ( a ), initial_value , merge , finish ), lambda s : f \"f.aggregate(f.array( { s } ), initial_value, merge, finish))\" , ) else : agg_finish = higher_order . identity if start is not None : agg_start = PrintableFunction ( start , lambda s : f \"start( { s } )\" ) else : agg_start = higher_order . identity field_parts = _split_field_name ( field_name ) def recurse_item ( parts : List [ str ], prefix = \"\" ): key = parts [ 0 ] is_struct = key == STRUCT_SEPARATOR is_repeated = key == REPETITION_MARKER has_children = len ( parts ) > 1 if has_children : child_transformation = recurse_item ( parts [ 1 :], prefix + key ) else : child_transformation = agg_start if is_struct : assert_true ( has_children , \"Error, this should not happen: struct without children\" ) return child_transformation elif is_repeated : return fp . compose ( agg_merge , higher_order . transform ( child_transformation )) else : return fp . compose ( child_transformation , higher_order . struct_get ( key )) root_transformation = recurse_item ( field_parts ) root_transformation = fp . compose ( agg_finish , root_transformation ) return root_transformation ( starting_level )","title":"aggregate()"},{"location":"reference/nested_functions/#spark_frame.nested_functions_impl.average.average","text":"Recursively compute the average of all elements in the given repeated field. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default field_name str Name of the repeated field to sum. It may be repeated multiple times. required starting_level Union [ Column , DataFrame , None] Nesting level from which the aggregation is started. None Returns: Type Description Column A Column expression Examples: Example 1 >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data () >>> nested . print_schema ( employee_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> employee_df . withColumn ( \"projects\" , f . to_json ( \"projects.tasks\" )) . show ( truncate = False ) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"projects!.tasks!.estimate\" : None ... }) . show ( truncate = False ) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"average_task_estimate\" : nf . average ( \"projects!.tasks!.estimate\" ), ... \"projects!.average_task_estimate_per_project\" : ... lambda project : nf . average ( \"tasks!.estimate\" , starting_level = project ), ... }) . show ( truncate = False ) +-----------+----------+---+---------------------+---------------+ |employee_id|name |age|average_task_estimate|projects | +-----------+----------+---+---------------------+---------------+ |1 |John Smith|30 |7.25 |[{6.5}, {8.0}] | |1 |Jane Doe |25 |11.5 |[{16.5}, {6.5}]| +-----------+----------+---+---------------------+---------------+ Example 2 : with all kind of nested structures >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> df . show ( truncate = False ) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ >>> df . select ( nf . average ( \"s1!.a\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 1.5| +-------+ >>> df . select ( nf . average ( \"s2!!\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 2.5| +-------+ >>> df . select ( nf . average ( \"s3!!.a\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 1.5| +-------+ >>> df . select ( nf . average ( \"s4!.a!\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 2.5| +-------+ >>> df . select ( nf . average ( \"s5!.a!.b.c\" ) . alias ( \"average\" )) . show () +-------+ |average| +-------+ | 2.5| +-------+ Source code in spark_frame/nested_functions_impl/average.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def average ( field_name : str , starting_level : Union [ Column , DataFrame , None ] = None ) -> Column : \"\"\"Recursively compute the average of all elements in the given repeated field. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: field_name: Name of the repeated field to sum. It may be repeated multiple times. starting_level: Nesting level from which the aggregation is started. Returns: A Column expression Examples: *Example 1* >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data() >>> nested.print_schema(employee_df) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) <BLANKLINE> >>> employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"projects!.tasks!.estimate\": None ... }).show(truncate=False) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"average_task_estimate\": nf.average(\"projects!.tasks!.estimate\"), ... \"projects!.average_task_estimate_per_project\": ... lambda project: nf.average(\"tasks!.estimate\", starting_level=project), ... }).show(truncate=False) +-----------+----------+---+---------------------+---------------+ |employee_id|name |age|average_task_estimate|projects | +-----------+----------+---+---------------------+---------------+ |1 |John Smith|30 |7.25 |[{6.5}, {8.0}] | |1 |Jane Doe |25 |11.5 |[{16.5}, {6.5}]| +-----------+----------+---+---------------------+---------------+ <BLANKLINE> *Example 2 : with all kind of nested structures* >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ <BLANKLINE> >>> df.select(nf.average(\"s1!.a\").alias(\"average\")).show() +-------+ |average| +-------+ | 1.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s2!!\").alias(\"average\")).show() +-------+ |average| +-------+ | 2.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s3!!.a\").alias(\"average\")).show() +-------+ |average| +-------+ | 1.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s4!.a!\").alias(\"average\")).show() +-------+ |average| +-------+ | 2.5| +-------+ <BLANKLINE> >>> df.select(nf.average(\"s5!.a!.b.c\").alias(\"average\")).show() +-------+ |average| +-------+ | 2.5| +-------+ <BLANKLINE> \"\"\" initial_value = f . struct ( f . lit ( 0 ) . cast ( \"BIGINT\" ) . alias ( \"sum\" ), f . lit ( 0 ) . cast ( \"BIGINT\" ) . alias ( \"count\" )) def start ( x : Column ) -> Column : return f . struct ( x . alias ( \"sum\" ), f . lit ( 1 ) . alias ( \"count\" )) def merge ( acc : Column , x : Column ) -> Column : return f . struct (( acc [ \"sum\" ] + x [ \"sum\" ]) . alias ( \"sum\" ), ( acc [ \"count\" ] + x [ \"count\" ]) . alias ( \"count\" )) def finish ( acc : Column ) -> Column : return f . when ( acc [ \"count\" ] > 0 , acc [ \"sum\" ] / acc [ \"count\" ]) return aggregate ( field_name , initial_value = initial_value , merge = merge , start = start , finish = finish , starting_level = starting_level )","title":"average()"},{"location":"reference/nested_functions/#spark_frame.nested_functions_impl.sum.sum","text":"Recursively compute the sum of all elements in the given repeated field. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default field_name str Name of the repeated field to sum. It may be repeated multiple times. required starting_level Union [ Column , DataFrame , None] Nesting level from which the aggregation is started. None Returns: Type Description Column A Column expression Examples: Example 1 >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data () >>> nested . print_schema ( employee_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> employee_df . withColumn ( \"projects\" , f . to_json ( \"projects.tasks\" )) . show ( truncate = False ) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"projects!.tasks!.estimate\" : None ... }) . show ( truncate = False ) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ >>> employee_df . transform ( nested . select , { ... \"employee_id\" : None , ... \"name\" : None , ... \"age\" : None , ... \"total_task_estimate\" : nf . sum ( \"projects!.tasks!.estimate\" ), ... \"projects!.task_estimate_per_project\" : lambda project : nf . sum ( \"tasks!.estimate\" , starting_level = project ), ... }) . show ( truncate = False ) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ Example 2 : with all kind of nested structures >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> df . show ( truncate = False ) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ >>> df . select ( nf . sum ( \"s1!.a\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 3| +---+ >>> df . select ( nf . sum ( \"s2!!\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 10| +---+ >>> df . select ( nf . sum ( \"s3!!.a\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 3| +---+ >>> df . select ( nf . sum ( \"s4!.a!\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 10| +---+ >>> df . select ( nf . sum ( \"s5!.a!.b.c\" ) . alias ( \"sum\" )) . show () +---+ |sum| +---+ | 10| +---+ Source code in spark_frame/nested_functions_impl/sum.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def sum ( field_name : str , starting_level : Union [ Column , DataFrame , None ] = None ) -> Column : \"\"\"Recursively compute the sum of all elements in the given repeated field. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: field_name: Name of the repeated field to sum. It may be repeated multiple times. starting_level: Nesting level from which the aggregation is started. Returns: A Column expression Examples: *Example 1* >>> from spark_frame.nested_functions_impl.aggregate import _get_sample_data >>> from spark_frame import nested >>> from spark_frame import nested_functions as nf >>> employee_df = _get_sample_data() >>> nested.print_schema(employee_df) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) <BLANKLINE> >>> employee_df.withColumn(\"projects\", f.to_json(\"projects.tasks\")).show(truncate=False) # noqa: E501 +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[[{\"name\":\"Task 1\",\"estimate\":8},{\"name\":\"Task 2\",\"estimate\":5}],[{\"name\":\"Task 3\",\"estimate\":13},{\"name\":\"Task 4\",\"estimate\":3}]] | |1 |Jane Doe |25 |[[{\"name\":\"Task 5\",\"estimate\":20},{\"name\":\"Task 6\",\"estimate\":13}],[{\"name\":\"Task 7\",\"estimate\":8},{\"name\":\"Task 8\",\"estimate\":5}]]| +-----------+----------+---+-----------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"projects!.tasks!.estimate\": None ... }).show(truncate=False) +-----------+----------+---+------------------------------+ |employee_id|name |age|projects | +-----------+----------+---+------------------------------+ |1 |John Smith|30 |[{[{8}, {5}]}, {[{13}, {3}]}] | |1 |Jane Doe |25 |[{[{20}, {13}]}, {[{8}, {5}]}]| +-----------+----------+---+------------------------------+ <BLANKLINE> >>> employee_df.transform(nested.select, { ... \"employee_id\": None, ... \"name\": None, ... \"age\": None, ... \"total_task_estimate\": nf.sum(\"projects!.tasks!.estimate\"), ... \"projects!.task_estimate_per_project\": lambda project: nf.sum(\"tasks!.estimate\", starting_level=project), ... }).show(truncate=False) +-----------+----------+---+-------------------+------------+ |employee_id|name |age|total_task_estimate|projects | +-----------+----------+---+-------------------+------------+ |1 |John Smith|30 |29 |[{13}, {16}]| |1 |Jane Doe |25 |46 |[{33}, {13}]| +-----------+----------+---+-------------------+------------+ <BLANKLINE> *Example 2 : with all kind of nested structures* >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+----------+----------------+--------------+--------------------+------------------------------------+ |id |s1 |s2 |s3 |s4 |s5 | +---+----------+----------------+--------------+--------------------+------------------------------------+ |1 |[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +---+----------+----------------+--------------+--------------------+------------------------------------+ <BLANKLINE> >>> df.select(nf.sum(\"s1!.a\").alias(\"sum\")).show() +---+ |sum| +---+ | 3| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s2!!\").alias(\"sum\")).show() +---+ |sum| +---+ | 10| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s3!!.a\").alias(\"sum\")).show() +---+ |sum| +---+ | 3| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s4!.a!\").alias(\"sum\")).show() +---+ |sum| +---+ | 10| +---+ <BLANKLINE> >>> df.select(nf.sum(\"s5!.a!.b.c\").alias(\"sum\")).show() +---+ |sum| +---+ | 10| +---+ <BLANKLINE> \"\"\" initial_value = f . lit ( 0 ) . cast ( \"BIGINT\" ) def merge ( acc : Column , x : Column ) -> Column : return acc + x return aggregate ( field_name , initial_value = initial_value , merge = merge , starting_level = starting_level )","title":"sum()"},{"location":"reference/schema_utils/","text":"This module contains methods useful for manipulating DataFrame schemas. schema_from_json ( json_string : str ) -> StructType Parses the given json string representing a Spark :class: StructType . Only schema representing StructTypes can be parsed, this means that schema_from_json(schema_to_json(data_type)) will crash if data_type is not a StructType. Parameters: Name Type Description Default json_string str A string representation of a DataFrame schema. required Returns: Type Description StructType A StructType object representing the DataFrame schema Examples: >>> schema_from_json ( '''{\"fields\":[ ... {\"metadata\": {} ,\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"}, ... {\"metadata\": {} ,\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"} ... ],\"type\":\"struct\"}''' ) StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_json ( '''{\"fields\":[ ... {\"metadata\": {} ,\"name\":\"a\",\"nullable\":true,\"type\":\"double\"}, ... {\"metadata\": {} ,\"name\":\"b\",\"nullable\":true,\"type\":\"string\"} ... ],\"type\":\"struct\"}''' ) StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_json ( '''{\"fields\":[ ... {\"metadata\": {} ,\"name\":\"a\",\"nullable\":true,\"type\":{ ... \"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\" ... }} ... ],\"type\":\"struct\"}''' ) StructType([StructField('a', ArrayType(ShortType(), True), True)]) Error cases: >>> schema_from_json ( '\"integer\"' ) Traceback (most recent call last): ... TypeError : string indices must be integers >>> schema_from_json ( '''{\"keyType\":\"string\",\"type\":\"map\", ... \"valueContainsNull\":true,\"valueType\":\"string\"}''' ) Traceback (most recent call last): ... KeyError : 'fields' Source code in spark_frame/schema_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def schema_from_json ( json_string : str ) -> StructType : \"\"\"Parses the given json string representing a Spark :class:`StructType`. Only schema representing StructTypes can be parsed, this means that `schema_from_json(schema_to_json(data_type))` will crash if `data_type` is not a StructType. Args: json_string: A string representation of a DataFrame schema. Returns: A StructType object representing the DataFrame schema Examples: >>> schema_from_json('''{\"fields\":[ ... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"}, ... {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"} ... ],\"type\":\"struct\"}''') StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_json('''{\"fields\":[ ... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"}, ... {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"} ... ],\"type\":\"struct\"}''') StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_json('''{\"fields\":[ ... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{ ... \"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\" ... }} ... ],\"type\":\"struct\"}''') StructType([StructField('a', ArrayType(ShortType(), True), True)]) **Error cases:** >>> schema_from_json('\"integer\"') # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... TypeError: string indices must be integers >>> schema_from_json('''{\"keyType\":\"string\",\"type\":\"map\", ... \"valueContainsNull\":true,\"valueType\":\"string\"}''') # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... KeyError: 'fields' \"\"\" return StructType . fromJson ( json . loads ( json_string )) schema_from_simple_string ( schema_string : str ) -> DataType Parses the given data type string to a :class: DataType . The data type string format equals pyspark.sql.types.DataType.simpleString , except that the top level struct type can omit the struct<> . This method requires the SparkSession to have already been instantiated. Parameters: Name Type Description Default schema_string str A simpleString representing a DataFrame schema. required Returns: Type Description DataType A DataType object representing the DataFrame schema. Raises: Type Description AssertionError If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> schema_from_simple_string ( \"int \" ) IntegerType() >>> schema_from_simple_string ( \"INT \" ) IntegerType() >>> schema_from_simple_string ( \"a: byte, b: decimal( 16 , 8 ) \" ) StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string ( \"a DOUBLE, b STRING\" ) StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string ( \"a: array< short>\" ) StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string ( \" map<string , string > \" ) MapType(StringType(), StringType(), True) Error cases: >>> schema_from_simple_string ( \"blabla\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"a: int,\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"array<int\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"map<int, boolean>>\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... Source code in spark_frame/schema_utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def schema_from_simple_string ( schema_string : str ) -> DataType : \"\"\"Parses the given data type string to a :class:`DataType`. The data type string format equals [pyspark.sql.types.DataType.simpleString][], except that the top level struct type can omit the ``struct<>``. This method requires the SparkSession to have already been instantiated. Args: schema_string: A simpleString representing a DataFrame schema. Returns: A DataType object representing the DataFrame schema. Raises: AssertionError: If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> schema_from_simple_string(\"int \") IntegerType() >>> schema_from_simple_string(\"INT \") IntegerType() >>> schema_from_simple_string(\"a: byte, b: decimal( 16 , 8 ) \") StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string(\"a DOUBLE, b STRING\") StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string(\"a: array< short>\") StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string(\" map<string , string > \") MapType(StringType(), StringType(), True) **Error cases:** >>> schema_from_simple_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... \"\"\" sc = SparkContext . _active_spark_context assert_true ( sc is not None , \"No SparkContext has been instantiated yet\" ) return _parse_datatype_string ( schema_string ) schema_to_json ( schema : DataType ) -> str Convert the given datatype into a json string. Parameters: Name Type Description Default schema DataType A DataFrame schema. required Returns: Type Description str A single-line json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_json ( IntegerType ()) '\"integer\"' >>> schema_to_json ( StructType ([ StructField ( 'a' , ByteType (), True ), StructField ( 'b' , DecimalType ( 16 , 8 ), True )])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}],\"type\":\"struct\"}' >>> schema_to_json ( StructType ([ StructField ( 'a' , DoubleType (), True ), StructField ( 'b' , StringType (), True )])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> schema_to_json ( StructType ([ StructField ( 'a' , ArrayType ( ShortType (), True ), True )])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"}}],\"type\":\"struct\"}' >>> schema_to_json ( MapType ( StringType (), StringType (), True )) '{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}' Source code in spark_frame/schema_utils.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def schema_to_json ( schema : DataType ) -> str : \"\"\"Convert the given datatype into a json string. Args: schema: A DataFrame schema. Returns: A single-line json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_json(IntegerType()) '\"integer\"' >>> schema_to_json(StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}],\"type\":\"struct\"}' >>> schema_to_json(StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> schema_to_json(StructType([StructField('a', ArrayType(ShortType(), True), True)])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"}}],\"type\":\"struct\"}' >>> schema_to_json(MapType(StringType(), StringType(), True)) '{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}' \"\"\" return schema . json () schema_to_pretty_json ( schema : DataType ) -> str Convert the given datatype into a pretty (indented) json string. Parameters: Name Type Description Default schema DataType A DataFrame schema. required Returns: Type Description str A multi-line indented json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> print ( schema_to_pretty_json ( IntegerType ())) \"integer\" >>> print ( schema_to_pretty_json ( StructType ([ StructField ( 'a' , ArrayType ( ShortType (), True ), True )]))) { \"fields\": [ { \"metadata\": {}, \"name\": \"a\", \"nullable\": true, \"type\": { \"containsNull\": true, \"elementType\": \"short\", \"type\": \"array\" } } ], \"type\": \"struct\" } >>> print ( schema_to_pretty_json ( MapType ( StringType (), StringType (), True ))) { \"keyType\": \"string\", \"type\": \"map\", \"valueContainsNull\": true, \"valueType\": \"string\" } :param schema: :return: Source code in spark_frame/schema_utils.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def schema_to_pretty_json ( schema : DataType ) -> str : \"\"\"Convert the given datatype into a pretty (indented) json string. Args: schema: A DataFrame schema. Returns: A multi-line indented json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> print(schema_to_pretty_json(IntegerType())) \"integer\" >>> print(schema_to_pretty_json(StructType([StructField('a', ArrayType(ShortType(), True), True)]))) { \"fields\": [ { \"metadata\": {}, \"name\": \"a\", \"nullable\": true, \"type\": { \"containsNull\": true, \"elementType\": \"short\", \"type\": \"array\" } } ], \"type\": \"struct\" } >>> print(schema_to_pretty_json(MapType(StringType(), StringType(), True))) { \"keyType\": \"string\", \"type\": \"map\", \"valueContainsNull\": true, \"valueType\": \"string\" } :param schema: :return: \"\"\" schema_dict = json . loads ( schema . json ()) return json . dumps ( schema_dict , indent = 2 , sort_keys = True ) schema_to_simple_string ( schema : DataType ) -> str Convert the given datatype into a simple sql string. This method is equivalent to calling schema.simpleString() directly. Parameters: Name Type Description Default schema DataType A DataFrame schema. required Returns: Type Description str A simpleString representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_simple_string ( IntegerType ()) 'int' >>> schema_to_simple_string ( StructType ([ ... StructField ( 'a' , ByteType (), True ), ... StructField ( 'b' , DecimalType ( 16 , 8 ), True ) ... ])) 'struct<a:tinyint,b:decimal(16,8)>' >>> schema_to_simple_string ( StructType ([ ... StructField ( 'a' , DoubleType (), True ), ... StructField ( 'b' , StringType (), True ) ... ])) 'struct<a:double,b:string>' >>> schema_to_simple_string ( StructType ([ StructField ( 'a' , ArrayType ( ShortType (), True ), True )])) 'struct<a:array<smallint>>' >>> schema_to_simple_string ( MapType ( StringType (), StringType (), True )) 'map<string,string>' Source code in spark_frame/schema_utils.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def schema_to_simple_string ( schema : DataType ) -> str : \"\"\"Convert the given datatype into a simple sql string. This method is equivalent to calling [`schema.simpleString()`][pyspark.sql.types.DataType.simpleString] directly. Args: schema: A DataFrame schema. Returns: A simpleString representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_simple_string(IntegerType()) 'int' >>> schema_to_simple_string(StructType([ ... StructField('a', ByteType(), True), ... StructField('b', DecimalType(16,8), True) ... ])) 'struct<a:tinyint,b:decimal(16,8)>' >>> schema_to_simple_string(StructType([ ... StructField('a', DoubleType(), True), ... StructField('b', StringType(), True) ... ])) 'struct<a:double,b:string>' >>> schema_to_simple_string(StructType([StructField('a', ArrayType(ShortType(), True), True)])) 'struct<a:array<smallint>>' >>> schema_to_simple_string(MapType(StringType(), StringType(), True)) 'map<string,string>' \"\"\" return schema . simpleString ()","title":"spark_frame.schema_utils"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_from_json","text":"Parses the given json string representing a Spark :class: StructType . Only schema representing StructTypes can be parsed, this means that schema_from_json(schema_to_json(data_type)) will crash if data_type is not a StructType. Parameters: Name Type Description Default json_string str A string representation of a DataFrame schema. required Returns: Type Description StructType A StructType object representing the DataFrame schema Examples: >>> schema_from_json ( '''{\"fields\":[ ... {\"metadata\": {} ,\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"}, ... {\"metadata\": {} ,\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"} ... ],\"type\":\"struct\"}''' ) StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_json ( '''{\"fields\":[ ... {\"metadata\": {} ,\"name\":\"a\",\"nullable\":true,\"type\":\"double\"}, ... {\"metadata\": {} ,\"name\":\"b\",\"nullable\":true,\"type\":\"string\"} ... ],\"type\":\"struct\"}''' ) StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_json ( '''{\"fields\":[ ... {\"metadata\": {} ,\"name\":\"a\",\"nullable\":true,\"type\":{ ... \"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\" ... }} ... ],\"type\":\"struct\"}''' ) StructType([StructField('a', ArrayType(ShortType(), True), True)]) Error cases: >>> schema_from_json ( '\"integer\"' ) Traceback (most recent call last): ... TypeError : string indices must be integers >>> schema_from_json ( '''{\"keyType\":\"string\",\"type\":\"map\", ... \"valueContainsNull\":true,\"valueType\":\"string\"}''' ) Traceback (most recent call last): ... KeyError : 'fields' Source code in spark_frame/schema_utils.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def schema_from_json ( json_string : str ) -> StructType : \"\"\"Parses the given json string representing a Spark :class:`StructType`. Only schema representing StructTypes can be parsed, this means that `schema_from_json(schema_to_json(data_type))` will crash if `data_type` is not a StructType. Args: json_string: A string representation of a DataFrame schema. Returns: A StructType object representing the DataFrame schema Examples: >>> schema_from_json('''{\"fields\":[ ... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"}, ... {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"} ... ],\"type\":\"struct\"}''') StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_json('''{\"fields\":[ ... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"}, ... {\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"} ... ],\"type\":\"struct\"}''') StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_json('''{\"fields\":[ ... {\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{ ... \"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\" ... }} ... ],\"type\":\"struct\"}''') StructType([StructField('a', ArrayType(ShortType(), True), True)]) **Error cases:** >>> schema_from_json('\"integer\"') # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... TypeError: string indices must be integers >>> schema_from_json('''{\"keyType\":\"string\",\"type\":\"map\", ... \"valueContainsNull\":true,\"valueType\":\"string\"}''') # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... KeyError: 'fields' \"\"\" return StructType . fromJson ( json . loads ( json_string ))","title":"schema_from_json()"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_from_simple_string","text":"Parses the given data type string to a :class: DataType . The data type string format equals pyspark.sql.types.DataType.simpleString , except that the top level struct type can omit the struct<> . This method requires the SparkSession to have already been instantiated. Parameters: Name Type Description Default schema_string str A simpleString representing a DataFrame schema. required Returns: Type Description DataType A DataType object representing the DataFrame schema. Raises: Type Description AssertionError If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> schema_from_simple_string ( \"int \" ) IntegerType() >>> schema_from_simple_string ( \"INT \" ) IntegerType() >>> schema_from_simple_string ( \"a: byte, b: decimal( 16 , 8 ) \" ) StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string ( \"a DOUBLE, b STRING\" ) StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string ( \"a: array< short>\" ) StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string ( \" map<string , string > \" ) MapType(StringType(), StringType(), True) Error cases: >>> schema_from_simple_string ( \"blabla\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"a: int,\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"array<int\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"map<int, boolean>>\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... Source code in spark_frame/schema_utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def schema_from_simple_string ( schema_string : str ) -> DataType : \"\"\"Parses the given data type string to a :class:`DataType`. The data type string format equals [pyspark.sql.types.DataType.simpleString][], except that the top level struct type can omit the ``struct<>``. This method requires the SparkSession to have already been instantiated. Args: schema_string: A simpleString representing a DataFrame schema. Returns: A DataType object representing the DataFrame schema. Raises: AssertionError: If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> schema_from_simple_string(\"int \") IntegerType() >>> schema_from_simple_string(\"INT \") IntegerType() >>> schema_from_simple_string(\"a: byte, b: decimal( 16 , 8 ) \") StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string(\"a DOUBLE, b STRING\") StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string(\"a: array< short>\") StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string(\" map<string , string > \") MapType(StringType(), StringType(), True) **Error cases:** >>> schema_from_simple_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... \"\"\" sc = SparkContext . _active_spark_context assert_true ( sc is not None , \"No SparkContext has been instantiated yet\" ) return _parse_datatype_string ( schema_string )","title":"schema_from_simple_string()"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_to_json","text":"Convert the given datatype into a json string. Parameters: Name Type Description Default schema DataType A DataFrame schema. required Returns: Type Description str A single-line json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_json ( IntegerType ()) '\"integer\"' >>> schema_to_json ( StructType ([ StructField ( 'a' , ByteType (), True ), StructField ( 'b' , DecimalType ( 16 , 8 ), True )])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}],\"type\":\"struct\"}' >>> schema_to_json ( StructType ([ StructField ( 'a' , DoubleType (), True ), StructField ( 'b' , StringType (), True )])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> schema_to_json ( StructType ([ StructField ( 'a' , ArrayType ( ShortType (), True ), True )])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"}}],\"type\":\"struct\"}' >>> schema_to_json ( MapType ( StringType (), StringType (), True )) '{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}' Source code in spark_frame/schema_utils.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def schema_to_json ( schema : DataType ) -> str : \"\"\"Convert the given datatype into a json string. Args: schema: A DataFrame schema. Returns: A single-line json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_json(IntegerType()) '\"integer\"' >>> schema_to_json(StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"decimal(16,8)\"}],\"type\":\"struct\"}' >>> schema_to_json(StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"b\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> schema_to_json(StructType([StructField('a', ArrayType(ShortType(), True), True)])) '{\"fields\":[{\"metadata\":{},\"name\":\"a\",\"nullable\":true,\"type\":{\"containsNull\":true,\"elementType\":\"short\",\"type\":\"array\"}}],\"type\":\"struct\"}' >>> schema_to_json(MapType(StringType(), StringType(), True)) '{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}' \"\"\" return schema . json ()","title":"schema_to_json()"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_to_pretty_json","text":"Convert the given datatype into a pretty (indented) json string. Parameters: Name Type Description Default schema DataType A DataFrame schema. required Returns: Type Description str A multi-line indented json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> print ( schema_to_pretty_json ( IntegerType ())) \"integer\" >>> print ( schema_to_pretty_json ( StructType ([ StructField ( 'a' , ArrayType ( ShortType (), True ), True )]))) { \"fields\": [ { \"metadata\": {}, \"name\": \"a\", \"nullable\": true, \"type\": { \"containsNull\": true, \"elementType\": \"short\", \"type\": \"array\" } } ], \"type\": \"struct\" } >>> print ( schema_to_pretty_json ( MapType ( StringType (), StringType (), True ))) { \"keyType\": \"string\", \"type\": \"map\", \"valueContainsNull\": true, \"valueType\": \"string\" } :param schema: :return: Source code in spark_frame/schema_utils.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def schema_to_pretty_json ( schema : DataType ) -> str : \"\"\"Convert the given datatype into a pretty (indented) json string. Args: schema: A DataFrame schema. Returns: A multi-line indented json string representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> print(schema_to_pretty_json(IntegerType())) \"integer\" >>> print(schema_to_pretty_json(StructType([StructField('a', ArrayType(ShortType(), True), True)]))) { \"fields\": [ { \"metadata\": {}, \"name\": \"a\", \"nullable\": true, \"type\": { \"containsNull\": true, \"elementType\": \"short\", \"type\": \"array\" } } ], \"type\": \"struct\" } >>> print(schema_to_pretty_json(MapType(StringType(), StringType(), True))) { \"keyType\": \"string\", \"type\": \"map\", \"valueContainsNull\": true, \"valueType\": \"string\" } :param schema: :return: \"\"\" schema_dict = json . loads ( schema . json ()) return json . dumps ( schema_dict , indent = 2 , sort_keys = True )","title":"schema_to_pretty_json()"},{"location":"reference/schema_utils/#spark_frame.schema_utils.schema_to_simple_string","text":"Convert the given datatype into a simple sql string. This method is equivalent to calling schema.simpleString() directly. Parameters: Name Type Description Default schema DataType A DataFrame schema. required Returns: Type Description str A simpleString representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_simple_string ( IntegerType ()) 'int' >>> schema_to_simple_string ( StructType ([ ... StructField ( 'a' , ByteType (), True ), ... StructField ( 'b' , DecimalType ( 16 , 8 ), True ) ... ])) 'struct<a:tinyint,b:decimal(16,8)>' >>> schema_to_simple_string ( StructType ([ ... StructField ( 'a' , DoubleType (), True ), ... StructField ( 'b' , StringType (), True ) ... ])) 'struct<a:double,b:string>' >>> schema_to_simple_string ( StructType ([ StructField ( 'a' , ArrayType ( ShortType (), True ), True )])) 'struct<a:array<smallint>>' >>> schema_to_simple_string ( MapType ( StringType (), StringType (), True )) 'map<string,string>' Source code in spark_frame/schema_utils.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def schema_to_simple_string ( schema : DataType ) -> str : \"\"\"Convert the given datatype into a simple sql string. This method is equivalent to calling [`schema.simpleString()`][pyspark.sql.types.DataType.simpleString] directly. Args: schema: A DataFrame schema. Returns: A simpleString representing the DataFrame schema. Examples: >>> from pyspark.sql.types import * >>> schema_to_simple_string(IntegerType()) 'int' >>> schema_to_simple_string(StructType([ ... StructField('a', ByteType(), True), ... StructField('b', DecimalType(16,8), True) ... ])) 'struct<a:tinyint,b:decimal(16,8)>' >>> schema_to_simple_string(StructType([ ... StructField('a', DoubleType(), True), ... StructField('b', StringType(), True) ... ])) 'struct<a:double,b:string>' >>> schema_to_simple_string(StructType([StructField('a', ArrayType(ShortType(), True), True)])) 'struct<a:array<smallint>>' >>> schema_to_simple_string(MapType(StringType(), StringType(), True)) 'map<string,string>' \"\"\" return schema . simpleString ()","title":"schema_to_simple_string()"},{"location":"reference/transformations/","text":"Unlike those in spark_frame.functions , the methods in this module all take at least one DataFrame as argument and return a new transformed DataFrame. These methods generally offer higher order transformation that requires to inspect the schema or event the content of the input DataFrame(s) before generating the next transformation. Those are typically generic operations that cannot be implemented with one single SQL query. Tip Since Spark 3.3.0, all transformations can be inlined using DataFrame.transform , like this: python df.transform(flatten).withColumn( \"base_stats.Total\", f.col(\"`base_stats.Attack`\") + f.col(\"`base_stats.Defense`\") + f.col(\"`base_stats.HP`\") + f.col(\"`base_stats.Sp Attack`\") + f.col(\"`base_stats.Sp Defense`\") + f.col(\"`base_stats.Speed`\") ).transform(unflatten).show(vertical=True, truncate=False) This example is taken convert_all_maps_to_arrays ( df : DataFrame ) -> DataFrame Transform all columns of type Map<K,V> inside the given DataFrame into ARRAY<STRUCT<key: K, value: V>> . This transformation works recursively on every nesting level. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description DataFrame A new DataFrame in which all maps have been replaced with arrays of entries. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(MAP(1, STRUCT(MAP(1, \"a\") as m2))) as m1' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: map (containsNull = false) | | |-- key: integer | | |-- value: struct (valueContainsNull = false) | | | |-- m2: map (nullable = false) | | | | |-- key: integer | | | | |-- value: string (valueContainsNull = false) >>> df . show () +---+-------------------+ | id| m1| +---+-------------------+ | 1|[{1 -> {{1 -> a}}}]| +---+-------------------+ >>> res_df = convert_all_maps_to_arrays ( df ) >>> res_df . printSchema () root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- key: integer (nullable = false) | | | |-- value: struct (nullable = false) | | | | |-- m2: array (nullable = false) | | | | | |-- element: struct (containsNull = false) | | | | | | |-- key: integer (nullable = false) | | | | | | |-- value: string (nullable = false) >>> res_df . show () +---+-------------------+ | id| m1| +---+-------------------+ | 1|[[{1, {[{1, a}]}}]]| +---+-------------------+ Source code in spark_frame/transformations_impl/convert_all_maps_to_arrays.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def convert_all_maps_to_arrays ( df : DataFrame ) -> DataFrame : \"\"\"Transform all columns of type `Map<K,V>` inside the given DataFrame into `ARRAY<STRUCT<key: K, value: V>>`. This transformation works recursively on every nesting level. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Args: df: A Spark DataFrame Returns: A new DataFrame in which all maps have been replaced with arrays of entries. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('SELECT 1 as id, ARRAY(MAP(1, STRUCT(MAP(1, \"a\") as m2))) as m1') >>> df.printSchema() root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: map (containsNull = false) | | |-- key: integer | | |-- value: struct (valueContainsNull = false) | | | |-- m2: map (nullable = false) | | | | |-- key: integer | | | | |-- value: string (valueContainsNull = false) <BLANKLINE> >>> df.show() +---+-------------------+ | id| m1| +---+-------------------+ | 1|[{1 -> {{1 -> a}}}]| +---+-------------------+ <BLANKLINE> >>> res_df = convert_all_maps_to_arrays(df) >>> res_df.printSchema() root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- key: integer (nullable = false) | | | |-- value: struct (nullable = false) | | | | |-- m2: array (nullable = false) | | | | | |-- element: struct (containsNull = false) | | | | | | |-- key: integer (nullable = false) | | | | | | |-- value: string (nullable = false) <BLANKLINE> >>> res_df.show() +---+-------------------+ | id| m1| +---+-------------------+ | 1|[[{1, {[{1, a}]}}]]| +---+-------------------+ <BLANKLINE> \"\"\" def map_to_arrays ( col : Column , data_type : DataType ): if isinstance ( data_type , MapType ): return f . map_entries ( col ) return transform_all_fields ( df , map_to_arrays ) flatten ( df : DataFrame , struct_separator : str = '.' ) -> DataFrame Flatten all the struct columns of a Spark DataFrame . Nested fields names will be joined together using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required struct_separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ( ... [( 1 , { \"a\" : 1 , \"b\" : { \"c\" : 1 , \"d\" : 1 }})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> flatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> df = spark . createDataFrame ( ... [( 1 , { \"a.a1\" : 1 , \"b.b1\" : { \"c.c1\" : 1 , \"d.d1\" : 1 }})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) >>> flatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) Source code in spark_frame/transformations_impl/flatten.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def flatten ( df : DataFrame , struct_separator : str = \".\" ) -> DataFrame : \"\"\"Flatten all the struct columns of a Spark [DataFrame][pyspark.sql.DataFrame]. Nested fields names will be joined together using the specified separator Args: df: A Spark DataFrame struct_separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame( ... [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> flatten(df).printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame( ... [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) <BLANKLINE> >>> flatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT s.b.c as `s.b.c`\" for each nested column. cols = [] def expand_struct ( struct : StructType , col_stack : List [ str ]) -> None : for field in struct : if type ( field . dataType ) == StructType : struct_field = cast ( StructType , field . dataType ) expand_struct ( struct_field , col_stack + [ field . name ]) else : column = f . col ( \".\" . join ( quote_columns ( col_stack + [ field . name ]))) cols . append ( column . alias ( struct_separator . join ( col_stack + [ field . name ]))) expand_struct ( df . schema , col_stack = []) return df . select ( cols ) flatten_all_arrays ( df : DataFrame ) -> DataFrame Flatten all columns of type ARRAY<ARRAY<T>> inside the given DataFrame into ARRAY<<T>>> . This transformation works recursively on every nesting level. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and accepts field names containing dots ( . ), exclamation marks ( ! ) or percentage ( % ). Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description DataFrame A new DataFrame in which all arrays of array have been flattened Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(ARRAY(ARRAY(1, 2), ARRAY(3)), ARRAY(ARRAY(4), ARRAY(5))) as a' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: array (containsNull = false) | | | |-- element: integer (containsNull = false) >>> df . show ( truncate = False ) +---+---------------------------+ |id |a | +---+---------------------------+ |1 |[[[1, 2], [3]], [[4], [5]]]| +---+---------------------------+ >>> res_df = flatten_all_arrays ( df ) >>> res_df . printSchema () root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: integer (containsNull = false) >>> res_df . show ( truncate = False ) +---+---------------+ |id |a | +---+---------------+ |1 |[1, 2, 3, 4, 5]| +---+---------------+ Source code in spark_frame/transformations_impl/flatten_all_arrays.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def flatten_all_arrays ( df : DataFrame ) -> DataFrame : \"\"\"Flatten all columns of type `ARRAY<ARRAY<T>>` inside the given DataFrame into `ARRAY<<T>>>`. This transformation works recursively on every nesting level. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and accepts field names containing dots (`.`), exclamation marks (`!`) or percentage (`%`). Args: df: A Spark DataFrame Returns: A new DataFrame in which all arrays of array have been flattened Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('SELECT 1 as id, ARRAY(ARRAY(ARRAY(1, 2), ARRAY(3)), ARRAY(ARRAY(4), ARRAY(5))) as a') >>> df.printSchema() root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: array (containsNull = false) | | | |-- element: integer (containsNull = false) <BLANKLINE> >>> df.show(truncate=False) +---+---------------------------+ |id |a | +---+---------------------------+ |1 |[[[1, 2], [3]], [[4], [5]]]| +---+---------------------------+ <BLANKLINE> >>> res_df = flatten_all_arrays(df) >>> res_df.printSchema() root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: integer (containsNull = false) <BLANKLINE> >>> res_df.show(truncate=False) +---+---------------+ |id |a | +---+---------------+ |1 |[1, 2, 3, 4, 5]| +---+---------------+ <BLANKLINE> \"\"\" def flatten_array ( col : Column , data_type : DataType ): if isinstance ( data_type , ArrayType ) and isinstance ( data_type . elementType , ArrayType ): return f . flatten ( col ) return transform_all_fields ( df , flatten_array ) harmonize_dataframes ( left_df : DataFrame , right_df : DataFrame , common_columns : Optional [ Sequence [ Tuple [ str , Optional [ str ]]]] = None ) -> Tuple [ DataFrame , DataFrame ] Given two DataFrames, returns two new corresponding DataFrames with the same schemas by applying the following changes: Only common columns are kept Columns of type MAP are cast into ARRAY > Columns are re-order to have the same ordering in both DataFrames When matching columns have different types, their type is widened to their most narrow common type. This transformation is applied recursively on nested columns, including those inside repeated records (a.k.a. ARRAY<STRUCT<>>). Parameters: Name Type Description Default left_df DataFrame A Spark DataFrame required right_df DataFrame A Spark DataFrame required common_columns Optional [ Sequence [ Tuple [ str , Optional [ str ]]]] A list of (column name, type) tuples. Column names must appear in both DataFrames, and each column will be cast into the corresponding type. None Returns: Type Description Tuple [ DataFrame , DataFrame ] Two new Spark DataFrames with the same schema Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df1 = spark . sql ( 'SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3 as b, 4 as c)) as s2) as s1' ) >>> df2 = spark . sql ( 'SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3.0 as b, \"4\" as c, 5 as d)) as s2) as s1' ) >>> df1 . union ( df2 ) . show ( truncate = False ) Traceback (most recent call last): ... pyspark.sql.utils.AnalysisException : Union can only be performed on tables with the compatible column types. ... >>> df1 , df2 = harmonize_dataframes ( df1 , df2 ) >>> df1 . union ( df2 ) . show () +---+---------------+ | id| s1| +---+---------------+ | 1|{2, [{3.0, 4}]}| | 1|{2, [{3.0, 4}]}| +---+---------------+ Source code in spark_frame/transformations_impl/harmonize_dataframes.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def harmonize_dataframes ( left_df : DataFrame , right_df : DataFrame , common_columns : Optional [ Sequence [ Tuple [ str , Optional [ str ]]]] = None ) -> Tuple [ DataFrame , DataFrame ]: \"\"\"Given two DataFrames, returns two new corresponding DataFrames with the same schemas by applying the following changes: - Only common columns are kept - Columns of type MAP<key, value> are cast into ARRAY<STRUCT<key, value>> - Columns are re-order to have the same ordering in both DataFrames - When matching columns have different types, their type is widened to their most narrow common type. This transformation is applied recursively on nested columns, including those inside repeated records (a.k.a. ARRAY<STRUCT<>>). Args: left_df: A Spark DataFrame right_df: A Spark DataFrame common_columns: A list of (column name, type) tuples. Column names must appear in both DataFrames, and each column will be cast into the corresponding type. Returns: Two new Spark DataFrames with the same schema Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df1 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3 as b, 4 as c)) as s2) as s1') >>> df2 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3.0 as b, \"4\" as c, 5 as d)) as s2) as s1') >>> df1.union(df2).show(truncate=False) # doctest: +ELLIPSIS Traceback (most recent call last): ... pyspark.sql.utils.AnalysisException: Union can only be performed on tables with the compatible column types. ... >>> df1, df2 = harmonize_dataframes(df1, df2) >>> df1.union(df2).show() +---+---------------+ | id| s1| +---+---------------+ | 1|{2, [{3.0, 4}]}| | 1|{2, [{3.0, 4}]}| +---+---------------+ <BLANKLINE> \"\"\" if common_columns is None : left_schema_flat = flatten_schema ( left_df . schema , explode = True ) right_schema_flat = flatten_schema ( right_df . schema , explode = True ) common_columns = get_common_columns ( left_schema_flat , right_schema_flat ) def build_col ( col_name : str , col_type : Optional [ str ]) -> PrintableFunction : parent_structs = _deepest_granularity ( col_name ) if col_type is not None : tpe = cast ( str , col_type ) f1 = PrintableFunction ( lambda s : s . cast ( tpe ), lambda s : f \" { s } .cast( { tpe } )\" ) else : f1 = higher_order . identity f2 = higher_order . recursive_struct_get ( parent_structs ) return fp . compose ( f1 , f2 ) common_columns_dict = { col_name : build_col ( col_name , col_type ) for ( col_name , col_type ) in common_columns } tree = _build_nested_struct_tree ( common_columns_dict ) root_transformation = _build_transformation_from_tree ( tree ) return left_df . select ( * root_transformation ([ left_df ])), right_df . select ( * root_transformation ([ right_df ])) parse_json_columns ( df : DataFrame , columns : Union [ str , List [ str ], Dict [ str , str ]]) -> DataFrame Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's from_json function, with one main difference: from_json requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \" a.b.c \") (See Example 2). Parameters: Name Type Description Default df DataFrame A Spark DataFrame required columns Union [ str , List [ str ], Dict [ str , str ]] A column name, list of column names, or dict(column_name, parsed_column_name) required Returns: Type Description DataFrame A new DataFrame Examples: Example 1 : >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([ ... ( 1 , '[{\"a\": 1}, {\"a\": 2}]' ), ... ( 1 , '[{\"a\": 2}, {\"a\": 4}]' ), ... ( 2 , None ) ... ], \"id INT, json1 STRING\" ... ) >>> df . show () +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) >>> parse_json_columns ( df , 'json1' ) . printSchema () root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 2 : different output column name : >>> parse_json_columns ( df , { 'json1' : 'parsed_json1' }) . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 3 : json inside a struct : >>> df = spark . createDataFrame ([ ... ( 1 , { 'json1' : '[{\"a\": 1}, {\"a\": 2}]' }), ... ( 1 , { 'json1' : '[{\"a\": 2}, {\"a\": 4}]' }), ... ( 2 , None ) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df . show ( 10 , False ) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) >>> res = parse_json_columns ( df , 'struct.json1' ) >>> res . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) >>> res . show ( 10 , False ) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ Source code in spark_frame/transformations_impl/parse_json_columns.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def parse_json_columns ( df : DataFrame , columns : Union [ str , List [ str ], Dict [ str , str ]]) -> DataFrame : \"\"\"Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's `from_json` function, with one main difference: `from_json` requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \"`a.b.c`\") (See Example 2). Args: df: A Spark DataFrame columns: A column name, list of column names, or dict(column_name, parsed_column_name) Returns: A new DataFrame Examples: **Example 1 :** >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([ ... (1, '[{\"a\": 1}, {\"a\": 2}]'), ... (1, '[{\"a\": 2}, {\"a\": 4}]'), ... (2, None) ... ], \"id INT, json1 STRING\" ... ) >>> df.show() +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) <BLANKLINE> >>> parse_json_columns(df, 'json1').printSchema() root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 2 : different output column name :** >>> parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 3 : json inside a struct :** >>> df = spark.createDataFrame([ ... (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}), ... (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}), ... (2, None) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df.show(10, False) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) <BLANKLINE> >>> res = parse_json_columns(df, 'struct.json1') >>> res.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> >>> res.show(10, False) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ <BLANKLINE> \"\"\" if isinstance ( columns , str ): columns = [ columns ] if isinstance ( columns , list ): columns = { col : col for col in columns } wrapped_df = __wrap_json_columns ( df , columns ) schema_per_col = __infer_schema_per_column ( wrapped_df , list ( columns . values ())) res = __parse_json_columns ( wrapped_df , schema_per_col ) return res sort_all_arrays ( df : DataFrame ) -> DataFrame Given a DataFrame, sort all fields of type ARRAY in a canonical order, making them comparable. This also applies to nested fields, even those inside other arrays. Limitation Arrays containing sub-fields of type Map cannot be sorted, as the Map type is not comparable. A possible workaround is to first use the transformation spark_frame.transformations.convert_all_maps_to_arrays Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description DataFrame A new DataFrame where all arrays have been sorted. Examples: Example 1: with a simple ARRAY<INT> >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(3, 2, 1) as a' ) >>> df . show () +---+---------+ | id| a| +---+---------+ | 1|[3, 2, 1]| +---+---------+ >>> sort_all_arrays ( df ) . show () +---+---------+ | id| a| +---+---------+ | 1|[1, 2, 3]| +---+---------+ Example 2: with an ARRAY<STRUCT<...>> >>> df = spark . sql ( 'SELECT ARRAY(STRUCT(2 as a, 1 as b), STRUCT(1 as a, 2 as b), STRUCT(1 as a, 1 as b)) as s' ) >>> df . show ( truncate = False ) +------------------------+ |s | +------------------------+ |[{2, 1}, {1, 2}, {1, 1}]| +------------------------+ >>> df . transform ( sort_all_arrays ) . show ( truncate = False ) +------------------------+ |s | +------------------------+ |[{1, 1}, {1, 2}, {2, 1}]| +------------------------+ Example 3: with an ARRAY<STRUCT<STRUCT<...>>> >>> df = spark . sql ( '''SELECT ARRAY( ... STRUCT(STRUCT(2 as a, 2 as b) as s), ... STRUCT(STRUCT(1 as a, 2 as b) as s) ... ) as l1 ... ''' ) >>> df . show ( truncate = False ) +--------------------+ |l1 | +--------------------+ |[{{2, 2}}, {{1, 2}}]| +--------------------+ >>> df . transform ( sort_all_arrays ) . show ( truncate = False ) +--------------------+ |l1 | +--------------------+ |[{{1, 2}}, {{2, 2}}]| +--------------------+ Example 4: with an ARRAY<ARRAY<ARRAY<INT>>> As this example shows, the innermost arrays are sorted before the outermost arrays. >>> df = spark . sql ( '''SELECT ARRAY( ... ARRAY(ARRAY(4, 1), ARRAY(3, 2)), ... ARRAY(ARRAY(2, 2), ARRAY(2, 1)) ... ) as l1 ... ''' ) >>> df . show ( truncate = False ) +------------------------------------+ |l1 | +------------------------------------+ |[[[4, 1], [3, 2]], [[2, 2], [2, 1]]]| +------------------------------------+ >>> df . transform ( sort_all_arrays ) . show ( truncate = False ) +------------------------------------+ |l1 | +------------------------------------+ |[[[1, 2], [2, 2]], [[1, 4], [2, 3]]]| +------------------------------------+ Source code in spark_frame/transformations_impl/sort_all_arrays.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def sort_all_arrays ( df : DataFrame ) -> DataFrame : \"\"\"Given a DataFrame, sort all fields of type `ARRAY` in a canonical order, making them comparable. This also applies to nested fields, even those inside other arrays. !!! warning \"Limitation\" - Arrays containing sub-fields of type Map cannot be sorted, as the Map type is not comparable. - A possible workaround is to first use the transformation [`spark_frame.transformations.convert_all_maps_to_arrays`] [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays] Args: df: A Spark DataFrame Returns: A new DataFrame where all arrays have been sorted. Examples: *Example 1:* with a simple `ARRAY<INT>` >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('SELECT 1 as id, ARRAY(3, 2, 1) as a') >>> df.show() +---+---------+ | id| a| +---+---------+ | 1|[3, 2, 1]| +---+---------+ <BLANKLINE> >>> sort_all_arrays(df).show() +---+---------+ | id| a| +---+---------+ | 1|[1, 2, 3]| +---+---------+ <BLANKLINE> *Example 2:* with an `ARRAY<STRUCT<...>>` >>> df = spark.sql('SELECT ARRAY(STRUCT(2 as a, 1 as b), STRUCT(1 as a, 2 as b), STRUCT(1 as a, 1 as b)) as s') >>> df.show(truncate=False) +------------------------+ |s | +------------------------+ |[{2, 1}, {1, 2}, {1, 1}]| +------------------------+ <BLANKLINE> >>> df.transform(sort_all_arrays).show(truncate=False) +------------------------+ |s | +------------------------+ |[{1, 1}, {1, 2}, {2, 1}]| +------------------------+ <BLANKLINE> *Example 3:* with an `ARRAY<STRUCT<STRUCT<...>>>` >>> df = spark.sql('''SELECT ARRAY( ... STRUCT(STRUCT(2 as a, 2 as b) as s), ... STRUCT(STRUCT(1 as a, 2 as b) as s) ... ) as l1 ... ''') >>> df.show(truncate=False) +--------------------+ |l1 | +--------------------+ |[{{2, 2}}, {{1, 2}}]| +--------------------+ <BLANKLINE> >>> df.transform(sort_all_arrays).show(truncate=False) +--------------------+ |l1 | +--------------------+ |[{{1, 2}}, {{2, 2}}]| +--------------------+ <BLANKLINE> *Example 4:* with an `ARRAY<ARRAY<ARRAY<INT>>>` As this example shows, the innermost arrays are sorted before the outermost arrays. >>> df = spark.sql('''SELECT ARRAY( ... ARRAY(ARRAY(4, 1), ARRAY(3, 2)), ... ARRAY(ARRAY(2, 2), ARRAY(2, 1)) ... ) as l1 ... ''') >>> df.show(truncate=False) +------------------------------------+ |l1 | +------------------------------------+ |[[[4, 1], [3, 2]], [[2, 2], [2, 1]]]| +------------------------------------+ <BLANKLINE> >>> df.transform(sort_all_arrays).show(truncate=False) +------------------------------------+ |l1 | +------------------------------------+ |[[[1, 2], [2, 2]], [[1, 4], [2, 3]]]| +------------------------------------+ <BLANKLINE> \"\"\" def sort_array ( col : Column , data_type : DataType ): if isinstance ( data_type , ArrayType ): return f . sort_array ( col ) return transform_all_fields ( df , sort_array ) transform_all_field_names ( df : DataFrame , transformation : Callable [[ str ], str ]) -> DataFrame Apply a transformation to all nested field names of a DataFrame. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Parameters: Name Type Description Default df DataFrame A Spark DataFrame required transformation Callable [[ str ], str ] Transformation to apply to all field names in the DataFrame. required Returns: Type Description DataFrame A new DataFrame Examples: Example 1: with a nested schema structure In this example we cast all the field names of the schema to uppercase: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> new_df = df . transform ( transform_all_field_names , lambda s : s . upper ()) >>> nested . print_schema ( new_df ) root |-- NAME: string (nullable = false) |-- S1!.A: integer (nullable = false) |-- S2!!: integer (nullable = false) |-- S3!!.A: integer (nullable = false) |-- S4!.A!: integer (nullable = false) |-- S5!.A!.B.C: integer (nullable = false) Example 2: sanitizing field names In this example we replace all dots and exclamation marks in field names with underscores. This is useful to make a DataFrame compatible with the spark_frame.nested module. >>> df = spark . sql ( '''SELECT ... ARRAY(STRUCT( ... ARRAY(STRUCT( ... STRUCT(1 as `d.d!d`) as `c.c!c` ... )) as `b.b!b` ... )) as `a.a!a` ... ''' ) >>> df . printSchema () root |-- a.a!a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b.b!b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c.c!c: struct (nullable = false) | | | | | |-- d.d!d: integer (nullable = false) >>> new_df = df . transform ( transform_all_field_names , lambda s : s . replace ( \".\" , \"_\" ) . replace ( \"!\" , \"_\" )) >>> new_df . printSchema () root |-- a_a_a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b_b_b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c_c_c: struct (nullable = false) | | | | | |-- d_d_d: integer (nullable = false) This also works on fields of type MAP<K,V> . >>> df = spark . sql ( 'SELECT MAP(STRUCT(1 as `a.a!a`), STRUCT(2 as `b.b!b`)) as `m.m!m`' ) >>> df . printSchema () root |-- m.m!m: map (nullable = false) | |-- key: struct | | |-- a.a!a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b.b!b: integer (nullable = false) >>> new_df = df . transform ( transform_all_field_names , lambda s : s . replace ( \".\" , \"_\" ) . replace ( \"!\" , \"_\" )) >>> new_df . printSchema () root |-- m_m_m: map (nullable = false) | |-- key: struct | | |-- a_a_a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b_b_b: integer (nullable = false) Source code in spark_frame/transformations_impl/transform_all_field_names.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def transform_all_field_names ( df : DataFrame , transformation : Callable [[ str ], str ]) -> DataFrame : \"\"\"Apply a transformation to all nested field names of a DataFrame. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Args: df: A Spark DataFrame transformation: Transformation to apply to all field names in the DataFrame. Returns: A new DataFrame Examples: _**Example 1: with a nested schema structure**_ In this example we cast all the field names of the schema to uppercase: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> new_df = df.transform(transform_all_field_names, lambda s: s.upper()) >>> nested.print_schema(new_df) root |-- NAME: string (nullable = false) |-- S1!.A: integer (nullable = false) |-- S2!!: integer (nullable = false) |-- S3!!.A: integer (nullable = false) |-- S4!.A!: integer (nullable = false) |-- S5!.A!.B.C: integer (nullable = false) <BLANKLINE> _**Example 2: sanitizing field names**_ In this example we replace all dots and exclamation marks in field names with underscores. This is useful to make a DataFrame compatible with the [spark_frame.nested](/spark-frame/reference/nested) module. >>> df = spark.sql('''SELECT ... ARRAY(STRUCT( ... ARRAY(STRUCT( ... STRUCT(1 as `d.d!d`) as `c.c!c` ... )) as `b.b!b` ... )) as `a.a!a` ... ''') >>> df.printSchema() root |-- a.a!a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b.b!b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c.c!c: struct (nullable = false) | | | | | |-- d.d!d: integer (nullable = false) <BLANKLINE> >>> new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\")) >>> new_df.printSchema() root |-- a_a_a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b_b_b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c_c_c: struct (nullable = false) | | | | | |-- d_d_d: integer (nullable = false) <BLANKLINE> This also works on fields of type `MAP<K,V>`. >>> df = spark.sql('SELECT MAP(STRUCT(1 as `a.a!a`), STRUCT(2 as `b.b!b`)) as `m.m!m`') >>> df.printSchema() root |-- m.m!m: map (nullable = false) | |-- key: struct | | |-- a.a!a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b.b!b: integer (nullable = false) <BLANKLINE> >>> new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\")) >>> new_df.printSchema() root |-- m_m_m: map (nullable = false) | |-- key: struct | | |-- a_a_a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b_b_b: integer (nullable = false) <BLANKLINE> \"\"\" root_transformation = build_transformation_from_schema ( df . schema , name_transformation = transformation ) return df . select ( * root_transformation ( df )) transform_all_fields ( df : DataFrame , transformation : Callable [[ Column , DataType ], Column ]) -> DataFrame Apply a transformation to all nested fields of a DataFrame. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Parameters: Name Type Description Default df DataFrame A Spark DataFrame required transformation Callable [[ Column , DataType ], Column ] Transformation to apply to all fields of the DataFrame. The transformation must take as input a Column expression and the DataType of the corresponding expression. required Returns: Type Description DataFrame A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> df . show ( truncate = False ) +----+----------+----------------+--------------+--------------------+------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+----------+----------------+--------------+--------------------+------------------------------------+ |John|[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +----+----------+----------------+--------------+--------------------+------------------------------------+ >>> from pyspark.sql.types import IntegerType >>> def cast_int_as_double ( col : Column , data_type : DataType ): ... if isinstance ( data_type , IntegerType ): ... return col . cast ( \"DOUBLE\" ) >>> new_df = df . transform ( transform_all_fields , cast_int_as_double ) >>> nested . print_schema ( new_df ) root |-- name: string (nullable = false) |-- s1!.a: double (nullable = false) |-- s2!!: double (nullable = false) |-- s3!!.a: double (nullable = false) |-- s4!.a!: double (nullable = false) |-- s5!.a!.b.c: double (nullable = false) >>> new_df . show ( truncate = False ) # noqa: E501 +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |John|[{1.0}, {2.0}]|[[1.0, 2.0], [3.0, 4.0]]|[[{1.0}], [{2.0}]]|[{[1.0, 2.0]}, {[3.0, 4.0]}]|[{[{{1.0}}, {{2.0}}]}, {[{{3.0}}, {{4.0}}]}]| +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ Source code in spark_frame/transformations_impl/transform_all_fields.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def transform_all_fields ( df : DataFrame , transformation : Callable [[ Column , DataType ], Column ]) -> DataFrame : \"\"\"Apply a transformation to all nested fields of a DataFrame. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Args: df: A Spark DataFrame transformation: Transformation to apply to all fields of the DataFrame. The transformation must take as input a Column expression and the DataType of the corresponding expression. Returns: A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +----+----------+----------------+--------------+--------------------+------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+----------+----------------+--------------+--------------------+------------------------------------+ |John|[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +----+----------+----------------+--------------+--------------------+------------------------------------+ <BLANKLINE> >>> from pyspark.sql.types import IntegerType >>> def cast_int_as_double(col: Column, data_type: DataType): ... if isinstance(data_type, IntegerType): ... return col.cast(\"DOUBLE\") >>> new_df = df.transform(transform_all_fields, cast_int_as_double) >>> nested.print_schema(new_df) root |-- name: string (nullable = false) |-- s1!.a: double (nullable = false) |-- s2!!: double (nullable = false) |-- s3!!.a: double (nullable = false) |-- s4!.a!: double (nullable = false) |-- s5!.a!.b.c: double (nullable = false) <BLANKLINE> >>> new_df.show(truncate=False) # noqa: E501 +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |John|[{1.0}, {2.0}]|[[1.0, 2.0], [3.0, 4.0]]|[[{1.0}], [{2.0}]]|[{[1.0, 2.0]}, {[3.0, 4.0]}]|[{[{{1.0}}, {{2.0}}]}, {[{{3.0}}, {{4.0}}]}]| +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ <BLANKLINE> \"\"\" root_transformation = build_transformation_from_schema ( df . schema , column_transformation = transformation ) return df . select ( * root_transformation ( df )) unflatten ( df : DataFrame , separator : str = '.' ) -> DataFrame Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([( 1 , 1 , 1 , 1 )], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> unflatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> df = spark . createDataFrame ([( 1 , 1 , 1 )], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) >>> unflatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) Source code in spark_frame/transformations_impl/unflatten.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def unflatten ( df : DataFrame , separator : str = \".\" ) -> DataFrame : \"\"\"Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Args: df: A Spark DataFrame separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> unflatten(df).printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) <BLANKLINE> >>> unflatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT struct(a, struct(s.b.c, s.b.d)) as s\" for each nested column. # There is a little twist as we don't want to rebuild the struct if all its fields are null, so we add a CASE WHEN def has_structs ( df : DataFrame ) -> bool : struct_fields = [ field for field in df . schema if is_struct ( field )] return len ( struct_fields ) > 0 if has_structs ( df ): df = flatten ( df ) tree = _build_nested_struct_tree ( df . columns , separator ) cols = _build_struct_from_tree ( tree , separator ) return df . select ( cols ) union_dataframes ( * dfs : DataFrame ) -> DataFrame Returns the union between multiple DataFrames Parameters: Name Type Description Default dfs DataFrame One or more Spark DataFrames () Returns: Type Description DataFrame A new DataFrame containing the union of all input DataFrames Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df1 = spark . sql ( 'SELECT 1 as a' ) >>> df2 = spark . sql ( 'SELECT 2 as a' ) >>> df3 = spark . sql ( 'SELECT 3 as a' ) >>> union_dataframes ( df1 , df2 , df3 ) . show () +---+ | a| +---+ | 1| | 2| | 3| +---+ >>> df1 . transform ( union_dataframes , df2 , df3 ) . show () +---+ | a| +---+ | 1| | 2| | 3| +---+ Source code in spark_frame/transformations_impl/union_dataframes.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def union_dataframes ( * dfs : DataFrame ) -> DataFrame : \"\"\"Returns the union between multiple DataFrames Args: dfs: One or more Spark DataFrames Returns: A new DataFrame containing the union of all input DataFrames Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df1 = spark.sql('SELECT 1 as a') >>> df2 = spark.sql('SELECT 2 as a') >>> df3 = spark.sql('SELECT 3 as a') >>> union_dataframes(df1, df2, df3).show() +---+ | a| +---+ | 1| | 2| | 3| +---+ <BLANKLINE> >>> df1.transform(union_dataframes, df2, df3).show() +---+ | a| +---+ | 1| | 2| | 3| +---+ <BLANKLINE> \"\"\" assert_true ( len ( dfs ) > 0 , ValueError ( \"Input list is empty\" )) res = dfs [ 0 ] for df in dfs [ 1 :]: res = res . union ( df ) return res unpivot ( df : DataFrame , pivot_columns : List [ str ], key_alias : str = 'key' , value_alias : str = 'value' ) -> DataFrame Unpivot the given DataFrame along the specified pivot columns. All columns that are not pivot columns should have the same type. This is the inverse transformation of the pyspark.sql.GroupedData.pivot operation. Parameters: Name Type Description Default df DataFrame A DataFrame required pivot_columns List [ str ] The list of columns names on which to perform the pivot required key_alias str Alias given to the 'key' column 'key' value_alias str Alias given to the 'value' column 'value' Returns: Type Description DataFrame An unpivotted DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([ ... ( 2018 , \"Orange\" , None , 4000 , None ), ... ( 2018 , \"Beans\" , None , 1500 , 2000 ), ... ( 2018 , \"Banana\" , 2000 , 400 , None ), ... ( 2018 , \"Carrots\" , 2000 , 1200 , None ), ... ( 2019 , \"Orange\" , 5000 , None , 5000 ), ... ( 2019 , \"Beans\" , None , 1500 , 2000 ), ... ( 2019 , \"Banana\" , None , 1400 , 400 ), ... ( 2019 , \"Carrots\" , None , 200 , None ), ... ], \"year INT, product STRING, Canada INT, China INT, Mexico INT\" ... ) >>> df . show () +----+-------+------+-----+------+ |year|product|Canada|China|Mexico| +----+-------+------+-----+------+ |2018| Orange| null| 4000| null| |2018| Beans| null| 1500| 2000| |2018| Banana| 2000| 400| null| |2018|Carrots| 2000| 1200| null| |2019| Orange| 5000| null| 5000| |2019| Beans| null| 1500| 2000| |2019| Banana| null| 1400| 400| |2019|Carrots| null| 200| null| +----+-------+------+-----+------+ >>> unpivot ( df , [ 'year' , 'product' ], key_alias = 'country' , value_alias = 'total' ) . show ( 100 ) +----+-------+-------+-----+ |year|product|country|total| +----+-------+-------+-----+ |2018| Orange| Canada| null| |2018| Orange| China| 4000| |2018| Orange| Mexico| null| |2018| Beans| Canada| null| |2018| Beans| China| 1500| |2018| Beans| Mexico| 2000| |2018| Banana| Canada| 2000| |2018| Banana| China| 400| |2018| Banana| Mexico| null| |2018|Carrots| Canada| 2000| |2018|Carrots| China| 1200| |2018|Carrots| Mexico| null| |2019| Orange| Canada| 5000| |2019| Orange| China| null| |2019| Orange| Mexico| 5000| |2019| Beans| Canada| null| |2019| Beans| China| 1500| |2019| Beans| Mexico| 2000| |2019| Banana| Canada| null| |2019| Banana| China| 1400| |2019| Banana| Mexico| 400| |2019|Carrots| Canada| null| |2019|Carrots| China| 200| |2019|Carrots| Mexico| null| +----+-------+-------+-----+ Source code in spark_frame/transformations_impl/unpivot.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def unpivot ( df : DataFrame , pivot_columns : List [ str ], key_alias : str = \"key\" , value_alias : str = \"value\" ) -> DataFrame : \"\"\"Unpivot the given DataFrame along the specified pivot columns. All columns that are not pivot columns should have the same type. This is the inverse transformation of the [pyspark.sql.GroupedData.pivot][] operation. Args: df: A DataFrame pivot_columns: The list of columns names on which to perform the pivot key_alias: Alias given to the 'key' column value_alias: Alias given to the 'value' column Returns: An unpivotted DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([ ... (2018, \"Orange\", None, 4000, None), ... (2018, \"Beans\", None, 1500, 2000), ... (2018, \"Banana\", 2000, 400, None), ... (2018, \"Carrots\", 2000, 1200, None), ... (2019, \"Orange\", 5000, None, 5000), ... (2019, \"Beans\", None, 1500, 2000), ... (2019, \"Banana\", None, 1400, 400), ... (2019, \"Carrots\", None, 200, None), ... ], \"year INT, product STRING, Canada INT, China INT, Mexico INT\" ... ) >>> df.show() +----+-------+------+-----+------+ |year|product|Canada|China|Mexico| +----+-------+------+-----+------+ |2018| Orange| null| 4000| null| |2018| Beans| null| 1500| 2000| |2018| Banana| 2000| 400| null| |2018|Carrots| 2000| 1200| null| |2019| Orange| 5000| null| 5000| |2019| Beans| null| 1500| 2000| |2019| Banana| null| 1400| 400| |2019|Carrots| null| 200| null| +----+-------+------+-----+------+ <BLANKLINE> >>> unpivot(df, ['year', 'product'], key_alias='country', value_alias='total').show(100) +----+-------+-------+-----+ |year|product|country|total| +----+-------+-------+-----+ |2018| Orange| Canada| null| |2018| Orange| China| 4000| |2018| Orange| Mexico| null| |2018| Beans| Canada| null| |2018| Beans| China| 1500| |2018| Beans| Mexico| 2000| |2018| Banana| Canada| 2000| |2018| Banana| China| 400| |2018| Banana| Mexico| null| |2018|Carrots| Canada| 2000| |2018|Carrots| China| 1200| |2018|Carrots| Mexico| null| |2019| Orange| Canada| 5000| |2019| Orange| China| null| |2019| Orange| Mexico| 5000| |2019| Beans| Canada| null| |2019| Beans| China| 1500| |2019| Beans| Mexico| 2000| |2019| Banana| Canada| null| |2019| Banana| China| 1400| |2019| Banana| Mexico| 400| |2019|Carrots| Canada| null| |2019|Carrots| China| 200| |2019|Carrots| Mexico| null| +----+-------+-------+-----+ <BLANKLINE> \"\"\" pivoted_columns = [( c , t ) for ( c , t ) in df . dtypes if c not in pivot_columns ] cols , types = zip ( * pivoted_columns ) # Check that all columns have the same type. assert_true ( len ( set ( types )) == 1 , ( \"All pivoted columns should be of the same type: \\n Pivoted columns are: %s \" % pivoted_columns ), ) # Create and explode an array of (column_name, column_value) structs kvs = f . explode ( f . array ( * [ f . struct ( f . lit ( c ) . alias ( key_alias ), f . col ( quote ( c )) . alias ( value_alias )) for c in cols ]) ) . alias ( \"kvs\" ) return df . select ([ f . col ( c ) for c in quote_columns ( pivot_columns )] + [ kvs ]) . select ( quote_columns ( pivot_columns ) + [ \"kvs.*\" ] ) with_generic_typed_struct ( df : DataFrame , col_names : List [ str ]) -> DataFrame Transform the specified struct columns of a given Dataframe into generic typed struct columns with the following generic schema (based on https://spark.apache.org/docs/latest/sql-ref-datatypes.html ) : STRUCT< key: STRING, -- (name of the field inside the struct) type: STRING, -- (type of the field inside the struct) value: STRUCT< -- (all the fields will be null except for the one with the correct type) date: DATE, timestamp: TIMESTAMP, int: LONG, float: DOUBLE, boolean: BOOLEAN, string: STRING, bytes: BINARY > > The following spark types will be automatically cast into the more generic following types: tinyint , smallint , int -> bigint float , decimal -> double Parameters: Name Type Description Default df DataFrame The Dataframe to transform required col_names List [ str ] A list of column names to transform required Returns: Type Description DataFrame A Dataframe with the columns transformed into generic typed structs Limitations Currently, complex field types (structs, maps, arrays) are not supported. All fields of the struct columns to convert must be of basic types. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ( ... [( 1 , { \"first.name\" : \"Jacques\" , \"age\" : 25 , \"is.an.adult\" : True }), ... ( 2 , { \"first.name\" : \"Michel\" , \"age\" : 12 , \"is.an.adult\" : False }), ... ( 3 , { \"first.name\" : \"Marie\" , \"age\" : 36 , \"is.an.adult\" : True })], ... \"id INT, `person.struct` STRUCT<`first.name`:STRING, age:INT, `is.an.adult`:BOOLEAN>\" ... ) >>> df . show ( truncate = False ) +---+-------------------+ |id |person.struct | +---+-------------------+ |1 |{Jacques, 25, true}| |2 |{Michel, 12, false}| |3 |{Marie, 36, true} | +---+-------------------+ >>> res = with_generic_typed_struct ( df , [ \"`person.struct`\" ]) >>> res . printSchema () root |-- id: integer (nullable = true) |-- person.struct: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- key: string (nullable = false) | | |-- type: string (nullable = false) | | |-- value: struct (nullable = false) | | | |-- boolean: boolean (nullable = true) | | | |-- bytes: binary (nullable = true) | | | |-- date: date (nullable = true) | | | |-- float: double (nullable = true) | | | |-- int: long (nullable = true) | | | |-- string: string (nullable = true) | | | |-- timestamp: timestamp (nullable = true) >>> res . show ( 10 , False ) # noqa: E501 +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |id |person.struct | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |[{first.name, string, {null, null, null, null, null, Jacques, null}}, {age, int, {null, null, null, null, 25, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}]| |2 |[{first.name, string, {null, null, null, null, null, Michel, null}}, {age, int, {null, null, null, null, 12, null, null}}, {is.an.adult, boolean, {false, null, null, null, null, null, null}}]| |3 |[{first.name, string, {null, null, null, null, null, Marie, null}}, {age, int, {null, null, null, null, 36, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}] | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Source code in spark_frame/transformations_impl/with_generic_typed_struct.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def with_generic_typed_struct ( df : DataFrame , col_names : List [ str ]) -> DataFrame : \"\"\"Transform the specified struct columns of a given [Dataframe][pyspark.sql.DataFrame] into generic typed struct columns with the following generic schema (based on [https://spark.apache.org/docs/latest/sql-ref-datatypes.html]( https://spark.apache.org/docs/latest/sql-ref-datatypes.html)) : STRUCT< key: STRING, -- (name of the field inside the struct) type: STRING, -- (type of the field inside the struct) value: STRUCT< -- (all the fields will be null except for the one with the correct type) date: DATE, timestamp: TIMESTAMP, int: LONG, float: DOUBLE, boolean: BOOLEAN, string: STRING, bytes: BINARY > > The following spark types will be automatically cast into the more generic following types: - `tinyint`, `smallint`, `int` -> `bigint` - `float`, `decimal` -> `double` Args: df: The Dataframe to transform col_names: A list of column names to transform Returns: A Dataframe with the columns transformed into generic typed structs !!! warning \"Limitations\" Currently, complex field types (structs, maps, arrays) are not supported. All fields of the struct columns to convert must be of basic types. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame( ... [(1, {\"first.name\": \"Jacques\", \"age\": 25, \"is.an.adult\": True}), ... (2, {\"first.name\": \"Michel\", \"age\": 12, \"is.an.adult\": False}), ... (3, {\"first.name\": \"Marie\", \"age\": 36, \"is.an.adult\": True})], ... \"id INT, `person.struct` STRUCT<`first.name`:STRING, age:INT, `is.an.adult`:BOOLEAN>\" ... ) >>> df.show(truncate=False) +---+-------------------+ |id |person.struct | +---+-------------------+ |1 |{Jacques, 25, true}| |2 |{Michel, 12, false}| |3 |{Marie, 36, true} | +---+-------------------+ <BLANKLINE> >>> res = with_generic_typed_struct(df, [\"`person.struct`\"]) >>> res.printSchema() root |-- id: integer (nullable = true) |-- person.struct: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- key: string (nullable = false) | | |-- type: string (nullable = false) | | |-- value: struct (nullable = false) | | | |-- boolean: boolean (nullable = true) | | | |-- bytes: binary (nullable = true) | | | |-- date: date (nullable = true) | | | |-- float: double (nullable = true) | | | |-- int: long (nullable = true) | | | |-- string: string (nullable = true) | | | |-- timestamp: timestamp (nullable = true) <BLANKLINE> >>> res.show(10, False) # noqa: E501 +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |id |person.struct | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |[{first.name, string, {null, null, null, null, null, Jacques, null}}, {age, int, {null, null, null, null, 25, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}]| |2 |[{first.name, string, {null, null, null, null, null, Michel, null}}, {age, int, {null, null, null, null, 12, null, null}}, {is.an.adult, boolean, {false, null, null, null, null, null, null}}]| |3 |[{first.name, string, {null, null, null, null, null, Marie, null}}, {age, int, {null, null, null, null, 36, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}] | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> \"\"\" source_to_cast = { \"date\" : \"date\" , \"timestamp\" : \"timestamp\" , \"tinyint\" : \"bigint\" , \"smallint\" : \"bigint\" , \"int\" : \"bigint\" , \"bigint\" : \"bigint\" , \"float\" : \"double\" , \"double\" : \"double\" , \"boolean\" : \"boolean\" , \"string\" : \"string\" , \"binary\" : \"binary\" , } \"\"\"Mapping indicating for each source Spark DataTypes the type into which it will be cast.\"\"\" cast_to_name = { \"binary\" : \"bytes\" , \"bigint\" : \"int\" , \"double\" : \"float\" , } \"\"\"Mapping indicating for each already cast Spark DataTypes the name of the corresponding field. When missing, the same name will be kept.\"\"\" name_cast = { cast_to_name . get ( value , value ): value for value in source_to_cast . values ()} # We make sure the types are sorted name_cast = { k : v for k , v in sorted ( name_cast . items ())} def match_regex_types ( source_type : str ) -> Optional [ str ]: \"\"\"Matches the source types against regexes to identify more complex types (like Decimal(x, y))\"\"\" regex_to_cast_types = [( re . compile ( \"decimal(.*)\" ), \"float\" )] for regex , cast_type in regex_to_cast_types : if regex . match ( source_type ) is not None : return cast_type return None def field_to_col ( field : StructField , column_name : str ) -> Optional [ Column ]: \"\"\"Transforms the specified field into a generic column\"\"\" source_type = field . dataType . simpleString () cast_type = source_to_cast . get ( source_type ) field_name = column_name + \".\" + quote ( field . name ) if cast_type is None : cast_type = match_regex_types ( source_type ) if cast_type is None : print ( \"WARNING: The field {field_name} is of type {source_type} which is currently unsupported. \" \"This field will be dropped.\" . format ( field_name = field_name , source_type = source_type ) ) return None name_type = cast_to_name . get ( cast_type , cast_type ) return f . struct ( f . lit ( field . name ) . alias ( \"key\" ), f . lit ( name_type ) . alias ( \"type\" ), # In the code below, we use f.expr instead of f.col because it looks like f.col # does not support column names with backquotes in them, but f.expr does :-p f . struct ( * [ ( f . expr ( field_name ) if name_type == name_t else f . lit ( None )) . astype ( cast_t ) . alias ( name_t ) for name_t , cast_t in name_cast . items () ] ) . alias ( \"value\" ), ) for col_name in col_names : schema = _get_nested_col_type_from_schema ( col_name , df . schema ) assert_true ( isinstance ( schema , StructType )) schema = cast ( StructType , schema ) columns = [ field_to_col ( field , col_name ) for field in schema . fields ] columns_2 = [ col for col in columns if col is not None ] df = df . withColumn ( unquote ( col_name ), f . array ( * columns_2 ) . alias ( \"values\" )) return df","title":"spark_frame.transformations"},{"location":"reference/transformations/#spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays","text":"Transform all columns of type Map<K,V> inside the given DataFrame into ARRAY<STRUCT<key: K, value: V>> . This transformation works recursively on every nesting level. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description DataFrame A new DataFrame in which all maps have been replaced with arrays of entries. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(MAP(1, STRUCT(MAP(1, \"a\") as m2))) as m1' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: map (containsNull = false) | | |-- key: integer | | |-- value: struct (valueContainsNull = false) | | | |-- m2: map (nullable = false) | | | | |-- key: integer | | | | |-- value: string (valueContainsNull = false) >>> df . show () +---+-------------------+ | id| m1| +---+-------------------+ | 1|[{1 -> {{1 -> a}}}]| +---+-------------------+ >>> res_df = convert_all_maps_to_arrays ( df ) >>> res_df . printSchema () root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- key: integer (nullable = false) | | | |-- value: struct (nullable = false) | | | | |-- m2: array (nullable = false) | | | | | |-- element: struct (containsNull = false) | | | | | | |-- key: integer (nullable = false) | | | | | | |-- value: string (nullable = false) >>> res_df . show () +---+-------------------+ | id| m1| +---+-------------------+ | 1|[[{1, {[{1, a}]}}]]| +---+-------------------+ Source code in spark_frame/transformations_impl/convert_all_maps_to_arrays.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def convert_all_maps_to_arrays ( df : DataFrame ) -> DataFrame : \"\"\"Transform all columns of type `Map<K,V>` inside the given DataFrame into `ARRAY<STRUCT<key: K, value: V>>`. This transformation works recursively on every nesting level. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Args: df: A Spark DataFrame Returns: A new DataFrame in which all maps have been replaced with arrays of entries. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('SELECT 1 as id, ARRAY(MAP(1, STRUCT(MAP(1, \"a\") as m2))) as m1') >>> df.printSchema() root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: map (containsNull = false) | | |-- key: integer | | |-- value: struct (valueContainsNull = false) | | | |-- m2: map (nullable = false) | | | | |-- key: integer | | | | |-- value: string (valueContainsNull = false) <BLANKLINE> >>> df.show() +---+-------------------+ | id| m1| +---+-------------------+ | 1|[{1 -> {{1 -> a}}}]| +---+-------------------+ <BLANKLINE> >>> res_df = convert_all_maps_to_arrays(df) >>> res_df.printSchema() root |-- id: integer (nullable = false) |-- m1: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- key: integer (nullable = false) | | | |-- value: struct (nullable = false) | | | | |-- m2: array (nullable = false) | | | | | |-- element: struct (containsNull = false) | | | | | | |-- key: integer (nullable = false) | | | | | | |-- value: string (nullable = false) <BLANKLINE> >>> res_df.show() +---+-------------------+ | id| m1| +---+-------------------+ | 1|[[{1, {[{1, a}]}}]]| +---+-------------------+ <BLANKLINE> \"\"\" def map_to_arrays ( col : Column , data_type : DataType ): if isinstance ( data_type , MapType ): return f . map_entries ( col ) return transform_all_fields ( df , map_to_arrays )","title":"convert_all_maps_to_arrays()"},{"location":"reference/transformations/#spark_frame.transformations_impl.flatten.flatten","text":"Flatten all the struct columns of a Spark DataFrame . Nested fields names will be joined together using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required struct_separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ( ... [( 1 , { \"a\" : 1 , \"b\" : { \"c\" : 1 , \"d\" : 1 }})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> flatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> df = spark . createDataFrame ( ... [( 1 , { \"a.a1\" : 1 , \"b.b1\" : { \"c.c1\" : 1 , \"d.d1\" : 1 }})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) >>> flatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) Source code in spark_frame/transformations_impl/flatten.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def flatten ( df : DataFrame , struct_separator : str = \".\" ) -> DataFrame : \"\"\"Flatten all the struct columns of a Spark [DataFrame][pyspark.sql.DataFrame]. Nested fields names will be joined together using the specified separator Args: df: A Spark DataFrame struct_separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame( ... [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> flatten(df).printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame( ... [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) <BLANKLINE> >>> flatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT s.b.c as `s.b.c`\" for each nested column. cols = [] def expand_struct ( struct : StructType , col_stack : List [ str ]) -> None : for field in struct : if type ( field . dataType ) == StructType : struct_field = cast ( StructType , field . dataType ) expand_struct ( struct_field , col_stack + [ field . name ]) else : column = f . col ( \".\" . join ( quote_columns ( col_stack + [ field . name ]))) cols . append ( column . alias ( struct_separator . join ( col_stack + [ field . name ]))) expand_struct ( df . schema , col_stack = []) return df . select ( cols )","title":"flatten()"},{"location":"reference/transformations/#spark_frame.transformations_impl.flatten_all_arrays.flatten_all_arrays","text":"Flatten all columns of type ARRAY<ARRAY<T>> inside the given DataFrame into ARRAY<<T>>> . This transformation works recursively on every nesting level. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and accepts field names containing dots ( . ), exclamation marks ( ! ) or percentage ( % ). Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description DataFrame A new DataFrame in which all arrays of array have been flattened Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(ARRAY(ARRAY(1, 2), ARRAY(3)), ARRAY(ARRAY(4), ARRAY(5))) as a' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: array (containsNull = false) | | | |-- element: integer (containsNull = false) >>> df . show ( truncate = False ) +---+---------------------------+ |id |a | +---+---------------------------+ |1 |[[[1, 2], [3]], [[4], [5]]]| +---+---------------------------+ >>> res_df = flatten_all_arrays ( df ) >>> res_df . printSchema () root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: integer (containsNull = false) >>> res_df . show ( truncate = False ) +---+---------------+ |id |a | +---+---------------+ |1 |[1, 2, 3, 4, 5]| +---+---------------+ Source code in spark_frame/transformations_impl/flatten_all_arrays.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def flatten_all_arrays ( df : DataFrame ) -> DataFrame : \"\"\"Flatten all columns of type `ARRAY<ARRAY<T>>` inside the given DataFrame into `ARRAY<<T>>>`. This transformation works recursively on every nesting level. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and accepts field names containing dots (`.`), exclamation marks (`!`) or percentage (`%`). Args: df: A Spark DataFrame Returns: A new DataFrame in which all arrays of array have been flattened Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('SELECT 1 as id, ARRAY(ARRAY(ARRAY(1, 2), ARRAY(3)), ARRAY(ARRAY(4), ARRAY(5))) as a') >>> df.printSchema() root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: array (containsNull = false) | | | |-- element: integer (containsNull = false) <BLANKLINE> >>> df.show(truncate=False) +---+---------------------------+ |id |a | +---+---------------------------+ |1 |[[[1, 2], [3]], [[4], [5]]]| +---+---------------------------+ <BLANKLINE> >>> res_df = flatten_all_arrays(df) >>> res_df.printSchema() root |-- id: integer (nullable = false) |-- a: array (nullable = false) | |-- element: integer (containsNull = false) <BLANKLINE> >>> res_df.show(truncate=False) +---+---------------+ |id |a | +---+---------------+ |1 |[1, 2, 3, 4, 5]| +---+---------------+ <BLANKLINE> \"\"\" def flatten_array ( col : Column , data_type : DataType ): if isinstance ( data_type , ArrayType ) and isinstance ( data_type . elementType , ArrayType ): return f . flatten ( col ) return transform_all_fields ( df , flatten_array )","title":"flatten_all_arrays()"},{"location":"reference/transformations/#spark_frame.transformations_impl.harmonize_dataframes.harmonize_dataframes","text":"Given two DataFrames, returns two new corresponding DataFrames with the same schemas by applying the following changes: Only common columns are kept Columns of type MAP are cast into ARRAY > Columns are re-order to have the same ordering in both DataFrames When matching columns have different types, their type is widened to their most narrow common type. This transformation is applied recursively on nested columns, including those inside repeated records (a.k.a. ARRAY<STRUCT<>>). Parameters: Name Type Description Default left_df DataFrame A Spark DataFrame required right_df DataFrame A Spark DataFrame required common_columns Optional [ Sequence [ Tuple [ str , Optional [ str ]]]] A list of (column name, type) tuples. Column names must appear in both DataFrames, and each column will be cast into the corresponding type. None Returns: Type Description Tuple [ DataFrame , DataFrame ] Two new Spark DataFrames with the same schema Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df1 = spark . sql ( 'SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3 as b, 4 as c)) as s2) as s1' ) >>> df2 = spark . sql ( 'SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3.0 as b, \"4\" as c, 5 as d)) as s2) as s1' ) >>> df1 . union ( df2 ) . show ( truncate = False ) Traceback (most recent call last): ... pyspark.sql.utils.AnalysisException : Union can only be performed on tables with the compatible column types. ... >>> df1 , df2 = harmonize_dataframes ( df1 , df2 ) >>> df1 . union ( df2 ) . show () +---+---------------+ | id| s1| +---+---------------+ | 1|{2, [{3.0, 4}]}| | 1|{2, [{3.0, 4}]}| +---+---------------+ Source code in spark_frame/transformations_impl/harmonize_dataframes.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def harmonize_dataframes ( left_df : DataFrame , right_df : DataFrame , common_columns : Optional [ Sequence [ Tuple [ str , Optional [ str ]]]] = None ) -> Tuple [ DataFrame , DataFrame ]: \"\"\"Given two DataFrames, returns two new corresponding DataFrames with the same schemas by applying the following changes: - Only common columns are kept - Columns of type MAP<key, value> are cast into ARRAY<STRUCT<key, value>> - Columns are re-order to have the same ordering in both DataFrames - When matching columns have different types, their type is widened to their most narrow common type. This transformation is applied recursively on nested columns, including those inside repeated records (a.k.a. ARRAY<STRUCT<>>). Args: left_df: A Spark DataFrame right_df: A Spark DataFrame common_columns: A list of (column name, type) tuples. Column names must appear in both DataFrames, and each column will be cast into the corresponding type. Returns: Two new Spark DataFrames with the same schema Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df1 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3 as b, 4 as c)) as s2) as s1') >>> df2 = spark.sql('SELECT 1 as id, STRUCT(2 as a, ARRAY(STRUCT(3.0 as b, \"4\" as c, 5 as d)) as s2) as s1') >>> df1.union(df2).show(truncate=False) # doctest: +ELLIPSIS Traceback (most recent call last): ... pyspark.sql.utils.AnalysisException: Union can only be performed on tables with the compatible column types. ... >>> df1, df2 = harmonize_dataframes(df1, df2) >>> df1.union(df2).show() +---+---------------+ | id| s1| +---+---------------+ | 1|{2, [{3.0, 4}]}| | 1|{2, [{3.0, 4}]}| +---+---------------+ <BLANKLINE> \"\"\" if common_columns is None : left_schema_flat = flatten_schema ( left_df . schema , explode = True ) right_schema_flat = flatten_schema ( right_df . schema , explode = True ) common_columns = get_common_columns ( left_schema_flat , right_schema_flat ) def build_col ( col_name : str , col_type : Optional [ str ]) -> PrintableFunction : parent_structs = _deepest_granularity ( col_name ) if col_type is not None : tpe = cast ( str , col_type ) f1 = PrintableFunction ( lambda s : s . cast ( tpe ), lambda s : f \" { s } .cast( { tpe } )\" ) else : f1 = higher_order . identity f2 = higher_order . recursive_struct_get ( parent_structs ) return fp . compose ( f1 , f2 ) common_columns_dict = { col_name : build_col ( col_name , col_type ) for ( col_name , col_type ) in common_columns } tree = _build_nested_struct_tree ( common_columns_dict ) root_transformation = _build_transformation_from_tree ( tree ) return left_df . select ( * root_transformation ([ left_df ])), right_df . select ( * root_transformation ([ right_df ]))","title":"harmonize_dataframes()"},{"location":"reference/transformations/#spark_frame.transformations_impl.parse_json_columns.parse_json_columns","text":"Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's from_json function, with one main difference: from_json requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \" a.b.c \") (See Example 2). Parameters: Name Type Description Default df DataFrame A Spark DataFrame required columns Union [ str , List [ str ], Dict [ str , str ]] A column name, list of column names, or dict(column_name, parsed_column_name) required Returns: Type Description DataFrame A new DataFrame Examples: Example 1 : >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([ ... ( 1 , '[{\"a\": 1}, {\"a\": 2}]' ), ... ( 1 , '[{\"a\": 2}, {\"a\": 4}]' ), ... ( 2 , None ) ... ], \"id INT, json1 STRING\" ... ) >>> df . show () +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) >>> parse_json_columns ( df , 'json1' ) . printSchema () root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 2 : different output column name : >>> parse_json_columns ( df , { 'json1' : 'parsed_json1' }) . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 3 : json inside a struct : >>> df = spark . createDataFrame ([ ... ( 1 , { 'json1' : '[{\"a\": 1}, {\"a\": 2}]' }), ... ( 1 , { 'json1' : '[{\"a\": 2}, {\"a\": 4}]' }), ... ( 2 , None ) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df . show ( 10 , False ) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) >>> res = parse_json_columns ( df , 'struct.json1' ) >>> res . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) >>> res . show ( 10 , False ) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ Source code in spark_frame/transformations_impl/parse_json_columns.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def parse_json_columns ( df : DataFrame , columns : Union [ str , List [ str ], Dict [ str , str ]]) -> DataFrame : \"\"\"Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's `from_json` function, with one main difference: `from_json` requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \"`a.b.c`\") (See Example 2). Args: df: A Spark DataFrame columns: A column name, list of column names, or dict(column_name, parsed_column_name) Returns: A new DataFrame Examples: **Example 1 :** >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([ ... (1, '[{\"a\": 1}, {\"a\": 2}]'), ... (1, '[{\"a\": 2}, {\"a\": 4}]'), ... (2, None) ... ], \"id INT, json1 STRING\" ... ) >>> df.show() +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) <BLANKLINE> >>> parse_json_columns(df, 'json1').printSchema() root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 2 : different output column name :** >>> parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 3 : json inside a struct :** >>> df = spark.createDataFrame([ ... (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}), ... (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}), ... (2, None) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df.show(10, False) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) <BLANKLINE> >>> res = parse_json_columns(df, 'struct.json1') >>> res.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> >>> res.show(10, False) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ <BLANKLINE> \"\"\" if isinstance ( columns , str ): columns = [ columns ] if isinstance ( columns , list ): columns = { col : col for col in columns } wrapped_df = __wrap_json_columns ( df , columns ) schema_per_col = __infer_schema_per_column ( wrapped_df , list ( columns . values ())) res = __parse_json_columns ( wrapped_df , schema_per_col ) return res","title":"parse_json_columns()"},{"location":"reference/transformations/#spark_frame.transformations_impl.sort_all_arrays.sort_all_arrays","text":"Given a DataFrame, sort all fields of type ARRAY in a canonical order, making them comparable. This also applies to nested fields, even those inside other arrays. Limitation Arrays containing sub-fields of type Map cannot be sorted, as the Map type is not comparable. A possible workaround is to first use the transformation spark_frame.transformations.convert_all_maps_to_arrays Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Returns: Type Description DataFrame A new DataFrame where all arrays have been sorted. Examples: Example 1: with a simple ARRAY<INT> >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(3, 2, 1) as a' ) >>> df . show () +---+---------+ | id| a| +---+---------+ | 1|[3, 2, 1]| +---+---------+ >>> sort_all_arrays ( df ) . show () +---+---------+ | id| a| +---+---------+ | 1|[1, 2, 3]| +---+---------+ Example 2: with an ARRAY<STRUCT<...>> >>> df = spark . sql ( 'SELECT ARRAY(STRUCT(2 as a, 1 as b), STRUCT(1 as a, 2 as b), STRUCT(1 as a, 1 as b)) as s' ) >>> df . show ( truncate = False ) +------------------------+ |s | +------------------------+ |[{2, 1}, {1, 2}, {1, 1}]| +------------------------+ >>> df . transform ( sort_all_arrays ) . show ( truncate = False ) +------------------------+ |s | +------------------------+ |[{1, 1}, {1, 2}, {2, 1}]| +------------------------+ Example 3: with an ARRAY<STRUCT<STRUCT<...>>> >>> df = spark . sql ( '''SELECT ARRAY( ... STRUCT(STRUCT(2 as a, 2 as b) as s), ... STRUCT(STRUCT(1 as a, 2 as b) as s) ... ) as l1 ... ''' ) >>> df . show ( truncate = False ) +--------------------+ |l1 | +--------------------+ |[{{2, 2}}, {{1, 2}}]| +--------------------+ >>> df . transform ( sort_all_arrays ) . show ( truncate = False ) +--------------------+ |l1 | +--------------------+ |[{{1, 2}}, {{2, 2}}]| +--------------------+ Example 4: with an ARRAY<ARRAY<ARRAY<INT>>> As this example shows, the innermost arrays are sorted before the outermost arrays. >>> df = spark . sql ( '''SELECT ARRAY( ... ARRAY(ARRAY(4, 1), ARRAY(3, 2)), ... ARRAY(ARRAY(2, 2), ARRAY(2, 1)) ... ) as l1 ... ''' ) >>> df . show ( truncate = False ) +------------------------------------+ |l1 | +------------------------------------+ |[[[4, 1], [3, 2]], [[2, 2], [2, 1]]]| +------------------------------------+ >>> df . transform ( sort_all_arrays ) . show ( truncate = False ) +------------------------------------+ |l1 | +------------------------------------+ |[[[1, 2], [2, 2]], [[1, 4], [2, 3]]]| +------------------------------------+ Source code in spark_frame/transformations_impl/sort_all_arrays.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def sort_all_arrays ( df : DataFrame ) -> DataFrame : \"\"\"Given a DataFrame, sort all fields of type `ARRAY` in a canonical order, making them comparable. This also applies to nested fields, even those inside other arrays. !!! warning \"Limitation\" - Arrays containing sub-fields of type Map cannot be sorted, as the Map type is not comparable. - A possible workaround is to first use the transformation [`spark_frame.transformations.convert_all_maps_to_arrays`] [spark_frame.transformations_impl.convert_all_maps_to_arrays.convert_all_maps_to_arrays] Args: df: A Spark DataFrame Returns: A new DataFrame where all arrays have been sorted. Examples: *Example 1:* with a simple `ARRAY<INT>` >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('SELECT 1 as id, ARRAY(3, 2, 1) as a') >>> df.show() +---+---------+ | id| a| +---+---------+ | 1|[3, 2, 1]| +---+---------+ <BLANKLINE> >>> sort_all_arrays(df).show() +---+---------+ | id| a| +---+---------+ | 1|[1, 2, 3]| +---+---------+ <BLANKLINE> *Example 2:* with an `ARRAY<STRUCT<...>>` >>> df = spark.sql('SELECT ARRAY(STRUCT(2 as a, 1 as b), STRUCT(1 as a, 2 as b), STRUCT(1 as a, 1 as b)) as s') >>> df.show(truncate=False) +------------------------+ |s | +------------------------+ |[{2, 1}, {1, 2}, {1, 1}]| +------------------------+ <BLANKLINE> >>> df.transform(sort_all_arrays).show(truncate=False) +------------------------+ |s | +------------------------+ |[{1, 1}, {1, 2}, {2, 1}]| +------------------------+ <BLANKLINE> *Example 3:* with an `ARRAY<STRUCT<STRUCT<...>>>` >>> df = spark.sql('''SELECT ARRAY( ... STRUCT(STRUCT(2 as a, 2 as b) as s), ... STRUCT(STRUCT(1 as a, 2 as b) as s) ... ) as l1 ... ''') >>> df.show(truncate=False) +--------------------+ |l1 | +--------------------+ |[{{2, 2}}, {{1, 2}}]| +--------------------+ <BLANKLINE> >>> df.transform(sort_all_arrays).show(truncate=False) +--------------------+ |l1 | +--------------------+ |[{{1, 2}}, {{2, 2}}]| +--------------------+ <BLANKLINE> *Example 4:* with an `ARRAY<ARRAY<ARRAY<INT>>>` As this example shows, the innermost arrays are sorted before the outermost arrays. >>> df = spark.sql('''SELECT ARRAY( ... ARRAY(ARRAY(4, 1), ARRAY(3, 2)), ... ARRAY(ARRAY(2, 2), ARRAY(2, 1)) ... ) as l1 ... ''') >>> df.show(truncate=False) +------------------------------------+ |l1 | +------------------------------------+ |[[[4, 1], [3, 2]], [[2, 2], [2, 1]]]| +------------------------------------+ <BLANKLINE> >>> df.transform(sort_all_arrays).show(truncate=False) +------------------------------------+ |l1 | +------------------------------------+ |[[[1, 2], [2, 2]], [[1, 4], [2, 3]]]| +------------------------------------+ <BLANKLINE> \"\"\" def sort_array ( col : Column , data_type : DataType ): if isinstance ( data_type , ArrayType ): return f . sort_array ( col ) return transform_all_fields ( df , sort_array )","title":"sort_all_arrays()"},{"location":"reference/transformations/#spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names","text":"Apply a transformation to all nested field names of a DataFrame. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Parameters: Name Type Description Default df DataFrame A Spark DataFrame required transformation Callable [[ str ], str ] Transformation to apply to all field names in the DataFrame. required Returns: Type Description DataFrame A new DataFrame Examples: Example 1: with a nested schema structure In this example we cast all the field names of the schema to uppercase: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> new_df = df . transform ( transform_all_field_names , lambda s : s . upper ()) >>> nested . print_schema ( new_df ) root |-- NAME: string (nullable = false) |-- S1!.A: integer (nullable = false) |-- S2!!: integer (nullable = false) |-- S3!!.A: integer (nullable = false) |-- S4!.A!: integer (nullable = false) |-- S5!.A!.B.C: integer (nullable = false) Example 2: sanitizing field names In this example we replace all dots and exclamation marks in field names with underscores. This is useful to make a DataFrame compatible with the spark_frame.nested module. >>> df = spark . sql ( '''SELECT ... ARRAY(STRUCT( ... ARRAY(STRUCT( ... STRUCT(1 as `d.d!d`) as `c.c!c` ... )) as `b.b!b` ... )) as `a.a!a` ... ''' ) >>> df . printSchema () root |-- a.a!a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b.b!b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c.c!c: struct (nullable = false) | | | | | |-- d.d!d: integer (nullable = false) >>> new_df = df . transform ( transform_all_field_names , lambda s : s . replace ( \".\" , \"_\" ) . replace ( \"!\" , \"_\" )) >>> new_df . printSchema () root |-- a_a_a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b_b_b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c_c_c: struct (nullable = false) | | | | | |-- d_d_d: integer (nullable = false) This also works on fields of type MAP<K,V> . >>> df = spark . sql ( 'SELECT MAP(STRUCT(1 as `a.a!a`), STRUCT(2 as `b.b!b`)) as `m.m!m`' ) >>> df . printSchema () root |-- m.m!m: map (nullable = false) | |-- key: struct | | |-- a.a!a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b.b!b: integer (nullable = false) >>> new_df = df . transform ( transform_all_field_names , lambda s : s . replace ( \".\" , \"_\" ) . replace ( \"!\" , \"_\" )) >>> new_df . printSchema () root |-- m_m_m: map (nullable = false) | |-- key: struct | | |-- a_a_a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b_b_b: integer (nullable = false) Source code in spark_frame/transformations_impl/transform_all_field_names.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def transform_all_field_names ( df : DataFrame , transformation : Callable [[ str ], str ]) -> DataFrame : \"\"\"Apply a transformation to all nested field names of a DataFrame. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Args: df: A Spark DataFrame transformation: Transformation to apply to all field names in the DataFrame. Returns: A new DataFrame Examples: _**Example 1: with a nested schema structure**_ In this example we cast all the field names of the schema to uppercase: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> new_df = df.transform(transform_all_field_names, lambda s: s.upper()) >>> nested.print_schema(new_df) root |-- NAME: string (nullable = false) |-- S1!.A: integer (nullable = false) |-- S2!!: integer (nullable = false) |-- S3!!.A: integer (nullable = false) |-- S4!.A!: integer (nullable = false) |-- S5!.A!.B.C: integer (nullable = false) <BLANKLINE> _**Example 2: sanitizing field names**_ In this example we replace all dots and exclamation marks in field names with underscores. This is useful to make a DataFrame compatible with the [spark_frame.nested](/spark-frame/reference/nested) module. >>> df = spark.sql('''SELECT ... ARRAY(STRUCT( ... ARRAY(STRUCT( ... STRUCT(1 as `d.d!d`) as `c.c!c` ... )) as `b.b!b` ... )) as `a.a!a` ... ''') >>> df.printSchema() root |-- a.a!a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b.b!b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c.c!c: struct (nullable = false) | | | | | |-- d.d!d: integer (nullable = false) <BLANKLINE> >>> new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\")) >>> new_df.printSchema() root |-- a_a_a: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- b_b_b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c_c_c: struct (nullable = false) | | | | | |-- d_d_d: integer (nullable = false) <BLANKLINE> This also works on fields of type `MAP<K,V>`. >>> df = spark.sql('SELECT MAP(STRUCT(1 as `a.a!a`), STRUCT(2 as `b.b!b`)) as `m.m!m`') >>> df.printSchema() root |-- m.m!m: map (nullable = false) | |-- key: struct | | |-- a.a!a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b.b!b: integer (nullable = false) <BLANKLINE> >>> new_df = df.transform(transform_all_field_names, lambda s: s.replace(\".\",\"_\").replace(\"!\", \"_\")) >>> new_df.printSchema() root |-- m_m_m: map (nullable = false) | |-- key: struct | | |-- a_a_a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b_b_b: integer (nullable = false) <BLANKLINE> \"\"\" root_transformation = build_transformation_from_schema ( df . schema , name_transformation = transformation ) return df . select ( * root_transformation ( df ))","title":"transform_all_field_names()"},{"location":"reference/transformations/#spark_frame.transformations_impl.transform_all_fields.transform_all_fields","text":"Apply a transformation to all nested fields of a DataFrame. Info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Parameters: Name Type Description Default df DataFrame A Spark DataFrame required transformation Callable [[ Column , DataType ], Column ] Transformation to apply to all fields of the DataFrame. The transformation must take as input a Column expression and the DataType of the corresponding expression. required Returns: Type Description DataFrame A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''' ) >>> nested . print_schema ( df ) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) >>> df . show ( truncate = False ) +----+----------+----------------+--------------+--------------------+------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+----------+----------------+--------------+--------------------+------------------------------------+ |John|[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +----+----------+----------------+--------------+--------------------+------------------------------------+ >>> from pyspark.sql.types import IntegerType >>> def cast_int_as_double ( col : Column , data_type : DataType ): ... if isinstance ( data_type , IntegerType ): ... return col . cast ( \"DOUBLE\" ) >>> new_df = df . transform ( transform_all_fields , cast_int_as_double ) >>> nested . print_schema ( new_df ) root |-- name: string (nullable = false) |-- s1!.a: double (nullable = false) |-- s2!!: double (nullable = false) |-- s3!!.a: double (nullable = false) |-- s4!.a!: double (nullable = false) |-- s5!.a!.b.c: double (nullable = false) >>> new_df . show ( truncate = False ) # noqa: E501 +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |John|[{1.0}, {2.0}]|[[1.0, 2.0], [3.0, 4.0]]|[[{1.0}], [{2.0}]]|[{[1.0, 2.0]}, {[3.0, 4.0]}]|[{[{{1.0}}, {{2.0}}]}, {[{{3.0}}, {{4.0}}]}]| +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ Source code in spark_frame/transformations_impl/transform_all_fields.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def transform_all_fields ( df : DataFrame , transformation : Callable [[ Column , DataType ], Column ]) -> DataFrame : \"\"\"Apply a transformation to all nested fields of a DataFrame. !!! info This method is compatible with any schema. It recursively applies on structs, arrays and maps and is compatible with field names containing special characters. Args: df: A Spark DataFrame transformation: Transformation to apply to all fields of the DataFrame. The transformation must take as input a Column expression and the DataType of the corresponding expression. Returns: A new DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... \"John\" as name, ... ARRAY(STRUCT(1 as a), STRUCT(2 as a)) as s1, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s2, ... ARRAY(ARRAY(STRUCT(1 as a)), ARRAY(STRUCT(2 as a))) as s3, ... ARRAY(STRUCT(ARRAY(1, 2) as a), STRUCT(ARRAY(3, 4) as a)) as s4, ... ARRAY( ... STRUCT(ARRAY(STRUCT(STRUCT(1 as c) as b), STRUCT(STRUCT(2 as c) as b)) as a), ... STRUCT(ARRAY(STRUCT(STRUCT(3 as c) as b), STRUCT(STRUCT(4 as c) as b)) as a) ... ) as s5 ... ''') >>> nested.print_schema(df) root |-- name: string (nullable = false) |-- s1!.a: integer (nullable = false) |-- s2!!: integer (nullable = false) |-- s3!!.a: integer (nullable = false) |-- s4!.a!: integer (nullable = false) |-- s5!.a!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +----+----------+----------------+--------------+--------------------+------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+----------+----------------+--------------+--------------------+------------------------------------+ |John|[{1}, {2}]|[[1, 2], [3, 4]]|[[{1}], [{2}]]|[{[1, 2]}, {[3, 4]}]|[{[{{1}}, {{2}}]}, {[{{3}}, {{4}}]}]| +----+----------+----------------+--------------+--------------------+------------------------------------+ <BLANKLINE> >>> from pyspark.sql.types import IntegerType >>> def cast_int_as_double(col: Column, data_type: DataType): ... if isinstance(data_type, IntegerType): ... return col.cast(\"DOUBLE\") >>> new_df = df.transform(transform_all_fields, cast_int_as_double) >>> nested.print_schema(new_df) root |-- name: string (nullable = false) |-- s1!.a: double (nullable = false) |-- s2!!: double (nullable = false) |-- s3!!.a: double (nullable = false) |-- s4!.a!: double (nullable = false) |-- s5!.a!.b.c: double (nullable = false) <BLANKLINE> >>> new_df.show(truncate=False) # noqa: E501 +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |name|s1 |s2 |s3 |s4 |s5 | +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ |John|[{1.0}, {2.0}]|[[1.0, 2.0], [3.0, 4.0]]|[[{1.0}], [{2.0}]]|[{[1.0, 2.0]}, {[3.0, 4.0]}]|[{[{{1.0}}, {{2.0}}]}, {[{{3.0}}, {{4.0}}]}]| +----+--------------+------------------------+------------------+----------------------------+--------------------------------------------+ <BLANKLINE> \"\"\" root_transformation = build_transformation_from_schema ( df . schema , column_transformation = transformation ) return df . select ( * root_transformation ( df ))","title":"transform_all_fields()"},{"location":"reference/transformations/#spark_frame.transformations_impl.unflatten.unflatten","text":"Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([( 1 , 1 , 1 , 1 )], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> unflatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> df = spark . createDataFrame ([( 1 , 1 , 1 )], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) >>> unflatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) Source code in spark_frame/transformations_impl/unflatten.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def unflatten ( df : DataFrame , separator : str = \".\" ) -> DataFrame : \"\"\"Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Args: df: A Spark DataFrame separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> unflatten(df).printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) <BLANKLINE> >>> unflatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT struct(a, struct(s.b.c, s.b.d)) as s\" for each nested column. # There is a little twist as we don't want to rebuild the struct if all its fields are null, so we add a CASE WHEN def has_structs ( df : DataFrame ) -> bool : struct_fields = [ field for field in df . schema if is_struct ( field )] return len ( struct_fields ) > 0 if has_structs ( df ): df = flatten ( df ) tree = _build_nested_struct_tree ( df . columns , separator ) cols = _build_struct_from_tree ( tree , separator ) return df . select ( cols )","title":"unflatten()"},{"location":"reference/transformations/#spark_frame.transformations_impl.union_dataframes.union_dataframes","text":"Returns the union between multiple DataFrames Parameters: Name Type Description Default dfs DataFrame One or more Spark DataFrames () Returns: Type Description DataFrame A new DataFrame containing the union of all input DataFrames Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df1 = spark . sql ( 'SELECT 1 as a' ) >>> df2 = spark . sql ( 'SELECT 2 as a' ) >>> df3 = spark . sql ( 'SELECT 3 as a' ) >>> union_dataframes ( df1 , df2 , df3 ) . show () +---+ | a| +---+ | 1| | 2| | 3| +---+ >>> df1 . transform ( union_dataframes , df2 , df3 ) . show () +---+ | a| +---+ | 1| | 2| | 3| +---+ Source code in spark_frame/transformations_impl/union_dataframes.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def union_dataframes ( * dfs : DataFrame ) -> DataFrame : \"\"\"Returns the union between multiple DataFrames Args: dfs: One or more Spark DataFrames Returns: A new DataFrame containing the union of all input DataFrames Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df1 = spark.sql('SELECT 1 as a') >>> df2 = spark.sql('SELECT 2 as a') >>> df3 = spark.sql('SELECT 3 as a') >>> union_dataframes(df1, df2, df3).show() +---+ | a| +---+ | 1| | 2| | 3| +---+ <BLANKLINE> >>> df1.transform(union_dataframes, df2, df3).show() +---+ | a| +---+ | 1| | 2| | 3| +---+ <BLANKLINE> \"\"\" assert_true ( len ( dfs ) > 0 , ValueError ( \"Input list is empty\" )) res = dfs [ 0 ] for df in dfs [ 1 :]: res = res . union ( df ) return res","title":"union_dataframes()"},{"location":"reference/transformations/#spark_frame.transformations_impl.unpivot.unpivot","text":"Unpivot the given DataFrame along the specified pivot columns. All columns that are not pivot columns should have the same type. This is the inverse transformation of the pyspark.sql.GroupedData.pivot operation. Parameters: Name Type Description Default df DataFrame A DataFrame required pivot_columns List [ str ] The list of columns names on which to perform the pivot required key_alias str Alias given to the 'key' column 'key' value_alias str Alias given to the 'value' column 'value' Returns: Type Description DataFrame An unpivotted DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([ ... ( 2018 , \"Orange\" , None , 4000 , None ), ... ( 2018 , \"Beans\" , None , 1500 , 2000 ), ... ( 2018 , \"Banana\" , 2000 , 400 , None ), ... ( 2018 , \"Carrots\" , 2000 , 1200 , None ), ... ( 2019 , \"Orange\" , 5000 , None , 5000 ), ... ( 2019 , \"Beans\" , None , 1500 , 2000 ), ... ( 2019 , \"Banana\" , None , 1400 , 400 ), ... ( 2019 , \"Carrots\" , None , 200 , None ), ... ], \"year INT, product STRING, Canada INT, China INT, Mexico INT\" ... ) >>> df . show () +----+-------+------+-----+------+ |year|product|Canada|China|Mexico| +----+-------+------+-----+------+ |2018| Orange| null| 4000| null| |2018| Beans| null| 1500| 2000| |2018| Banana| 2000| 400| null| |2018|Carrots| 2000| 1200| null| |2019| Orange| 5000| null| 5000| |2019| Beans| null| 1500| 2000| |2019| Banana| null| 1400| 400| |2019|Carrots| null| 200| null| +----+-------+------+-----+------+ >>> unpivot ( df , [ 'year' , 'product' ], key_alias = 'country' , value_alias = 'total' ) . show ( 100 ) +----+-------+-------+-----+ |year|product|country|total| +----+-------+-------+-----+ |2018| Orange| Canada| null| |2018| Orange| China| 4000| |2018| Orange| Mexico| null| |2018| Beans| Canada| null| |2018| Beans| China| 1500| |2018| Beans| Mexico| 2000| |2018| Banana| Canada| 2000| |2018| Banana| China| 400| |2018| Banana| Mexico| null| |2018|Carrots| Canada| 2000| |2018|Carrots| China| 1200| |2018|Carrots| Mexico| null| |2019| Orange| Canada| 5000| |2019| Orange| China| null| |2019| Orange| Mexico| 5000| |2019| Beans| Canada| null| |2019| Beans| China| 1500| |2019| Beans| Mexico| 2000| |2019| Banana| Canada| null| |2019| Banana| China| 1400| |2019| Banana| Mexico| 400| |2019|Carrots| Canada| null| |2019|Carrots| China| 200| |2019|Carrots| Mexico| null| +----+-------+-------+-----+ Source code in spark_frame/transformations_impl/unpivot.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def unpivot ( df : DataFrame , pivot_columns : List [ str ], key_alias : str = \"key\" , value_alias : str = \"value\" ) -> DataFrame : \"\"\"Unpivot the given DataFrame along the specified pivot columns. All columns that are not pivot columns should have the same type. This is the inverse transformation of the [pyspark.sql.GroupedData.pivot][] operation. Args: df: A DataFrame pivot_columns: The list of columns names on which to perform the pivot key_alias: Alias given to the 'key' column value_alias: Alias given to the 'value' column Returns: An unpivotted DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([ ... (2018, \"Orange\", None, 4000, None), ... (2018, \"Beans\", None, 1500, 2000), ... (2018, \"Banana\", 2000, 400, None), ... (2018, \"Carrots\", 2000, 1200, None), ... (2019, \"Orange\", 5000, None, 5000), ... (2019, \"Beans\", None, 1500, 2000), ... (2019, \"Banana\", None, 1400, 400), ... (2019, \"Carrots\", None, 200, None), ... ], \"year INT, product STRING, Canada INT, China INT, Mexico INT\" ... ) >>> df.show() +----+-------+------+-----+------+ |year|product|Canada|China|Mexico| +----+-------+------+-----+------+ |2018| Orange| null| 4000| null| |2018| Beans| null| 1500| 2000| |2018| Banana| 2000| 400| null| |2018|Carrots| 2000| 1200| null| |2019| Orange| 5000| null| 5000| |2019| Beans| null| 1500| 2000| |2019| Banana| null| 1400| 400| |2019|Carrots| null| 200| null| +----+-------+------+-----+------+ <BLANKLINE> >>> unpivot(df, ['year', 'product'], key_alias='country', value_alias='total').show(100) +----+-------+-------+-----+ |year|product|country|total| +----+-------+-------+-----+ |2018| Orange| Canada| null| |2018| Orange| China| 4000| |2018| Orange| Mexico| null| |2018| Beans| Canada| null| |2018| Beans| China| 1500| |2018| Beans| Mexico| 2000| |2018| Banana| Canada| 2000| |2018| Banana| China| 400| |2018| Banana| Mexico| null| |2018|Carrots| Canada| 2000| |2018|Carrots| China| 1200| |2018|Carrots| Mexico| null| |2019| Orange| Canada| 5000| |2019| Orange| China| null| |2019| Orange| Mexico| 5000| |2019| Beans| Canada| null| |2019| Beans| China| 1500| |2019| Beans| Mexico| 2000| |2019| Banana| Canada| null| |2019| Banana| China| 1400| |2019| Banana| Mexico| 400| |2019|Carrots| Canada| null| |2019|Carrots| China| 200| |2019|Carrots| Mexico| null| +----+-------+-------+-----+ <BLANKLINE> \"\"\" pivoted_columns = [( c , t ) for ( c , t ) in df . dtypes if c not in pivot_columns ] cols , types = zip ( * pivoted_columns ) # Check that all columns have the same type. assert_true ( len ( set ( types )) == 1 , ( \"All pivoted columns should be of the same type: \\n Pivoted columns are: %s \" % pivoted_columns ), ) # Create and explode an array of (column_name, column_value) structs kvs = f . explode ( f . array ( * [ f . struct ( f . lit ( c ) . alias ( key_alias ), f . col ( quote ( c )) . alias ( value_alias )) for c in cols ]) ) . alias ( \"kvs\" ) return df . select ([ f . col ( c ) for c in quote_columns ( pivot_columns )] + [ kvs ]) . select ( quote_columns ( pivot_columns ) + [ \"kvs.*\" ] )","title":"unpivot()"},{"location":"reference/transformations/#spark_frame.transformations_impl.with_generic_typed_struct.with_generic_typed_struct","text":"Transform the specified struct columns of a given Dataframe into generic typed struct columns with the following generic schema (based on https://spark.apache.org/docs/latest/sql-ref-datatypes.html ) : STRUCT< key: STRING, -- (name of the field inside the struct) type: STRING, -- (type of the field inside the struct) value: STRUCT< -- (all the fields will be null except for the one with the correct type) date: DATE, timestamp: TIMESTAMP, int: LONG, float: DOUBLE, boolean: BOOLEAN, string: STRING, bytes: BINARY > > The following spark types will be automatically cast into the more generic following types: tinyint , smallint , int -> bigint float , decimal -> double Parameters: Name Type Description Default df DataFrame The Dataframe to transform required col_names List [ str ] A list of column names to transform required Returns: Type Description DataFrame A Dataframe with the columns transformed into generic typed structs Limitations Currently, complex field types (structs, maps, arrays) are not supported. All fields of the struct columns to convert must be of basic types. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ( ... [( 1 , { \"first.name\" : \"Jacques\" , \"age\" : 25 , \"is.an.adult\" : True }), ... ( 2 , { \"first.name\" : \"Michel\" , \"age\" : 12 , \"is.an.adult\" : False }), ... ( 3 , { \"first.name\" : \"Marie\" , \"age\" : 36 , \"is.an.adult\" : True })], ... \"id INT, `person.struct` STRUCT<`first.name`:STRING, age:INT, `is.an.adult`:BOOLEAN>\" ... ) >>> df . show ( truncate = False ) +---+-------------------+ |id |person.struct | +---+-------------------+ |1 |{Jacques, 25, true}| |2 |{Michel, 12, false}| |3 |{Marie, 36, true} | +---+-------------------+ >>> res = with_generic_typed_struct ( df , [ \"`person.struct`\" ]) >>> res . printSchema () root |-- id: integer (nullable = true) |-- person.struct: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- key: string (nullable = false) | | |-- type: string (nullable = false) | | |-- value: struct (nullable = false) | | | |-- boolean: boolean (nullable = true) | | | |-- bytes: binary (nullable = true) | | | |-- date: date (nullable = true) | | | |-- float: double (nullable = true) | | | |-- int: long (nullable = true) | | | |-- string: string (nullable = true) | | | |-- timestamp: timestamp (nullable = true) >>> res . show ( 10 , False ) # noqa: E501 +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |id |person.struct | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |[{first.name, string, {null, null, null, null, null, Jacques, null}}, {age, int, {null, null, null, null, 25, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}]| |2 |[{first.name, string, {null, null, null, null, null, Michel, null}}, {age, int, {null, null, null, null, 12, null, null}}, {is.an.adult, boolean, {false, null, null, null, null, null, null}}]| |3 |[{first.name, string, {null, null, null, null, null, Marie, null}}, {age, int, {null, null, null, null, 36, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}] | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Source code in spark_frame/transformations_impl/with_generic_typed_struct.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def with_generic_typed_struct ( df : DataFrame , col_names : List [ str ]) -> DataFrame : \"\"\"Transform the specified struct columns of a given [Dataframe][pyspark.sql.DataFrame] into generic typed struct columns with the following generic schema (based on [https://spark.apache.org/docs/latest/sql-ref-datatypes.html]( https://spark.apache.org/docs/latest/sql-ref-datatypes.html)) : STRUCT< key: STRING, -- (name of the field inside the struct) type: STRING, -- (type of the field inside the struct) value: STRUCT< -- (all the fields will be null except for the one with the correct type) date: DATE, timestamp: TIMESTAMP, int: LONG, float: DOUBLE, boolean: BOOLEAN, string: STRING, bytes: BINARY > > The following spark types will be automatically cast into the more generic following types: - `tinyint`, `smallint`, `int` -> `bigint` - `float`, `decimal` -> `double` Args: df: The Dataframe to transform col_names: A list of column names to transform Returns: A Dataframe with the columns transformed into generic typed structs !!! warning \"Limitations\" Currently, complex field types (structs, maps, arrays) are not supported. All fields of the struct columns to convert must be of basic types. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame( ... [(1, {\"first.name\": \"Jacques\", \"age\": 25, \"is.an.adult\": True}), ... (2, {\"first.name\": \"Michel\", \"age\": 12, \"is.an.adult\": False}), ... (3, {\"first.name\": \"Marie\", \"age\": 36, \"is.an.adult\": True})], ... \"id INT, `person.struct` STRUCT<`first.name`:STRING, age:INT, `is.an.adult`:BOOLEAN>\" ... ) >>> df.show(truncate=False) +---+-------------------+ |id |person.struct | +---+-------------------+ |1 |{Jacques, 25, true}| |2 |{Michel, 12, false}| |3 |{Marie, 36, true} | +---+-------------------+ <BLANKLINE> >>> res = with_generic_typed_struct(df, [\"`person.struct`\"]) >>> res.printSchema() root |-- id: integer (nullable = true) |-- person.struct: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- key: string (nullable = false) | | |-- type: string (nullable = false) | | |-- value: struct (nullable = false) | | | |-- boolean: boolean (nullable = true) | | | |-- bytes: binary (nullable = true) | | | |-- date: date (nullable = true) | | | |-- float: double (nullable = true) | | | |-- int: long (nullable = true) | | | |-- string: string (nullable = true) | | | |-- timestamp: timestamp (nullable = true) <BLANKLINE> >>> res.show(10, False) # noqa: E501 +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |id |person.struct | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |[{first.name, string, {null, null, null, null, null, Jacques, null}}, {age, int, {null, null, null, null, 25, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}]| |2 |[{first.name, string, {null, null, null, null, null, Michel, null}}, {age, int, {null, null, null, null, 12, null, null}}, {is.an.adult, boolean, {false, null, null, null, null, null, null}}]| |3 |[{first.name, string, {null, null, null, null, null, Marie, null}}, {age, int, {null, null, null, null, 36, null, null}}, {is.an.adult, boolean, {true, null, null, null, null, null, null}}] | +---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ <BLANKLINE> \"\"\" source_to_cast = { \"date\" : \"date\" , \"timestamp\" : \"timestamp\" , \"tinyint\" : \"bigint\" , \"smallint\" : \"bigint\" , \"int\" : \"bigint\" , \"bigint\" : \"bigint\" , \"float\" : \"double\" , \"double\" : \"double\" , \"boolean\" : \"boolean\" , \"string\" : \"string\" , \"binary\" : \"binary\" , } \"\"\"Mapping indicating for each source Spark DataTypes the type into which it will be cast.\"\"\" cast_to_name = { \"binary\" : \"bytes\" , \"bigint\" : \"int\" , \"double\" : \"float\" , } \"\"\"Mapping indicating for each already cast Spark DataTypes the name of the corresponding field. When missing, the same name will be kept.\"\"\" name_cast = { cast_to_name . get ( value , value ): value for value in source_to_cast . values ()} # We make sure the types are sorted name_cast = { k : v for k , v in sorted ( name_cast . items ())} def match_regex_types ( source_type : str ) -> Optional [ str ]: \"\"\"Matches the source types against regexes to identify more complex types (like Decimal(x, y))\"\"\" regex_to_cast_types = [( re . compile ( \"decimal(.*)\" ), \"float\" )] for regex , cast_type in regex_to_cast_types : if regex . match ( source_type ) is not None : return cast_type return None def field_to_col ( field : StructField , column_name : str ) -> Optional [ Column ]: \"\"\"Transforms the specified field into a generic column\"\"\" source_type = field . dataType . simpleString () cast_type = source_to_cast . get ( source_type ) field_name = column_name + \".\" + quote ( field . name ) if cast_type is None : cast_type = match_regex_types ( source_type ) if cast_type is None : print ( \"WARNING: The field {field_name} is of type {source_type} which is currently unsupported. \" \"This field will be dropped.\" . format ( field_name = field_name , source_type = source_type ) ) return None name_type = cast_to_name . get ( cast_type , cast_type ) return f . struct ( f . lit ( field . name ) . alias ( \"key\" ), f . lit ( name_type ) . alias ( \"type\" ), # In the code below, we use f.expr instead of f.col because it looks like f.col # does not support column names with backquotes in them, but f.expr does :-p f . struct ( * [ ( f . expr ( field_name ) if name_type == name_t else f . lit ( None )) . astype ( cast_t ) . alias ( name_t ) for name_t , cast_t in name_cast . items () ] ) . alias ( \"value\" ), ) for col_name in col_names : schema = _get_nested_col_type_from_schema ( col_name , df . schema ) assert_true ( isinstance ( schema , StructType )) schema = cast ( StructType , schema ) columns = [ field_to_col ( field , col_name ) for field in schema . fields ] columns_2 = [ col for col in columns if col is not None ] df = df . withColumn ( unquote ( col_name ), f . array ( * columns_2 ) . alias ( \"values\" )) return df","title":"with_generic_typed_struct()"},{"location":"use_cases/flatten_unflatten/","text":"Transforming nested fields Warning The use case presented in this page is deprecated, but is kept to illustrate what flatten/unflatten can do. The spark_frame.nested module is much more powerful for manipulating nested data, because unlike flatten/unflatten, it does work with arrays. We recommend checking this use-case to see the spark_frame.nested module in action. This example demonstrates how the spark_frame.transformations.flatten and unflatten spark_frame.transformations.unflatten methods can be used to make data cleaning pipeline easier with PySpark. Let's take a sample DataFrame with our favorite example: Pokemons! >>> from spark_frame.examples.flatten_unflatten import _get_sample_pokemon_data >>> df = _get_sample_pokemon_data () >>> df . printSchema () root |-- base_stats: struct (nullable = true) | |-- Attack: long (nullable = true) | |-- Defense: long (nullable = true) | |-- HP: long (nullable = true) | |-- Sp Attack: long (nullable = true) | |-- Sp Defense: long (nullable = true) | |-- Speed: long (nullable = true) |-- id: long (nullable = true) |-- name: struct (nullable = true) | |-- english: string (nullable = true) | |-- french: string (nullable = true) |-- types: array (nullable = true) | |-- element: string (containsNull = true) >>> df . show ( vertical = True , truncate = False ) -RECORD 0------------------------------ base_stats | {49, 49, 45, 65, 65, 45} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] Let's say we want to add a new enrich the \"base_stats\" struct with a new field named \"Total\". Without spark-frame Of course, we could write something in DataFrame or SQL like this: >>> df . createOrReplaceTempView ( \"df\" ) >>> new_df = df . sparkSession . sql ( ''' ... SELECT ... STRUCT( ... base_stats.*, ... base_stats.Attack + base_stats.Defense + base_stats.HP + ... base_stats.`Sp Attack` + base_stats.`Sp Defense` + base_stats.Speed as Total ... ) as base_stats, ... id, ... name, ... types ... FROM df ... ''' ) . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] It works, but it is a little cumbersome. Imagine how ugly the query would look like with a much bigger table, with hundreds of columns with three levels of nesting or more... With spark-frame Instead, we can use the spark_frame.transformations.flatten and unflatten spark_frame.transformations.unflatten methods to reduce boilerplate significantly. >>> from spark_frame.transformations import flatten , unflatten >>> from pyspark.sql import functions as f >>> flat_df = flatten ( df ) >>> flat_df = flat_df . withColumn ( \"base_stats.Total\" , ... f . col ( \"`base_stats.Attack`\" ) + f . col ( \"`base_stats.Defense`\" ) + f . col ( \"`base_stats.HP`\" ) + ... f . col ( \"`base_stats.Sp Attack`\" ) + f . col ( \"`base_stats.Sp Defense`\" ) + f . col ( \"`base_stats.Speed`\" ) ... ) >>> new_df = unflatten ( flat_df ) >>> new_df . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] This yield the same result, and we did not have to mention the names of the columns we did not care about. This makes pipelines much easier to maintain. If a new column is added to your source table, you don't need to update this data enrichment code to propagate it automatically. On the other hand, with the first SQL solution, you would have had to specifically add this new field to the query to propagate it. We can even use DataFrame.transform to inline everything! >>> df . transform ( flatten ) . withColumn ( ... \"base_stats.Total\" , ... f . col ( \"`base_stats.Attack`\" ) + f . col ( \"`base_stats.Defense`\" ) + f . col ( \"`base_stats.HP`\" ) + ... f . col ( \"`base_stats.Sp Attack`\" ) + f . col ( \"`base_stats.Sp Defense`\" ) + f . col ( \"`base_stats.Speed`\" ) ... ) . transform ( unflatten ) . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] Update: Since version 0.0.4, the same result can be achieved with a simpler and more powerful transformation >>> from spark_frame import nested >>> nested . print_schema ( df ) root |-- base_stats.Attack: long (nullable = true) |-- base_stats.Defense: long (nullable = true) |-- base_stats.HP: long (nullable = true) |-- base_stats.Sp Attack: long (nullable = true) |-- base_stats.Sp Defense: long (nullable = true) |-- base_stats.Speed: long (nullable = true) |-- id: long (nullable = true) |-- name.english: string (nullable = true) |-- name.french: string (nullable = true) |-- types!: string (nullable = true) >>> df . transform ( nested . with_fields , { ... \"base_stats.Total\" : ... f . col ( \"base_stats.Attack\" ) + f . col ( \"base_stats.Defense\" ) + f . col ( \"base_stats.HP\" ) + ... f . col ( \"base_stats.`Sp Attack`\" ) + f . col ( \"base_stats.`Sp Defense`\" ) + f . col ( \"base_stats.Speed\" ) ... }) . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] Info This example uses data taken from https://raw.githubusercontent.com/fanzeyi/pokemon.json/master/pokedex.json . Methods used in this example transformations.flatten Flatten all the struct columns of a Spark DataFrame . Nested fields names will be joined together using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required struct_separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ( ... [( 1 , { \"a\" : 1 , \"b\" : { \"c\" : 1 , \"d\" : 1 }})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> flatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> df = spark . createDataFrame ( ... [( 1 , { \"a.a1\" : 1 , \"b.b1\" : { \"c.c1\" : 1 , \"d.d1\" : 1 }})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) >>> flatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) Source code in spark_frame/transformations_impl/flatten.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def flatten ( df : DataFrame , struct_separator : str = \".\" ) -> DataFrame : \"\"\"Flatten all the struct columns of a Spark [DataFrame][pyspark.sql.DataFrame]. Nested fields names will be joined together using the specified separator Args: df: A Spark DataFrame struct_separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame( ... [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> flatten(df).printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame( ... [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) <BLANKLINE> >>> flatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT s.b.c as `s.b.c`\" for each nested column. cols = [] def expand_struct ( struct : StructType , col_stack : List [ str ]) -> None : for field in struct : if type ( field . dataType ) == StructType : struct_field = cast ( StructType , field . dataType ) expand_struct ( struct_field , col_stack + [ field . name ]) else : column = f . col ( \".\" . join ( quote_columns ( col_stack + [ field . name ]))) cols . append ( column . alias ( struct_separator . join ( col_stack + [ field . name ]))) expand_struct ( df . schema , col_stack = []) return df . select ( cols ) transformations.unflatten Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([( 1 , 1 , 1 , 1 )], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> unflatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> df = spark . createDataFrame ([( 1 , 1 , 1 )], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) >>> unflatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) Source code in spark_frame/transformations_impl/unflatten.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def unflatten ( df : DataFrame , separator : str = \".\" ) -> DataFrame : \"\"\"Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Args: df: A Spark DataFrame separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> unflatten(df).printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) <BLANKLINE> >>> unflatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT struct(a, struct(s.b.c, s.b.d)) as s\" for each nested column. # There is a little twist as we don't want to rebuild the struct if all its fields are null, so we add a CASE WHEN def has_structs ( df : DataFrame ) -> bool : struct_fields = [ field for field in df . schema if is_struct ( field )] return len ( struct_fields ) > 0 if has_structs ( df ): df = flatten ( df ) tree = _build_nested_struct_tree ( df . columns , separator ) cols = _build_struct_from_tree ( tree , separator ) return df . select ( cols )","title":"Using flatten/unflatten"},{"location":"use_cases/flatten_unflatten/#transforming-nested-fields","text":"Warning The use case presented in this page is deprecated, but is kept to illustrate what flatten/unflatten can do. The spark_frame.nested module is much more powerful for manipulating nested data, because unlike flatten/unflatten, it does work with arrays. We recommend checking this use-case to see the spark_frame.nested module in action. This example demonstrates how the spark_frame.transformations.flatten and unflatten spark_frame.transformations.unflatten methods can be used to make data cleaning pipeline easier with PySpark. Let's take a sample DataFrame with our favorite example: Pokemons! >>> from spark_frame.examples.flatten_unflatten import _get_sample_pokemon_data >>> df = _get_sample_pokemon_data () >>> df . printSchema () root |-- base_stats: struct (nullable = true) | |-- Attack: long (nullable = true) | |-- Defense: long (nullable = true) | |-- HP: long (nullable = true) | |-- Sp Attack: long (nullable = true) | |-- Sp Defense: long (nullable = true) | |-- Speed: long (nullable = true) |-- id: long (nullable = true) |-- name: struct (nullable = true) | |-- english: string (nullable = true) | |-- french: string (nullable = true) |-- types: array (nullable = true) | |-- element: string (containsNull = true) >>> df . show ( vertical = True , truncate = False ) -RECORD 0------------------------------ base_stats | {49, 49, 45, 65, 65, 45} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] Let's say we want to add a new enrich the \"base_stats\" struct with a new field named \"Total\".","title":"Transforming nested fields"},{"location":"use_cases/flatten_unflatten/#spark_frame.examples.flatten_unflatten.transform_nested_fields--without-spark-frame","text":"Of course, we could write something in DataFrame or SQL like this: >>> df . createOrReplaceTempView ( \"df\" ) >>> new_df = df . sparkSession . sql ( ''' ... SELECT ... STRUCT( ... base_stats.*, ... base_stats.Attack + base_stats.Defense + base_stats.HP + ... base_stats.`Sp Attack` + base_stats.`Sp Defense` + base_stats.Speed as Total ... ) as base_stats, ... id, ... name, ... types ... FROM df ... ''' ) . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] It works, but it is a little cumbersome. Imagine how ugly the query would look like with a much bigger table, with hundreds of columns with three levels of nesting or more...","title":"Without spark-frame"},{"location":"use_cases/flatten_unflatten/#spark_frame.examples.flatten_unflatten.transform_nested_fields--with-spark-frame","text":"Instead, we can use the spark_frame.transformations.flatten and unflatten spark_frame.transformations.unflatten methods to reduce boilerplate significantly. >>> from spark_frame.transformations import flatten , unflatten >>> from pyspark.sql import functions as f >>> flat_df = flatten ( df ) >>> flat_df = flat_df . withColumn ( \"base_stats.Total\" , ... f . col ( \"`base_stats.Attack`\" ) + f . col ( \"`base_stats.Defense`\" ) + f . col ( \"`base_stats.HP`\" ) + ... f . col ( \"`base_stats.Sp Attack`\" ) + f . col ( \"`base_stats.Sp Defense`\" ) + f . col ( \"`base_stats.Speed`\" ) ... ) >>> new_df = unflatten ( flat_df ) >>> new_df . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] This yield the same result, and we did not have to mention the names of the columns we did not care about. This makes pipelines much easier to maintain. If a new column is added to your source table, you don't need to update this data enrichment code to propagate it automatically. On the other hand, with the first SQL solution, you would have had to specifically add this new field to the query to propagate it. We can even use DataFrame.transform to inline everything! >>> df . transform ( flatten ) . withColumn ( ... \"base_stats.Total\" , ... f . col ( \"`base_stats.Attack`\" ) + f . col ( \"`base_stats.Defense`\" ) + f . col ( \"`base_stats.HP`\" ) + ... f . col ( \"`base_stats.Sp Attack`\" ) + f . col ( \"`base_stats.Sp Defense`\" ) + f . col ( \"`base_stats.Speed`\" ) ... ) . transform ( unflatten ) . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] Update: Since version 0.0.4, the same result can be achieved with a simpler and more powerful transformation >>> from spark_frame import nested >>> nested . print_schema ( df ) root |-- base_stats.Attack: long (nullable = true) |-- base_stats.Defense: long (nullable = true) |-- base_stats.HP: long (nullable = true) |-- base_stats.Sp Attack: long (nullable = true) |-- base_stats.Sp Defense: long (nullable = true) |-- base_stats.Speed: long (nullable = true) |-- id: long (nullable = true) |-- name.english: string (nullable = true) |-- name.french: string (nullable = true) |-- types!: string (nullable = true) >>> df . transform ( nested . with_fields , { ... \"base_stats.Total\" : ... f . col ( \"base_stats.Attack\" ) + f . col ( \"base_stats.Defense\" ) + f . col ( \"base_stats.HP\" ) + ... f . col ( \"base_stats.`Sp Attack`\" ) + f . col ( \"base_stats.`Sp Defense`\" ) + f . col ( \"base_stats.Speed\" ) ... }) . show ( vertical = True , truncate = False ) -RECORD 0----------------------------------- base_stats | {49, 49, 45, 65, 65, 45, 318} id | 1 name | {Bulbasaur, Bulbizarre} types | [Grass, Poison] Info This example uses data taken from https://raw.githubusercontent.com/fanzeyi/pokemon.json/master/pokedex.json . Methods used in this example transformations.flatten Flatten all the struct columns of a Spark DataFrame . Nested fields names will be joined together using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required struct_separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ( ... [( 1 , { \"a\" : 1 , \"b\" : { \"c\" : 1 , \"d\" : 1 }})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> flatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> df = spark . createDataFrame ( ... [( 1 , { \"a.a1\" : 1 , \"b.b1\" : { \"c.c1\" : 1 , \"d.d1\" : 1 }})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) >>> flatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) Source code in spark_frame/transformations_impl/flatten.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def flatten ( df : DataFrame , struct_separator : str = \".\" ) -> DataFrame : \"\"\"Flatten all the struct columns of a Spark [DataFrame][pyspark.sql.DataFrame]. Nested fields names will be joined together using the specified separator Args: df: A Spark DataFrame struct_separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame( ... [(1, {\"a\": 1, \"b\": {\"c\": 1, \"d\": 1}})], ... \"id INT, s STRUCT<a:INT, b:STRUCT<c:INT, d:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> flatten(df).printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame( ... [(1, {\"a.a1\": 1, \"b.b1\": {\"c.c1\": 1, \"d.d1\": 1}})], ... \"id INT, `s.s1` STRUCT<`a.a1`:INT, `b.b1`:STRUCT<`c.c1`:INT, `d.d1`:INT>>\" ... ) >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: struct (nullable = true) | | |-- c.c1: integer (nullable = true) | | |-- d.d1: integer (nullable = true) <BLANKLINE> >>> flatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1?c.c1: integer (nullable = true) |-- s.s1?b.b1?d.d1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT s.b.c as `s.b.c`\" for each nested column. cols = [] def expand_struct ( struct : StructType , col_stack : List [ str ]) -> None : for field in struct : if type ( field . dataType ) == StructType : struct_field = cast ( StructType , field . dataType ) expand_struct ( struct_field , col_stack + [ field . name ]) else : column = f . col ( \".\" . join ( quote_columns ( col_stack + [ field . name ]))) cols . append ( column . alias ( struct_separator . join ( col_stack + [ field . name ]))) expand_struct ( df . schema , col_stack = []) return df . select ( cols ) transformations.unflatten Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Parameters: Name Type Description Default df DataFrame A Spark DataFrame required separator str A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots '.' Returns: Type Description DataFrame A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([( 1 , 1 , 1 , 1 )], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) >>> unflatten ( df ) . printSchema () root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) >>> df = spark . createDataFrame ([( 1 , 1 , 1 )], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\" ) >>> df . printSchema () root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) >>> unflatten ( df , \"?\" ) . printSchema () root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) Source code in spark_frame/transformations_impl/unflatten.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def unflatten ( df : DataFrame , separator : str = \".\" ) -> DataFrame : \"\"\"Reverse of the flatten operation Nested fields names will be separated from each other using the specified separator Args: df: A Spark DataFrame separator: A string used to separate the structs names from their elements. It might be useful to change the separator when some DataFrame's column names already contain dots Returns: A flattened DataFrame Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([(1, 1, 1, 1)], \"id INT, `s.a` INT, `s.b.c` INT, `s.b.d` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.a: integer (nullable = true) |-- s.b.c: integer (nullable = true) |-- s.b.d: integer (nullable = true) <BLANKLINE> >>> unflatten(df).printSchema() root |-- id: integer (nullable = true) |-- s: struct (nullable = true) | |-- a: integer (nullable = true) | |-- b: struct (nullable = true) | | |-- c: integer (nullable = true) | | |-- d: integer (nullable = true) <BLANKLINE> >>> df = spark.createDataFrame([(1, 1, 1)], \"id INT, `s.s1?a.a1` INT, `s.s1?b.b1` INT\") >>> df.printSchema() root |-- id: integer (nullable = true) |-- s.s1?a.a1: integer (nullable = true) |-- s.s1?b.b1: integer (nullable = true) <BLANKLINE> >>> unflatten(df, \"?\").printSchema() root |-- id: integer (nullable = true) |-- s.s1: struct (nullable = true) | |-- a.a1: integer (nullable = true) | |-- b.b1: integer (nullable = true) <BLANKLINE> \"\"\" # The idea is to recursively write a \"SELECT struct(a, struct(s.b.c, s.b.d)) as s\" for each nested column. # There is a little twist as we don't want to rebuild the struct if all its fields are null, so we add a CASE WHEN def has_structs ( df : DataFrame ) -> bool : struct_fields = [ field for field in df . schema if is_struct ( field )] return len ( struct_fields ) > 0 if has_structs ( df ): df = flatten ( df ) tree = _build_nested_struct_tree ( df . columns , separator ) cols = _build_struct_from_tree ( tree , separator ) return df . select ( cols )","title":"With spark-frame"},{"location":"use_cases/intro/","text":"This section present several use cases to give ideas how each function may be used. Each time a method is used in a use cases, it's full documentation will be available in a collapsible section. For instance, if a use-case uses the method spark_frame.functions.nullable you will see this section at the end of the section: nullable nullable ( col : Column ) -> Column Make a pyspark.sql.Column nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> from pyspark.sql import functions as f >>> df = spark . sql ( '''SELECT 1 as a''' ) . withColumn ( \"b\" , f . lit ( \"2\" )) >>> df . printSchema () root |-- a: integer (nullable = false) |-- b: string (nullable = false) >>> res = df . withColumn ( 'a' , nullable ( f . col ( 'a' ))) . withColumn ( 'b' , nullable ( f . col ( 'b' ))) >>> res . printSchema () root |-- a: integer (nullable = true) |-- b: string (nullable = true) Source code in spark_frame/functions.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def nullable ( col : Column ) -> Column : \"\"\"Make a `pyspark.sql.Column` nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> from pyspark.sql import functions as f >>> df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\")) >>> df.printSchema() root |-- a: integer (nullable = false) |-- b: string (nullable = false) <BLANKLINE> >>> res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b'))) >>> res.printSchema() root |-- a: integer (nullable = true) |-- b: string (nullable = true) <BLANKLINE> \"\"\" return f . when ( ~ col . isNull (), col ) You can also find a comprehensive list of all methods in the reference .","title":"Intro"},{"location":"use_cases/intro/#spark_frame.functions.nullable","text":"Make a pyspark.sql.Column nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> from pyspark.sql import functions as f >>> df = spark . sql ( '''SELECT 1 as a''' ) . withColumn ( \"b\" , f . lit ( \"2\" )) >>> df . printSchema () root |-- a: integer (nullable = false) |-- b: string (nullable = false) >>> res = df . withColumn ( 'a' , nullable ( f . col ( 'a' ))) . withColumn ( 'b' , nullable ( f . col ( 'b' ))) >>> res . printSchema () root |-- a: integer (nullable = true) |-- b: string (nullable = true) Source code in spark_frame/functions.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def nullable ( col : Column ) -> Column : \"\"\"Make a `pyspark.sql.Column` nullable. This is especially useful for literal which are always non-nullable by default. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> from pyspark.sql import functions as f >>> df = spark.sql('''SELECT 1 as a''').withColumn(\"b\", f.lit(\"2\")) >>> df.printSchema() root |-- a: integer (nullable = false) |-- b: string (nullable = false) <BLANKLINE> >>> res = df.withColumn('a', nullable(f.col('a'))).withColumn('b', nullable(f.col('b'))) >>> res.printSchema() root |-- a: integer (nullable = true) |-- b: string (nullable = true) <BLANKLINE> \"\"\" return f . when ( ~ col . isNull (), col )","title":"nullable()"},{"location":"use_cases/intro/#spark_frame.functions.nullable","text":"You can also find a comprehensive list of all methods in the reference .","title":"nullable()"},{"location":"use_cases/working_with_json/","text":"Extracting json values Sometimes, a column in a data source contains raw json strings, and you want to extract this value before starting to understand it. This already happened to me in several cases, such as: - Some automatic data capture tool that wraps a payload's raw json value into an Avro file. - A microservice event that follows a data contract but one of the column contains the raw json payload that this microservice exchanged with another external API. Let's take a sample DataFrame with two raw json columns. >>> from spark_frame.examples.working_with_json import _get_sample_data >>> df = _get_sample_data () >>> df . printSchema () root |-- call_id: integer (nullable = true) |-- raw_input: string (nullable = true) |-- raw_output: string (nullable = true) >>> df . show ( truncate = False ) # noqa: E501 +-------+-----------------------------------------------------------------------------+---------------------------------------------------------+ |call_id|raw_input |raw_output | +-------+-----------------------------------------------------------------------------+---------------------------------------------------------+ |1 |{\"model_name\": \"bot_detector\", \"model_version\": 3, \"model_args\": \"some data\"}|{\"model_score\": 0.94654, \"model_parameters\": \"some data\"}| |2 |{\"model_name\": \"cat_finder\", \"model_version\": 3, \"model_args\": \"some data\"} |{\"model_score\": 0.4234, \"model_parameters\": \"some data\"} | +-------+-----------------------------------------------------------------------------+---------------------------------------------------------+ This DataFrame represents the logs of an application calling a machine learning model. Keeping the \"call_id\" is important to be able to link this call to other events that happen in the system, and we would like to analyze these logs with typed data. Without spark-frame Spark does provide a from_json function that can parse a raw json column and convert it into a struct, but it does require the user to provide the schema of the json column in advance, like this: >>> from pyspark.sql import functions as f >>> raw_input_schema = '{\"fields\":[{\"name\":\"model_name\",\"nullable\":true,\"type\":\"string\"},{\"name\":\"model_version\",\"nullable\":true,\"type\":\"integer\"},{\"name\":\"model_args\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> raw_output_schema = '{\"fields\":[{\"name\":\"model_score\",\"nullable\":true,\"type\":\"double\"},{\"name\":\"model_parameters\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> df . withColumn ( ... \"raw_input\" , f . from_json ( \"raw_input\" , raw_input_schema ) ... ) . withColumn ( ... \"raw_output\" , f . from_json ( \"raw_output\" , raw_output_schema ) ... ) . show ( truncate = False ) +-------+----------------------------+--------------------+ |call_id|raw_input |raw_output | +-------+----------------------------+--------------------+ |1 |{bot_detector, 3, some data}|{0.94654, some data}| |2 |{cat_finder, 3, some data} |{0.4234, some data} | +-------+----------------------------+--------------------+ With spark-frame While it does works, as you can see writing the schema can be quite heavy. Also, for some reason, from_json does not accept the \"simpleString\" format, unlike the SparkSession.createDataFrame method. The first thing we can do to make things simpler is by using the method spark_frame.schema_utils.schema_from_simple_string like this : >>> from spark_frame.schema_utils import schema_from_simple_string >>> raw_input_schema = schema_from_simple_string ( \"model_name: STRING, model_version: INT, model_args: STRING\" ) >>> raw_output_schema = schema_from_simple_string ( \"model_score: DOUBLE, model_parameters: STRING\" ) >>> df . withColumn ( ... \"raw_input\" , f . from_json ( \"raw_input\" , raw_input_schema ) ... ) . withColumn ( ... \"raw_output\" , f . from_json ( \"raw_output\" , raw_output_schema ) ... ) . show ( truncate = False ) +-------+----------------------------+--------------------+ |call_id|raw_input |raw_output | +-------+----------------------------+--------------------+ |1 |{bot_detector, 3, some data}|{0.94654, some data}| |2 |{cat_finder, 3, some data} |{0.4234, some data} | +-------+----------------------------+--------------------+ But if we don't know the schema or if we know that the schema may evolve and we want to add (or at least, detect) the new fields automatically, we can leverage Spark's automatic json schema inference by using the method [ spark_frame.transformations.parse_json_columns ] [ spark_frame.transformations_impl.parse_json_columns.parse_json_columns ] to infer automatically the schema of these json columns. >>> from spark_frame.transformations import parse_json_columns >>> res = parse_json_columns ( df , [ \"raw_input\" , \"raw_output\" ]) >>> res . show ( truncate = False ) +-------+----------------------------+--------------------+ |call_id|raw_input |raw_output | +-------+----------------------------+--------------------+ |1 |{some data, bot_detector, 3}|{some data, 0.94654}| |2 |{some data, cat_finder, 3} |{some data, 0.4234} | +-------+----------------------------+--------------------+ >>> res . printSchema () root |-- call_id: integer (nullable = true) |-- raw_input: struct (nullable = true) | |-- model_args: string (nullable = true) | |-- model_name: string (nullable = true) | |-- model_version: long (nullable = true) |-- raw_output: struct (nullable = true) | |-- model_parameters: string (nullable = true) | |-- model_score: double (nullable = true) As we can see, the order of the field is different, this is because Spark's automatic inference will always sort the json field by names. Methods used in this example schema_utils.schema_from_simple_string Parses the given data type string to a :class: DataType . The data type string format equals pyspark.sql.types.DataType.simpleString , except that the top level struct type can omit the struct<> . This method requires the SparkSession to have already been instantiated. Parameters: Name Type Description Default schema_string str A simpleString representing a DataFrame schema. required Returns: Type Description DataType A DataType object representing the DataFrame schema. Raises: Type Description AssertionError If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> schema_from_simple_string ( \"int \" ) IntegerType() >>> schema_from_simple_string ( \"INT \" ) IntegerType() >>> schema_from_simple_string ( \"a: byte, b: decimal( 16 , 8 ) \" ) StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string ( \"a DOUBLE, b STRING\" ) StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string ( \"a: array< short>\" ) StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string ( \" map<string , string > \" ) MapType(StringType(), StringType(), True) Error cases: >>> schema_from_simple_string ( \"blabla\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"a: int,\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"array<int\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"map<int, boolean>>\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... Source code in spark_frame/schema_utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def schema_from_simple_string ( schema_string : str ) -> DataType : \"\"\"Parses the given data type string to a :class:`DataType`. The data type string format equals [pyspark.sql.types.DataType.simpleString][], except that the top level struct type can omit the ``struct<>``. This method requires the SparkSession to have already been instantiated. Args: schema_string: A simpleString representing a DataFrame schema. Returns: A DataType object representing the DataFrame schema. Raises: AssertionError: If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> schema_from_simple_string(\"int \") IntegerType() >>> schema_from_simple_string(\"INT \") IntegerType() >>> schema_from_simple_string(\"a: byte, b: decimal( 16 , 8 ) \") StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string(\"a DOUBLE, b STRING\") StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string(\"a: array< short>\") StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string(\" map<string , string > \") MapType(StringType(), StringType(), True) **Error cases:** >>> schema_from_simple_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... \"\"\" sc = SparkContext . _active_spark_context assert_true ( sc is not None , \"No SparkContext has been instantiated yet\" ) return _parse_datatype_string ( schema_string ) schema_utils.parse_json_columns Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's from_json function, with one main difference: from_json requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \" a.b.c \") (See Example 2). Parameters: Name Type Description Default df DataFrame A Spark DataFrame required columns Union [ str , List [ str ], Dict [ str , str ]] A column name, list of column names, or dict(column_name, parsed_column_name) required Returns: Type Description DataFrame A new DataFrame Examples: Example 1 : >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([ ... ( 1 , '[{\"a\": 1}, {\"a\": 2}]' ), ... ( 1 , '[{\"a\": 2}, {\"a\": 4}]' ), ... ( 2 , None ) ... ], \"id INT, json1 STRING\" ... ) >>> df . show () +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) >>> parse_json_columns ( df , 'json1' ) . printSchema () root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 2 : different output column name : >>> parse_json_columns ( df , { 'json1' : 'parsed_json1' }) . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 3 : json inside a struct : >>> df = spark . createDataFrame ([ ... ( 1 , { 'json1' : '[{\"a\": 1}, {\"a\": 2}]' }), ... ( 1 , { 'json1' : '[{\"a\": 2}, {\"a\": 4}]' }), ... ( 2 , None ) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df . show ( 10 , False ) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) >>> res = parse_json_columns ( df , 'struct.json1' ) >>> res . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) >>> res . show ( 10 , False ) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ Source code in spark_frame/transformations_impl/parse_json_columns.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def parse_json_columns ( df : DataFrame , columns : Union [ str , List [ str ], Dict [ str , str ]]) -> DataFrame : \"\"\"Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's `from_json` function, with one main difference: `from_json` requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \"`a.b.c`\") (See Example 2). Args: df: A Spark DataFrame columns: A column name, list of column names, or dict(column_name, parsed_column_name) Returns: A new DataFrame Examples: **Example 1 :** >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([ ... (1, '[{\"a\": 1}, {\"a\": 2}]'), ... (1, '[{\"a\": 2}, {\"a\": 4}]'), ... (2, None) ... ], \"id INT, json1 STRING\" ... ) >>> df.show() +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) <BLANKLINE> >>> parse_json_columns(df, 'json1').printSchema() root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 2 : different output column name :** >>> parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 3 : json inside a struct :** >>> df = spark.createDataFrame([ ... (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}), ... (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}), ... (2, None) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df.show(10, False) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) <BLANKLINE> >>> res = parse_json_columns(df, 'struct.json1') >>> res.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> >>> res.show(10, False) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ <BLANKLINE> \"\"\" if isinstance ( columns , str ): columns = [ columns ] if isinstance ( columns , list ): columns = { col : col for col in columns } wrapped_df = __wrap_json_columns ( df , columns ) schema_per_col = __infer_schema_per_column ( wrapped_df , list ( columns . values ())) res = __parse_json_columns ( wrapped_df , schema_per_col ) return res","title":"Working with json"},{"location":"use_cases/working_with_json/#extracting-json-values","text":"Sometimes, a column in a data source contains raw json strings, and you want to extract this value before starting to understand it. This already happened to me in several cases, such as: - Some automatic data capture tool that wraps a payload's raw json value into an Avro file. - A microservice event that follows a data contract but one of the column contains the raw json payload that this microservice exchanged with another external API. Let's take a sample DataFrame with two raw json columns. >>> from spark_frame.examples.working_with_json import _get_sample_data >>> df = _get_sample_data () >>> df . printSchema () root |-- call_id: integer (nullable = true) |-- raw_input: string (nullable = true) |-- raw_output: string (nullable = true) >>> df . show ( truncate = False ) # noqa: E501 +-------+-----------------------------------------------------------------------------+---------------------------------------------------------+ |call_id|raw_input |raw_output | +-------+-----------------------------------------------------------------------------+---------------------------------------------------------+ |1 |{\"model_name\": \"bot_detector\", \"model_version\": 3, \"model_args\": \"some data\"}|{\"model_score\": 0.94654, \"model_parameters\": \"some data\"}| |2 |{\"model_name\": \"cat_finder\", \"model_version\": 3, \"model_args\": \"some data\"} |{\"model_score\": 0.4234, \"model_parameters\": \"some data\"} | +-------+-----------------------------------------------------------------------------+---------------------------------------------------------+ This DataFrame represents the logs of an application calling a machine learning model. Keeping the \"call_id\" is important to be able to link this call to other events that happen in the system, and we would like to analyze these logs with typed data.","title":"Extracting json values"},{"location":"use_cases/working_with_json/#spark_frame.examples.working_with_json.extracting_json_values--without-spark-frame","text":"Spark does provide a from_json function that can parse a raw json column and convert it into a struct, but it does require the user to provide the schema of the json column in advance, like this: >>> from pyspark.sql import functions as f >>> raw_input_schema = '{\"fields\":[{\"name\":\"model_name\",\"nullable\":true,\"type\":\"string\"},{\"name\":\"model_version\",\"nullable\":true,\"type\":\"integer\"},{\"name\":\"model_args\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> raw_output_schema = '{\"fields\":[{\"name\":\"model_score\",\"nullable\":true,\"type\":\"double\"},{\"name\":\"model_parameters\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}' >>> df . withColumn ( ... \"raw_input\" , f . from_json ( \"raw_input\" , raw_input_schema ) ... ) . withColumn ( ... \"raw_output\" , f . from_json ( \"raw_output\" , raw_output_schema ) ... ) . show ( truncate = False ) +-------+----------------------------+--------------------+ |call_id|raw_input |raw_output | +-------+----------------------------+--------------------+ |1 |{bot_detector, 3, some data}|{0.94654, some data}| |2 |{cat_finder, 3, some data} |{0.4234, some data} | +-------+----------------------------+--------------------+","title":"Without spark-frame"},{"location":"use_cases/working_with_json/#spark_frame.examples.working_with_json.extracting_json_values--with-spark-frame","text":"While it does works, as you can see writing the schema can be quite heavy. Also, for some reason, from_json does not accept the \"simpleString\" format, unlike the SparkSession.createDataFrame method. The first thing we can do to make things simpler is by using the method spark_frame.schema_utils.schema_from_simple_string like this : >>> from spark_frame.schema_utils import schema_from_simple_string >>> raw_input_schema = schema_from_simple_string ( \"model_name: STRING, model_version: INT, model_args: STRING\" ) >>> raw_output_schema = schema_from_simple_string ( \"model_score: DOUBLE, model_parameters: STRING\" ) >>> df . withColumn ( ... \"raw_input\" , f . from_json ( \"raw_input\" , raw_input_schema ) ... ) . withColumn ( ... \"raw_output\" , f . from_json ( \"raw_output\" , raw_output_schema ) ... ) . show ( truncate = False ) +-------+----------------------------+--------------------+ |call_id|raw_input |raw_output | +-------+----------------------------+--------------------+ |1 |{bot_detector, 3, some data}|{0.94654, some data}| |2 |{cat_finder, 3, some data} |{0.4234, some data} | +-------+----------------------------+--------------------+ But if we don't know the schema or if we know that the schema may evolve and we want to add (or at least, detect) the new fields automatically, we can leverage Spark's automatic json schema inference by using the method [ spark_frame.transformations.parse_json_columns ] [ spark_frame.transformations_impl.parse_json_columns.parse_json_columns ] to infer automatically the schema of these json columns. >>> from spark_frame.transformations import parse_json_columns >>> res = parse_json_columns ( df , [ \"raw_input\" , \"raw_output\" ]) >>> res . show ( truncate = False ) +-------+----------------------------+--------------------+ |call_id|raw_input |raw_output | +-------+----------------------------+--------------------+ |1 |{some data, bot_detector, 3}|{some data, 0.94654}| |2 |{some data, cat_finder, 3} |{some data, 0.4234} | +-------+----------------------------+--------------------+ >>> res . printSchema () root |-- call_id: integer (nullable = true) |-- raw_input: struct (nullable = true) | |-- model_args: string (nullable = true) | |-- model_name: string (nullable = true) | |-- model_version: long (nullable = true) |-- raw_output: struct (nullable = true) | |-- model_parameters: string (nullable = true) | |-- model_score: double (nullable = true) As we can see, the order of the field is different, this is because Spark's automatic inference will always sort the json field by names. Methods used in this example schema_utils.schema_from_simple_string Parses the given data type string to a :class: DataType . The data type string format equals pyspark.sql.types.DataType.simpleString , except that the top level struct type can omit the struct<> . This method requires the SparkSession to have already been instantiated. Parameters: Name Type Description Default schema_string str A simpleString representing a DataFrame schema. required Returns: Type Description DataType A DataType object representing the DataFrame schema. Raises: Type Description AssertionError If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> schema_from_simple_string ( \"int \" ) IntegerType() >>> schema_from_simple_string ( \"INT \" ) IntegerType() >>> schema_from_simple_string ( \"a: byte, b: decimal( 16 , 8 ) \" ) StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string ( \"a DOUBLE, b STRING\" ) StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string ( \"a: array< short>\" ) StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string ( \" map<string , string > \" ) MapType(StringType(), StringType(), True) Error cases: >>> schema_from_simple_string ( \"blabla\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"a: int,\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"array<int\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string ( \"map<int, boolean>>\" ) Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... Source code in spark_frame/schema_utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def schema_from_simple_string ( schema_string : str ) -> DataType : \"\"\"Parses the given data type string to a :class:`DataType`. The data type string format equals [pyspark.sql.types.DataType.simpleString][], except that the top level struct type can omit the ``struct<>``. This method requires the SparkSession to have already been instantiated. Args: schema_string: A simpleString representing a DataFrame schema. Returns: A DataType object representing the DataFrame schema. Raises: AssertionError: If no SparkContext has been instantiated first. Examples: >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> schema_from_simple_string(\"int \") IntegerType() >>> schema_from_simple_string(\"INT \") IntegerType() >>> schema_from_simple_string(\"a: byte, b: decimal( 16 , 8 ) \") StructType([StructField('a', ByteType(), True), StructField('b', DecimalType(16,8), True)]) >>> schema_from_simple_string(\"a DOUBLE, b STRING\") StructType([StructField('a', DoubleType(), True), StructField('b', StringType(), True)]) >>> schema_from_simple_string(\"a: array< short>\") StructType([StructField('a', ArrayType(ShortType(), True), True)]) >>> schema_from_simple_string(\" map<string , string > \") MapType(StringType(), StringType(), True) **Error cases:** >>> schema_from_simple_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... >>> schema_from_simple_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... pyspark.sql.utils.ParseException:... \"\"\" sc = SparkContext . _active_spark_context assert_true ( sc is not None , \"No SparkContext has been instantiated yet\" ) return _parse_datatype_string ( schema_string ) schema_utils.parse_json_columns Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's from_json function, with one main difference: from_json requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \" a.b.c \") (See Example 2). Parameters: Name Type Description Default df DataFrame A Spark DataFrame required columns Union [ str , List [ str ], Dict [ str , str ]] A column name, list of column names, or dict(column_name, parsed_column_name) required Returns: Type Description DataFrame A new DataFrame Examples: Example 1 : >>> from pyspark.sql import SparkSession >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . createDataFrame ([ ... ( 1 , '[{\"a\": 1}, {\"a\": 2}]' ), ... ( 1 , '[{\"a\": 2}, {\"a\": 4}]' ), ... ( 2 , None ) ... ], \"id INT, json1 STRING\" ... ) >>> df . show () +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) >>> parse_json_columns ( df , 'json1' ) . printSchema () root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 2 : different output column name : >>> parse_json_columns ( df , { 'json1' : 'parsed_json1' }) . printSchema () root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) Example 3 : json inside a struct : >>> df = spark . createDataFrame ([ ... ( 1 , { 'json1' : '[{\"a\": 1}, {\"a\": 2}]' }), ... ( 1 , { 'json1' : '[{\"a\": 2}, {\"a\": 4}]' }), ... ( 2 , None ) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df . show ( 10 , False ) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ >>> df . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) >>> res = parse_json_columns ( df , 'struct.json1' ) >>> res . printSchema () root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) >>> res . show ( 10 , False ) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ Source code in spark_frame/transformations_impl/parse_json_columns.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def parse_json_columns ( df : DataFrame , columns : Union [ str , List [ str ], Dict [ str , str ]]) -> DataFrame : \"\"\"Transform the specified columns containing json strings in the given DataFrame into structs containing the equivalent parsed information. This method is similar to Spark's `from_json` function, with one main difference: `from_json` requires the user to pass the expected json schema, while this method performs a first pass on the DataFrame to detect automatically the json schema of each column. By default, the output columns will have the same name as the input columns, but if you want to keep the input columns you can pass a dict(input_col_name, output_col_name) to specify different output column names. Please be aware that automatic schema detection is not very robust, and while this method can be quite helpful for quick prototyping and data exploration, it is recommended to use a fixed schema and make sure the schema of the input json data is properly enforce, or at the very least use schema have a drift detection mechanism. WARNING : when you use this method on a column that is inside a struct (e.g. column \"a.b.c\"), instead of replacing that column it will create a new column outside the struct (e.g. \"`a.b.c`\") (See Example 2). Args: df: A Spark DataFrame columns: A column name, list of column names, or dict(column_name, parsed_column_name) Returns: A new DataFrame Examples: **Example 1 :** >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.createDataFrame([ ... (1, '[{\"a\": 1}, {\"a\": 2}]'), ... (1, '[{\"a\": 2}, {\"a\": 4}]'), ... (2, None) ... ], \"id INT, json1 STRING\" ... ) >>> df.show() +---+--------------------+ | id| json1| +---+--------------------+ | 1|[{\"a\": 1}, {\"a\": 2}]| | 1|[{\"a\": 2}, {\"a\": 4}]| | 2| null| +---+--------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) <BLANKLINE> >>> parse_json_columns(df, 'json1').printSchema() root |-- id: integer (nullable = true) |-- json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 2 : different output column name :** >>> parse_json_columns(df, {'json1': 'parsed_json1'}).printSchema() root |-- id: integer (nullable = true) |-- json1: string (nullable = true) |-- parsed_json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> **Example 3 : json inside a struct :** >>> df = spark.createDataFrame([ ... (1, {'json1': '[{\"a\": 1}, {\"a\": 2}]'}), ... (1, {'json1': '[{\"a\": 2}, {\"a\": 4}]'}), ... (2, None) ... ], \"id INT, struct STRUCT<json1: STRING>\" ... ) >>> df.show(10, False) +---+----------------------+ |id |struct | +---+----------------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}| |1 |{[{\"a\": 2}, {\"a\": 4}]}| |2 |null | +---+----------------------+ <BLANKLINE> >>> df.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) <BLANKLINE> >>> res = parse_json_columns(df, 'struct.json1') >>> res.printSchema() root |-- id: integer (nullable = true) |-- struct: struct (nullable = true) | |-- json1: string (nullable = true) |-- struct.json1: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a: long (nullable = true) <BLANKLINE> >>> res.show(10, False) +---+----------------------+------------+ |id |struct |struct.json1| +---+----------------------+------------+ |1 |{[{\"a\": 1}, {\"a\": 2}]}|[{1}, {2}] | |1 |{[{\"a\": 2}, {\"a\": 4}]}|[{2}, {4}] | |2 |null |null | +---+----------------------+------------+ <BLANKLINE> \"\"\" if isinstance ( columns , str ): columns = [ columns ] if isinstance ( columns , list ): columns = { col : col for col in columns } wrapped_df = __wrap_json_columns ( df , columns ) schema_per_col = __infer_schema_per_column ( wrapped_df , list ( columns . values ())) res = __parse_json_columns ( wrapped_df , schema_per_col ) return res","title":"With spark-frame"},{"location":"use_cases/working_with_nested_data/","text":"Transforming nested fields Let's take a sample DataFrame with a deeply nested schema >>> from spark_frame.examples.working_with_nested_data import _get_sample_employee_data >>> from pyspark.sql import functions as f >>> df = _get_sample_employee_data () >>> df . printSchema () root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- level: string (nullable = true) |-- projects: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- client: string (nullable = true) | | |-- tasks: array (nullable = true) | | | |-- element: struct (containsNull = true) | | | | |-- name: string (nullable = true) | | | | |-- description: string (nullable = true) | | | | |-- status: string (nullable = true) | | | | |-- estimate: long (nullable = true) >>> df . show ( truncate = False ) # noqa: E501 +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|skills |projects | +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[{Java, expert}, {Python, intermediate}] |[{Project A, Acme Inc, [{Task 1, Implement feature X, completed, 8}, {Task 2, Fix bug Y, in progress, 5}]}, {Project B, Beta Corp, [{Task 3, Implement feature Z, pending, 13}, {Task 4, Improve performance, in progress, 3}]}] | |2 |Jane Doe |25 |[{JavaScript, advanced}, {PHP, intermediate}]|[{Project C, Gamma Inc, [{Task 5, Implement feature W, completed, 20}, {Task 6, Fix bug V, in progress, 13}]}, {Project D, Delta Ltd, [{Task 7, Implement feature U, pending, 8}, {Task 8, Improve performance, in progress, 5}]}]| +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ As we can see, the schema has two top-level columns of type ARRAY ( skills and projects ), and the projects array contains a second level of repetition projects.tasks . Manipulating this DataFrame with Spark can quickly become really painful, and it is still quite simple compare to what engineers may encounter while working with entreprise-grade datasets. The task Let's say we want to enrich this DataFrame by performing the following changes: Change the skills.level to uppercase Cast the projects.tasks.estimate to double Without spark_frame.nested Prior to Spark 3.0, we would have had only two choices: Flatten skills and projects.tasks into two separate DataFrames, perform the transformation then join the two DataFrames back together. Write a custom Python UDF to perform the changes. Option 1. is not a good solution, as it would be quite costly, and require several shuffle operations. Option 2. is not great either, as the Python UDF would be slow and not very reusable. Had we been using Java or Scala, this might have been a better option already, as we would not incure the performance costs associated with Python UDFs, but this would still have required a lot of work to code the whole Employee data structure in Java/Scala before being able to manipulate it. Since Spark 3.1.0, a third option is available, which consists in using pyspark.sql.functions.transform and pyspark.sql.Column.withField to achieve our goal. However, the code that we need to write is quite complex: >>> new_df = df . withColumn ( ... \"skills\" , ... f . transform ( f . col ( \"skills\" ), lambda skill : skill . withField ( \"level\" , f . upper ( skill [ \"level\" ]))) ... ) . withColumn ( ... \"projects\" , ... f . transform ( ... f . col ( \"projects\" ), ... lambda project : project . withField ( ... \"tasks\" , ... f . transform ( ... project [ \"tasks\" ], ... lambda task : task . withField ( \"estimate\" , task [ \"estimate\" ] . cast ( \"DOUBLE\" ))), ... ), ... ), ... ) >>> new_df . printSchema () root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- level: string (nullable = true) |-- projects: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- client: string (nullable = true) | | |-- tasks: array (nullable = true) | | | |-- element: struct (containsNull = true) | | | | |-- name: string (nullable = true) | | | | |-- description: string (nullable = true) | | | | |-- status: string (nullable = true) | | | | |-- estimate: double (nullable = true) >>> new_df . select ( \"employee_id\" , \"name\" , \"age\" , \"skills\" ) . show ( truncate = False ) +-----------+----------+---+---------------------------------------------+ |employee_id|name |age|skills | +-----------+----------+---+---------------------------------------------+ |1 |John Smith|30 |[{Java, EXPERT}, {Python, INTERMEDIATE}] | |2 |Jane Doe |25 |[{JavaScript, ADVANCED}, {PHP, INTERMEDIATE}]| +-----------+----------+---+---------------------------------------------+ As we can see, the transformation worked: the schema is the same except projects.tasks.estimate which is now a double , and skills.name is now in uppercase. But hopefully we can agree that the code to achieve this looks quite complex, and that it's complexity would grow even more if we tried to perform more transformations at the same time. With spark_frame.nested The module spark_frame.nested proposes several methods to help us deal with nested data structure more easily. First, let's use spark_frame.nested.print_schema to get a flat version of the DataFrame's schema: >>> from spark_frame import nested >>> nested . print_schema ( df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills!.name: string (nullable = true) |-- skills!.level: string (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.description: string (nullable = true) |-- projects!.tasks!.status: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) As we can see, this is the same schema as before, but instead of being displayed as a tree, it is displayed as a flat list where each field is represented with its full name. We can also see that fields of type ARRAY can be easily identified thanks to the exclamation marks ( ! ) added after their names. Once you get used to it, this flat representation is more compact and easier to read than the tree representation, while conveying the same amount of information. This notation will also help us performing the target transformations more easily. As a reminder, we want to: Change the skills.level to uppercase Cast the projects.tasks.estimate to double Using the spark_frame.nested.with_fields method, this can be done like this: >>> new_df = df . transform ( nested . with_fields , { ... \"skills!.level\" : lambda skill : f . upper ( skill [ \"level\" ]), ... \"projects!.tasks!.estimate\" : lambda task : task [ \"estimate\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills!.name: string (nullable = true) |-- skills!.level: string (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.description: string (nullable = true) |-- projects!.tasks!.status: string (nullable = true) |-- projects!.tasks!.estimate: double (nullable = true) >>> new_df . select ( \"employee_id\" , \"name\" , \"age\" , \"skills\" ) . show ( truncate = False ) +-----------+----------+---+---------------------------------------------+ |employee_id|name |age|skills | +-----------+----------+---+---------------------------------------------+ |1 |John Smith|30 |[{Java, EXPERT}, {Python, INTERMEDIATE}] | |2 |Jane Doe |25 |[{JavaScript, ADVANCED}, {PHP, INTERMEDIATE}]| +-----------+----------+---+---------------------------------------------+ As we can see, we obtained the same result with a much simpler and cleaner code. Now let's explain what this code did: The spark_frame.nested.with_fields method is similar to the pyspark.sql.DataFrame.withColumns method, except that it works on nested fields inside structs and arrays. We pass it a Dict(field_name, transformation) indicating the expression we want to apply for each field. The transformation must be a higher order function: a lambda expression or named function that takes a Column as argument and returns a Column. The column passed to that function will represent the struct parent of the target field. For instance, when we write \"skills!.level\": lambda skill: f.upper(skill[\"level\"]) , the lambda function will be applied to each struct element of the array skills . Info The data for this example was generated by ChatGPT :-) Methods used in this example nested.print_schema Print the DataFrame's flattened schema to the standard output. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) Source code in spark_frame/nested_impl/print_schema.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def print_schema ( df : DataFrame ) -> None : \"\"\"Print the DataFrame's flattened schema to the standard output. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) <BLANKLINE> >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> \"\"\" print ( schema_string ( df )) nested.with_field Return a new DataFrame by adding or replacing (when they already exist) columns. This method is similar to the DataFrame.withColumn method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , AnyKindOfTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been DataFrame applied to the corresponding fields. If a field name did not exist in the input DataFrame, DataFrame it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested . with_fields ( df , { ... \"s.id\" : \"id\" , # column name (string) ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ Example 2: repeated fields >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) >>> df . show () +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df . transform ( nested . with_fields , { ... \"s!.b.d\" : lambda s : s [ \"a\" ] + s [ \"b\" ][ \"c\" ]} ... ) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) >>> new_df . show ( truncate = False ) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b.c\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ Example 3: field repeated twice >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) >>> df . show () +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> df . transform ( nested . with_fields , { \"s!.e!\" : lambda e : e . cast ( \"DOUBLE\" )}) . show () +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . with_fields , { ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) >>> new_df . show () +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/with_fields.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def with_fields ( df : DataFrame , fields : Mapping [ str , AnyKindOfTransformation ]) -> DataFrame : \"\"\"Return a new [DataFrame][pyspark.sql.DataFrame] by adding or replacing (when they already exist) columns. This method is similar to the [DataFrame.withColumn][pyspark.sql.DataFrame.withColumn] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been applied to the corresponding fields. If a field name did not exist in the input DataFrame, it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested.with_fields(df, { ... \"s.id\": \"id\", # column name (string) ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show() +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df.transform(nested.with_fields, { ... \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]} ... ) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) <BLANKLINE> >>> new_df.show(truncate=False) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.with_fields, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b.c\": f.lit(2) ... }).show(truncate=False) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show() +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.with_fields, { ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" default_columns = { field : None for field in nested . fields ( df )} fields = { ** default_columns , ** fields } return df . select ( * resolve_nested_fields ( fields , starting_level = df )) Selecting nested fields In this example, we will see how to select and rename specific elements in a nested data structure >>> from spark_frame.examples.working_with_nested_data import _get_sample_employee_data >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> df = _get_sample_employee_data () >>> nested . print_schema ( df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills!.name: string (nullable = true) |-- skills!.level: string (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.description: string (nullable = true) |-- projects!.tasks!.status: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> df . show ( truncate = False ) # noqa: E501 +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|skills |projects | +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[{Java, expert}, {Python, intermediate}] |[{Project A, Acme Inc, [{Task 1, Implement feature X, completed, 8}, {Task 2, Fix bug Y, in progress, 5}]}, {Project B, Beta Corp, [{Task 3, Implement feature Z, pending, 13}, {Task 4, Improve performance, in progress, 3}]}] | |2 |Jane Doe |25 |[{JavaScript, advanced}, {PHP, intermediate}]|[{Project C, Gamma Inc, [{Task 5, Implement feature W, completed, 20}, {Task 6, Fix bug V, in progress, 13}]}, {Project D, Delta Ltd, [{Task 7, Implement feature U, pending, 8}, {Task 8, Improve performance, in progress, 5}]}]| +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ The task Let's say we want to select only the following fields, while keeping the same overall structure: - employee_id - projects.name - projects.tasks.name Without spark_frame.nested This forces us to do something quite complicated, using pyspark.sql.functions.transform >>> new_df = df . select ( ... \"employee_id\" , ... f . transform ( \"projects\" , lambda project : ... f . struct ( project [ \"name\" ] . alias ( \"name\" ), f . transform ( project [ \"tasks\" ], lambda task : ... f . struct ( task [ \"name\" ] . alias ( \"name\" )) ... ) . alias ( \"tasks\" )) ... ) . alias ( \"projects\" ) ... ) >>> nested . print_schema ( new_df ) root |-- employee_id: integer (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) >>> new_df . show ( truncate = False ) +-----------+----------------------------------------------------------------------+ |employee_id|projects | +-----------+----------------------------------------------------------------------+ |1 |[{Project A, [{Task 1}, {Task 2}]}, {Project B, [{Task 3}, {Task 4}]}]| |2 |[{Project C, [{Task 5}, {Task 6}]}, {Project D, [{Task 7}, {Task 8}]}]| +-----------+----------------------------------------------------------------------+ With spark_frame.nested Using spark_frame.nested.select , we can easily obtain the exact same result. >>> new_df = df . transform ( nested . select , { ... \"employee_id\" : None , ... \"projects!.name\" : None , ... \"projects!.tasks!.name\" : None ... }) >>> nested . print_schema ( new_df ) root |-- employee_id: integer (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) >>> new_df . show ( truncate = False ) +-----------+----------------------------------------------------------------------+ |employee_id|projects | +-----------+----------------------------------------------------------------------+ |1 |[{Project A, [{Task 1}, {Task 2}]}, {Project B, [{Task 3}, {Task 4}]}]| |2 |[{Project C, [{Task 5}, {Task 6}]}, {Project D, [{Task 7}, {Task 8}]}]| +-----------+----------------------------------------------------------------------+ Here, None is used to indicate that we don't want to perform any transformation on the column, be we could also replace them with functions to perform transformations at the same time. For instance, we could pass all the names to uppercase like this: >>> df . transform ( nested . select , { ... \"employee_id\" : None , ... \"projects!.name\" : lambda project : f . upper ( project [ \"name\" ]), ... \"projects!.tasks!.name\" : lambda task : f . upper ( task [ \"name\" ]) ... }) . show ( truncate = False ) +-----------+----------------------------------------------------------------------+ |employee_id|projects | +-----------+----------------------------------------------------------------------+ |1 |[{PROJECT A, [{TASK 1}, {TASK 2}]}, {PROJECT B, [{TASK 3}, {TASK 4}]}]| |2 |[{PROJECT C, [{TASK 5}, {TASK 6}]}, {PROJECT D, [{TASK 7}, {TASK 8}]}]| +-----------+----------------------------------------------------------------------+ nested.print_schema Print the DataFrame's flattened schema to the standard output. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) Source code in spark_frame/nested_impl/print_schema.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def print_schema ( df : DataFrame ) -> None : \"\"\"Print the DataFrame's flattened schema to the standard output. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) <BLANKLINE> >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> \"\"\" print ( schema_string ( df )) nested.select Project a set of expressions and returns a new DataFrame . This method is similar to the DataFrame.select method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , ColumnTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame where only the specified field have been selected and the corresponding DataFrame transformations were applied to each of them. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested . select ( df , { ... \"s.a\" : \"s.a\" , # Column name (string) ... \"s.b\" : None , # None: use to keep a column without having to repeat its name ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---------+ | s| +---------+ |{2, 3, 5}| +---------+ Example 2: repeated fields >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) >>> df . show () +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df . transform ( nested . select , { ... \"s!.a\" : lambda s : s [ \"a\" ], ... \"s!.b\" : None , ... \"s!.c\" : lambda s : s [ \"a\" ] + s [ \"b\" ] ... }) . show ( truncate = False ) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . select , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ Example 3: field repeated twice >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) >>> df . show () +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> new_df = df . transform ( nested . select , { ... \"s1!.e!\" : None , ... \"s2!.e!\" : lambda e : e . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) >>> new_df . show () +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) >>> new_df . show () +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/select_impl.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def select ( df : DataFrame , fields : Mapping [ str , ColumnTransformation ]) -> DataFrame : \"\"\"Project a set of expressions and returns a new [DataFrame][pyspark.sql.DataFrame]. This method is similar to the [DataFrame.select][pyspark.sql.DataFrame.select] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame where only the specified field have been selected and the corresponding transformations were applied to each of them. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested.select(df, { ... \"s.a\": \"s.a\", # Column name (string) ... \"s.b\": None, # None: use to keep a column without having to repeat its name ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---------+ | s| +---------+ |{2, 3, 5}| +---------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df.transform(nested.select, { ... \"s!.a\": lambda s: s[\"a\"], ... \"s!.b\": None, ... \"s!.c\": lambda s: s[\"a\"] + s[\"b\"] ... }).show(truncate=False) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.select, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b\": f.lit(2) ... }).show(truncate=False) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> new_df = df.transform(nested.select, { ... \"s1!.e!\": None, ... \"s2!.e!\": lambda e : e.cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) <BLANKLINE> >>> new_df.show() +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" return df . select ( * resolve_nested_fields ( fields , starting_level = df )) Aggregating nested fields [TODO] Advanced transformations with nested fields [TODO]","title":"Working with nested data"},{"location":"use_cases/working_with_nested_data/#transforming-nested-fields","text":"Let's take a sample DataFrame with a deeply nested schema >>> from spark_frame.examples.working_with_nested_data import _get_sample_employee_data >>> from pyspark.sql import functions as f >>> df = _get_sample_employee_data () >>> df . printSchema () root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- level: string (nullable = true) |-- projects: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- client: string (nullable = true) | | |-- tasks: array (nullable = true) | | | |-- element: struct (containsNull = true) | | | | |-- name: string (nullable = true) | | | | |-- description: string (nullable = true) | | | | |-- status: string (nullable = true) | | | | |-- estimate: long (nullable = true) >>> df . show ( truncate = False ) # noqa: E501 +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|skills |projects | +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[{Java, expert}, {Python, intermediate}] |[{Project A, Acme Inc, [{Task 1, Implement feature X, completed, 8}, {Task 2, Fix bug Y, in progress, 5}]}, {Project B, Beta Corp, [{Task 3, Implement feature Z, pending, 13}, {Task 4, Improve performance, in progress, 3}]}] | |2 |Jane Doe |25 |[{JavaScript, advanced}, {PHP, intermediate}]|[{Project C, Gamma Inc, [{Task 5, Implement feature W, completed, 20}, {Task 6, Fix bug V, in progress, 13}]}, {Project D, Delta Ltd, [{Task 7, Implement feature U, pending, 8}, {Task 8, Improve performance, in progress, 5}]}]| +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ As we can see, the schema has two top-level columns of type ARRAY ( skills and projects ), and the projects array contains a second level of repetition projects.tasks . Manipulating this DataFrame with Spark can quickly become really painful, and it is still quite simple compare to what engineers may encounter while working with entreprise-grade datasets.","title":"Transforming nested fields"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.transform_nested_fields--the-task","text":"Let's say we want to enrich this DataFrame by performing the following changes: Change the skills.level to uppercase Cast the projects.tasks.estimate to double","title":"The task"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.transform_nested_fields--without-spark_framenested","text":"Prior to Spark 3.0, we would have had only two choices: Flatten skills and projects.tasks into two separate DataFrames, perform the transformation then join the two DataFrames back together. Write a custom Python UDF to perform the changes. Option 1. is not a good solution, as it would be quite costly, and require several shuffle operations. Option 2. is not great either, as the Python UDF would be slow and not very reusable. Had we been using Java or Scala, this might have been a better option already, as we would not incure the performance costs associated with Python UDFs, but this would still have required a lot of work to code the whole Employee data structure in Java/Scala before being able to manipulate it. Since Spark 3.1.0, a third option is available, which consists in using pyspark.sql.functions.transform and pyspark.sql.Column.withField to achieve our goal. However, the code that we need to write is quite complex: >>> new_df = df . withColumn ( ... \"skills\" , ... f . transform ( f . col ( \"skills\" ), lambda skill : skill . withField ( \"level\" , f . upper ( skill [ \"level\" ]))) ... ) . withColumn ( ... \"projects\" , ... f . transform ( ... f . col ( \"projects\" ), ... lambda project : project . withField ( ... \"tasks\" , ... f . transform ( ... project [ \"tasks\" ], ... lambda task : task . withField ( \"estimate\" , task [ \"estimate\" ] . cast ( \"DOUBLE\" ))), ... ), ... ), ... ) >>> new_df . printSchema () root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- level: string (nullable = true) |-- projects: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- name: string (nullable = true) | | |-- client: string (nullable = true) | | |-- tasks: array (nullable = true) | | | |-- element: struct (containsNull = true) | | | | |-- name: string (nullable = true) | | | | |-- description: string (nullable = true) | | | | |-- status: string (nullable = true) | | | | |-- estimate: double (nullable = true) >>> new_df . select ( \"employee_id\" , \"name\" , \"age\" , \"skills\" ) . show ( truncate = False ) +-----------+----------+---+---------------------------------------------+ |employee_id|name |age|skills | +-----------+----------+---+---------------------------------------------+ |1 |John Smith|30 |[{Java, EXPERT}, {Python, INTERMEDIATE}] | |2 |Jane Doe |25 |[{JavaScript, ADVANCED}, {PHP, INTERMEDIATE}]| +-----------+----------+---+---------------------------------------------+ As we can see, the transformation worked: the schema is the same except projects.tasks.estimate which is now a double , and skills.name is now in uppercase. But hopefully we can agree that the code to achieve this looks quite complex, and that it's complexity would grow even more if we tried to perform more transformations at the same time.","title":"Without spark_frame.nested"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.transform_nested_fields--with-spark_framenested","text":"The module spark_frame.nested proposes several methods to help us deal with nested data structure more easily. First, let's use spark_frame.nested.print_schema to get a flat version of the DataFrame's schema: >>> from spark_frame import nested >>> nested . print_schema ( df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills!.name: string (nullable = true) |-- skills!.level: string (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.description: string (nullable = true) |-- projects!.tasks!.status: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) As we can see, this is the same schema as before, but instead of being displayed as a tree, it is displayed as a flat list where each field is represented with its full name. We can also see that fields of type ARRAY can be easily identified thanks to the exclamation marks ( ! ) added after their names. Once you get used to it, this flat representation is more compact and easier to read than the tree representation, while conveying the same amount of information. This notation will also help us performing the target transformations more easily. As a reminder, we want to: Change the skills.level to uppercase Cast the projects.tasks.estimate to double Using the spark_frame.nested.with_fields method, this can be done like this: >>> new_df = df . transform ( nested . with_fields , { ... \"skills!.level\" : lambda skill : f . upper ( skill [ \"level\" ]), ... \"projects!.tasks!.estimate\" : lambda task : task [ \"estimate\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills!.name: string (nullable = true) |-- skills!.level: string (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.description: string (nullable = true) |-- projects!.tasks!.status: string (nullable = true) |-- projects!.tasks!.estimate: double (nullable = true) >>> new_df . select ( \"employee_id\" , \"name\" , \"age\" , \"skills\" ) . show ( truncate = False ) +-----------+----------+---+---------------------------------------------+ |employee_id|name |age|skills | +-----------+----------+---+---------------------------------------------+ |1 |John Smith|30 |[{Java, EXPERT}, {Python, INTERMEDIATE}] | |2 |Jane Doe |25 |[{JavaScript, ADVANCED}, {PHP, INTERMEDIATE}]| +-----------+----------+---+---------------------------------------------+ As we can see, we obtained the same result with a much simpler and cleaner code. Now let's explain what this code did: The spark_frame.nested.with_fields method is similar to the pyspark.sql.DataFrame.withColumns method, except that it works on nested fields inside structs and arrays. We pass it a Dict(field_name, transformation) indicating the expression we want to apply for each field. The transformation must be a higher order function: a lambda expression or named function that takes a Column as argument and returns a Column. The column passed to that function will represent the struct parent of the target field. For instance, when we write \"skills!.level\": lambda skill: f.upper(skill[\"level\"]) , the lambda function will be applied to each struct element of the array skills . Info The data for this example was generated by ChatGPT :-) Methods used in this example nested.print_schema Print the DataFrame's flattened schema to the standard output. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) Source code in spark_frame/nested_impl/print_schema.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def print_schema ( df : DataFrame ) -> None : \"\"\"Print the DataFrame's flattened schema to the standard output. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) <BLANKLINE> >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> \"\"\" print ( schema_string ( df )) nested.with_field Return a new DataFrame by adding or replacing (when they already exist) columns. This method is similar to the DataFrame.withColumn method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , AnyKindOfTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been DataFrame applied to the corresponding fields. If a field name did not exist in the input DataFrame, DataFrame it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested . with_fields ( df , { ... \"s.id\" : \"id\" , # column name (string) ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ Example 2: repeated fields >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) >>> df . show () +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df . transform ( nested . with_fields , { ... \"s!.b.d\" : lambda s : s [ \"a\" ] + s [ \"b\" ][ \"c\" ]} ... ) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) >>> new_df . show ( truncate = False ) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b.c\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ Example 3: field repeated twice >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) >>> df . show () +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> df . transform ( nested . with_fields , { \"s!.e!\" : lambda e : e . cast ( \"DOUBLE\" )}) . show () +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . with_fields , { ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) >>> new_df . show () +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/with_fields.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 def with_fields ( df : DataFrame , fields : Mapping [ str , AnyKindOfTransformation ]) -> DataFrame : \"\"\"Return a new [DataFrame][pyspark.sql.DataFrame] by adding or replacing (when they already exist) columns. This method is similar to the [DataFrame.withColumn][pyspark.sql.DataFrame.withColumn] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame with the same fields as the input DataFrame, where the specified transformations have been applied to the corresponding fields. If a field name did not exist in the input DataFrame, it will be added to the output DataFrame. If it did exist, the original value will be replaced with the new one. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s.a: integer (nullable = false) |-- s.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name or a Column expression. >>> new_df = nested.with_fields(df, { ... \"s.id\": \"id\", # column name (string) ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- id: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| s| +---+------------+ | 1|{2, 3, 1, 5}| +---+------------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(1 as a, STRUCT(2 as c) as b), STRUCT(3 as a, STRUCT(4 as c) as b)) as s ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) <BLANKLINE> >>> df.show() +---+--------------------+ | id| s| +---+--------------------+ | 1|[{1, {2}}, {3, {4}}]| +---+--------------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> new_df = df.transform(nested.with_fields, { ... \"s!.b.d\": lambda s: s[\"a\"] + s[\"b\"][\"c\"]} ... ) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b.c: integer (nullable = false) |-- s!.b.d: integer (nullable = false) <BLANKLINE> >>> new_df.show(truncate=False) +---+--------------------------+ |id |s | +---+--------------------------+ |1 |[{1, {2, 3}}, {3, {4, 7}}]| +---+--------------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.with_fields, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b.c\": f.lit(2) ... }).show(truncate=False) +---+--------------------+ |id |s | +---+--------------------+ |1 |[{1, {2}}, {1, {2}}]| +---+--------------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| s| +---+-------------+ | 1|[{[1, 2, 3]}]| +---+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> df.transform(nested.with_fields, {\"s!.e!\": lambda e : e.cast(\"DOUBLE\")}).show() +---+-------------------+ | id| s| +---+-------------------+ | 1|[{[1.0, 2.0, 3.0]}]| +---+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.with_fields, { ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---+---------------+ | id| m1| +---+---------------+ | 1|{A -> {2.0, 3}}| +---+---------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" default_columns = { field : None for field in nested . fields ( df )} fields = { ** default_columns , ** fields } return df . select ( * resolve_nested_fields ( fields , starting_level = df ))","title":"With spark_frame.nested"},{"location":"use_cases/working_with_nested_data/#selecting-nested-fields","text":"In this example, we will see how to select and rename specific elements in a nested data structure >>> from spark_frame.examples.working_with_nested_data import _get_sample_employee_data >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> df = _get_sample_employee_data () >>> nested . print_schema ( df ) root |-- employee_id: integer (nullable = true) |-- name: string (nullable = true) |-- age: long (nullable = true) |-- skills!.name: string (nullable = true) |-- skills!.level: string (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.client: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) |-- projects!.tasks!.description: string (nullable = true) |-- projects!.tasks!.status: string (nullable = true) |-- projects!.tasks!.estimate: long (nullable = true) >>> df . show ( truncate = False ) # noqa: E501 +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |employee_id|name |age|skills |projects | +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ |1 |John Smith|30 |[{Java, expert}, {Python, intermediate}] |[{Project A, Acme Inc, [{Task 1, Implement feature X, completed, 8}, {Task 2, Fix bug Y, in progress, 5}]}, {Project B, Beta Corp, [{Task 3, Implement feature Z, pending, 13}, {Task 4, Improve performance, in progress, 3}]}] | |2 |Jane Doe |25 |[{JavaScript, advanced}, {PHP, intermediate}]|[{Project C, Gamma Inc, [{Task 5, Implement feature W, completed, 20}, {Task 6, Fix bug V, in progress, 13}]}, {Project D, Delta Ltd, [{Task 7, Implement feature U, pending, 8}, {Task 8, Improve performance, in progress, 5}]}]| +-----------+----------+---+---------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"Selecting nested fields"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.select_nested_fields--the-task","text":"Let's say we want to select only the following fields, while keeping the same overall structure: - employee_id - projects.name - projects.tasks.name","title":"The task"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.select_nested_fields--without-spark_framenested","text":"This forces us to do something quite complicated, using pyspark.sql.functions.transform >>> new_df = df . select ( ... \"employee_id\" , ... f . transform ( \"projects\" , lambda project : ... f . struct ( project [ \"name\" ] . alias ( \"name\" ), f . transform ( project [ \"tasks\" ], lambda task : ... f . struct ( task [ \"name\" ] . alias ( \"name\" )) ... ) . alias ( \"tasks\" )) ... ) . alias ( \"projects\" ) ... ) >>> nested . print_schema ( new_df ) root |-- employee_id: integer (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) >>> new_df . show ( truncate = False ) +-----------+----------------------------------------------------------------------+ |employee_id|projects | +-----------+----------------------------------------------------------------------+ |1 |[{Project A, [{Task 1}, {Task 2}]}, {Project B, [{Task 3}, {Task 4}]}]| |2 |[{Project C, [{Task 5}, {Task 6}]}, {Project D, [{Task 7}, {Task 8}]}]| +-----------+----------------------------------------------------------------------+","title":"Without spark_frame.nested"},{"location":"use_cases/working_with_nested_data/#spark_frame.examples.working_with_nested_data.select_nested_fields--with-spark_framenested","text":"Using spark_frame.nested.select , we can easily obtain the exact same result. >>> new_df = df . transform ( nested . select , { ... \"employee_id\" : None , ... \"projects!.name\" : None , ... \"projects!.tasks!.name\" : None ... }) >>> nested . print_schema ( new_df ) root |-- employee_id: integer (nullable = true) |-- projects!.name: string (nullable = true) |-- projects!.tasks!.name: string (nullable = true) >>> new_df . show ( truncate = False ) +-----------+----------------------------------------------------------------------+ |employee_id|projects | +-----------+----------------------------------------------------------------------+ |1 |[{Project A, [{Task 1}, {Task 2}]}, {Project B, [{Task 3}, {Task 4}]}]| |2 |[{Project C, [{Task 5}, {Task 6}]}, {Project D, [{Task 7}, {Task 8}]}]| +-----------+----------------------------------------------------------------------+ Here, None is used to indicate that we don't want to perform any transformation on the column, be we could also replace them with functions to perform transformations at the same time. For instance, we could pass all the names to uppercase like this: >>> df . transform ( nested . select , { ... \"employee_id\" : None , ... \"projects!.name\" : lambda project : f . upper ( project [ \"name\" ]), ... \"projects!.tasks!.name\" : lambda task : f . upper ( task [ \"name\" ]) ... }) . show ( truncate = False ) +-----------+----------------------------------------------------------------------+ |employee_id|projects | +-----------+----------------------------------------------------------------------+ |1 |[{PROJECT A, [{TASK 1}, {TASK 2}]}, {PROJECT B, [{TASK 3}, {TASK 4}]}]| |2 |[{PROJECT C, [{TASK 5}, {TASK 6}]}, {PROJECT D, [{TASK 7}, {TASK 8}]}]| +-----------+----------------------------------------------------------------------+ nested.print_schema Print the DataFrame's flattened schema to the standard output. Structs are flattened with a . after their name. Arrays are flattened with a ! character after their name. Maps are flattened with a %key and '%value' after their name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) Source code in spark_frame/nested_impl/print_schema.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def print_schema ( df : DataFrame ) -> None : \"\"\"Print the DataFrame's flattened schema to the standard output. - Structs are flattened with a `.` after their name. - Arrays are flattened with a `!` character after their name. - Maps are flattened with a `%key` and '%value' after their name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame Examples: >>> from pyspark.sql import SparkSession >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT ... 1 as id, ... ARRAY(STRUCT(2 as a, ARRAY(STRUCT(3 as c, 4 as d)) as b, ARRAY(5, 6) as e)) as s1, ... STRUCT(7 as f) as s2, ... ARRAY(ARRAY(1, 2), ARRAY(3, 4)) as s3, ... ARRAY(ARRAY(STRUCT(1 as e, 2 as f)), ARRAY(STRUCT(3 as e, 4 as f))) as s4, ... MAP(STRUCT(1 as a), STRUCT(2 as b)) as m1 ... ''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s1: array (nullable = false) | |-- element: struct (containsNull = false) | | |-- a: integer (nullable = false) | | |-- b: array (nullable = false) | | | |-- element: struct (containsNull = false) | | | | |-- c: integer (nullable = false) | | | | |-- d: integer (nullable = false) | | |-- e: array (nullable = false) | | | |-- element: integer (containsNull = false) |-- s2: struct (nullable = false) | |-- f: integer (nullable = false) |-- s3: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: integer (containsNull = false) |-- s4: array (nullable = false) | |-- element: array (containsNull = false) | | |-- element: struct (containsNull = false) | | | |-- e: integer (nullable = false) | | | |-- f: integer (nullable = false) |-- m1: map (nullable = false) | |-- key: struct | | |-- a: integer (nullable = false) | |-- value: struct (valueContainsNull = false) | | |-- b: integer (nullable = false) <BLANKLINE> >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.a: integer (nullable = false) |-- s1!.b!.c: integer (nullable = false) |-- s1!.b!.d: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2.f: integer (nullable = false) |-- s3!!: integer (nullable = false) |-- s4!!.e: integer (nullable = false) |-- s4!!.f: integer (nullable = false) |-- m1%key.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> \"\"\" print ( schema_string ( df )) nested.select Project a set of expressions and returns a new DataFrame . This method is similar to the DataFrame.select method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: \".\" is the separator for struct elements \"!\" must be appended at the end of fields that are repeated (arrays) Map keys are appended with %key Map values are appended with %value The following types of transformation are allowed: String and column expressions can be used on any non-repeated field, even nested ones. When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). None can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. Limitation: Dots, percents, and exclamation marks are not supported in field names Given the syntax used, every method defined in the spark_frame.nested module assumes that all field names in DataFrames do not contain any dot . , percent % or exclamation mark ! . This can be worked around using the transformation spark_frame.transformations.transform_all_field_names . Parameters: Name Type Description Default df DataFrame A Spark DataFrame required fields Mapping [ str , ColumnTransformation ] A Dict(field_name, transformation_to_apply) required Returns: Type Description DataFrame A new DataFrame where only the specified field have been selected and the corresponding DataFrame transformations were applied to each of them. Examples: Example 1: non-repeated fields >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession . builder . appName ( \"doctest\" ) . getOrCreate () >>> df = spark . sql ( '''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''' ) >>> df . printSchema () root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) >>> df . show () +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested . select ( df , { ... \"s.a\" : \"s.a\" , # Column name (string) ... \"s.b\" : None , # None: use to keep a column without having to repeat its name ... \"s.c\" : f . col ( \"s.a\" ) + f . col ( \"s.b\" ) # Column expression ... }) >>> new_df . printSchema () root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) >>> new_df . show () +---------+ | s| +---------+ |{2, 3, 5}| +---------+ Example 2: repeated fields >>> df = spark . sql ( 'SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) >>> df . show () +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df . transform ( nested . select , { ... \"s!.a\" : lambda s : s [ \"a\" ], ... \"s!.b\" : None , ... \"s!.c\" : lambda s : s [ \"a\" ] + s [ \"b\" ] ... }) . show ( truncate = False ) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df . transform ( nested . select , { ... \"id\" : None , ... \"s!.a\" : \"id\" , ... \"s!.b\" : f . lit ( 2 ) ... }) . show ( truncate = False ) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ Example 3: field repeated twice >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) >>> df . show () +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ Here, the lambda expression will be applied to the last repeated element e . >>> new_df = df . transform ( nested . select , { ... \"s1!.e!\" : None , ... \"s2!.e!\" : lambda e : e . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) >>> new_df . show () +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ Example 4: Dataframe with maps >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) >>> df . show () +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"m1%key\" : lambda key : f . upper ( key ), ... \"m1%value.a\" : lambda value : value [ \"a\" ] . cast ( \"DOUBLE\" ) ... }) >>> nested . print_schema ( new_df ) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) >>> new_df . show () +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ Example 5: Accessing multiple repetition levels >>> df = spark . sql ( ''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''' ) >>> nested . print_schema ( df ) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) >>> df . show ( truncate = False ) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df . transform ( nested . select , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda s1 , value : value - s1 [ \"average\" ] ... }) >>> new_df . show ( truncate = False ) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df . transform ( nested . with_fields , { ... \"id\" : None , ... \"s1!.average\" : None , ... \"s1!.values!\" : lambda root , s1 , value : value - s1 [ \"average\" ] + root [ \"id\" ] ... }) >>> new_df . show ( truncate = False ) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ Source code in spark_frame/nested_impl/select_impl.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def select ( df : DataFrame , fields : Mapping [ str , ColumnTransformation ]) -> DataFrame : \"\"\"Project a set of expressions and returns a new [DataFrame][pyspark.sql.DataFrame]. This method is similar to the [DataFrame.select][pyspark.sql.DataFrame.select] method, with the extra capability of working on nested and repeated fields (structs and arrays). The syntax for field names works as follows: - \".\" is the separator for struct elements - \"!\" must be appended at the end of fields that are repeated (arrays) - Map keys are appended with `%key` - Map values are appended with `%value` The following types of transformation are allowed: - String and column expressions can be used on any non-repeated field, even nested ones. - When working on repeated fields, transformations must be expressed as higher order functions (e.g. lambda expressions). String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. - When working on multiple levels of nested arrays, higher order functions may take multiple arguments, corresponding to each level of repetition (See Example 5.). - `None` can also be used to represent the identity transformation, this is useful to select a field without changing and without having to repeat its name. !!! warning \"Limitation: Dots, percents, and exclamation marks are not supported in field names\" Given the syntax used, every method defined in the `spark_frame.nested` module assumes that all field names in DataFrames do not contain any dot `.`, percent `%` or exclamation mark `!`. This can be worked around using the transformation [`spark_frame.transformations.transform_all_field_names`] [spark_frame.transformations_impl.transform_all_field_names.transform_all_field_names]. Args: df: A Spark DataFrame fields: A Dict(field_name, transformation_to_apply) Returns: A new DataFrame where only the specified field have been selected and the corresponding transformations were applied to each of them. Examples: *Example 1: non-repeated fields* >>> from pyspark.sql import SparkSession >>> from pyspark.sql import functions as f >>> from spark_frame import nested >>> spark = SparkSession.builder.appName(\"doctest\").getOrCreate() >>> df = spark.sql('''SELECT 1 as id, STRUCT(2 as a, 3 as b) as s''') >>> df.printSchema() root |-- id: integer (nullable = false) |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+------+ | id| s| +---+------+ | 1|{2, 3}| +---+------+ <BLANKLINE> Transformations on non-repeated fields may be expressed as a string representing a column name, a Column expression or None. (In this example the column \"id\" will be dropped because it was not selected) >>> new_df = nested.select(df, { ... \"s.a\": \"s.a\", # Column name (string) ... \"s.b\": None, # None: use to keep a column without having to repeat its name ... \"s.c\": f.col(\"s.a\") + f.col(\"s.b\") # Column expression ... }) >>> new_df.printSchema() root |-- s: struct (nullable = false) | |-- a: integer (nullable = false) | |-- b: integer (nullable = false) | |-- c: integer (nullable = false) <BLANKLINE> >>> new_df.show() +---------+ | s| +---------+ |{2, 3, 5}| +---------+ <BLANKLINE> *Example 2: repeated fields* >>> df = spark.sql('SELECT 1 as id, ARRAY(STRUCT(1 as a, 2 as b), STRUCT(3 as a, 4 as b)) as s') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s!.a: integer (nullable = false) |-- s!.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+----------------+ | id| s| +---+----------------+ | 1|[{1, 2}, {3, 4}]| +---+----------------+ <BLANKLINE> Transformations on repeated fields must be expressed as higher-order functions (lambda expressions or named functions). The value passed to this function will correspond to the last repeated element. >>> df.transform(nested.select, { ... \"s!.a\": lambda s: s[\"a\"], ... \"s!.b\": None, ... \"s!.c\": lambda s: s[\"a\"] + s[\"b\"] ... }).show(truncate=False) +----------------------+ |s | +----------------------+ |[{1, 2, 3}, {3, 4, 7}]| +----------------------+ <BLANKLINE> String and column expressions can be used on repeated fields as well, but their value will be repeated multiple times. >>> df.transform(nested.select, { ... \"id\": None, ... \"s!.a\": \"id\", ... \"s!.b\": f.lit(2) ... }).show(truncate=False) +---+----------------+ |id |s | +---+----------------+ |1 |[{1, 2}, {1, 2}]| +---+----------------+ <BLANKLINE> *Example 3: field repeated twice* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY(STRUCT(ARRAY(1, 2, 3) as e)) as s1, ... ARRAY(STRUCT(ARRAY(4, 5, 6) as e)) as s2 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.e!: integer (nullable = false) |-- s2!.e!: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+-------------+ | id| s1| s2| +---+-------------+-------------+ | 1|[{[1, 2, 3]}]|[{[4, 5, 6]}]| +---+-------------+-------------+ <BLANKLINE> Here, the lambda expression will be applied to the last repeated element `e`. >>> new_df = df.transform(nested.select, { ... \"s1!.e!\": None, ... \"s2!.e!\": lambda e : e.cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- s1!.e!: integer (nullable = false) |-- s2!.e!: double (nullable = false) <BLANKLINE> >>> new_df.show() +-------------+-------------------+ | s1| s2| +-------------+-------------------+ |[{[1, 2, 3]}]|[{[4.0, 5.0, 6.0]}]| +-------------+-------------------+ <BLANKLINE> *Example 4: Dataframe with maps* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... MAP(\"a\", STRUCT(2 as a, 3 as b)) as m1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: integer (nullable = false) |-- m1%value.b: integer (nullable = false) <BLANKLINE> >>> df.show() +---+-------------+ | id| m1| +---+-------------+ | 1|{a -> {2, 3}}| +---+-------------+ <BLANKLINE> >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"m1%key\": lambda key : f.upper(key), ... \"m1%value.a\": lambda value : value[\"a\"].cast(\"DOUBLE\") ... }) >>> nested.print_schema(new_df) root |-- id: integer (nullable = false) |-- m1%key: string (nullable = false) |-- m1%value.a: double (nullable = false) <BLANKLINE> >>> new_df.show() +---+------------+ | id| m1| +---+------------+ | 1|{A -> {2.0}}| +---+------------+ <BLANKLINE> *Example 5: Accessing multiple repetition levels* >>> df = spark.sql(''' ... SELECT ... 1 as id, ... ARRAY( ... STRUCT(2 as average, ARRAY(1, 2, 3) as values), ... STRUCT(3 as average, ARRAY(1, 2, 3, 4, 5) as values) ... ) as s1 ... ''') >>> nested.print_schema(df) root |-- id: integer (nullable = false) |-- s1!.average: integer (nullable = false) |-- s1!.values!: integer (nullable = false) <BLANKLINE> >>> df.show(truncate=False) +---+--------------------------------------+ |id |s1 | +---+--------------------------------------+ |1 |[{2, [1, 2, 3]}, {3, [1, 2, 3, 4, 5]}]| +---+--------------------------------------+ <BLANKLINE> Here, the transformation applied to \"s1!.values!\" takes two arguments. >>> new_df = df.transform(nested.select, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda s1, value : value - s1[\"average\"] ... }) >>> new_df.show(truncate=False) +---+-----------------------------------------+ |id |s1 | +---+-----------------------------------------+ |1 |[{2, [-1, 0, 1]}, {3, [-2, -1, 0, 1, 2]}]| +---+-----------------------------------------+ <BLANKLINE> Extra arguments can be added to the left for each repetition level, up to the root level. >>> new_df = df.transform(nested.with_fields, { ... \"id\": None, ... \"s1!.average\": None, ... \"s1!.values!\": lambda root, s1, value : value - s1[\"average\"] + root[\"id\"] ... }) >>> new_df.show(truncate=False) +---+---------------------------------------+ |id |s1 | +---+---------------------------------------+ |1 |[{2, [0, 1, 2]}, {3, [-1, 0, 1, 2, 3]}]| +---+---------------------------------------+ <BLANKLINE> \"\"\" return df . select ( * resolve_nested_fields ( fields , starting_level = df ))","title":"With spark_frame.nested"},{"location":"use_cases/working_with_nested_data/#aggregating-nested-fields-todo","text":"","title":"Aggregating nested fields [TODO]"},{"location":"use_cases/working_with_nested_data/#advanced-transformations-with-nested-fields-todo","text":"","title":"Advanced transformations with nested fields [TODO]"}]}